{"body":{"0":"pewanalytics\npewanalytics is a Python package that provides text processing and statistics utilities for computational social science researchers.\nInstallation\nTo install, you can use pip:\npip install git+https:\/\/github.com\/pewresearch\/pewanalytics#egg=pewanalytics\n\nOr you can install from source:\ngit clone https:\/\/github.com\/pewresearch\/pewanalytics.git\ncd pewanalytics\npython setup.py install\n\nInstallation Troubleshooting\nUsing 64-bit Python\nSome of our libraries require the use of 64-bit Python. If you encounter errors during installation \nthat are related to missing libraries, you may be using 32-bit Python. We recommend that you uninstall \nthis version and switch to a 64-bit version instead. On Windows, these will be marked with x86-64; you \ncan find the latest 64-bit versions of Python here.\nInstalling ssdeep\nssdeep is an optional dependency that can be used by the get_hash function in Pewtils. \nInstallation instructions for various Linux distributions can be found in the library's \ndocumentation. The ssdeep \nPython library is not currently compatible with Windows. \nInstalling ssdeep on Mac OS may involve a few additional steps, detailed below:\n\nInstall Homebrew\nInstall xcode\nxcode-select --install\n\n\nInstall system dependencies\nbrew install pkg-config libffi libtool automake\nln -s \/usr\/local\/bin\/glibtoolize \/usr\/local\/bin\/libtoolize\n\n\nInstall ssdeep with an additional flag to build the required libraries\nBUILD_LIB=1 pip install ssdeep\n\n\nIf step 4 fails, you may need to redirect your system to the new libraries by setting the following flags:\nexport LIBTOOL=`which glibtool`\nexport LIBTOOLIZE=`which glibtoolize`\n\nDo this and try step 4 again.\nNow you should be able to run the main installation process detailed above.\n\nDocumentation\nPlease refer to the official documentation for information on how to use this package.\nUse Policy\nIn addition to the license, Users must abide by the following conditions:\n\nUser may not use the Center's logo\nUser may not use the Center's name in any advertising, marketing or promotional materials.\nUser may not use the licensed materials in any manner that implies, suggests, or could otherwise be perceived as attributing a particular policy or lobbying objective or opinion to the Center, or as a Center endorsement of a cause, candidate, issue, party, product, business, organization, religion or viewpoint.\n\nIssues and Pull Requests\nThis code is provided as-is for use in your own projects. You are free to submit issues and pull requests with any questions or suggestions you may have. We will do our best to respond within a 30-day time period.\nRecommended Package Citation\nPew Research Center, 2020, \"pewanalytics\" Available at: github.com\/pewresearch\/pewanalytics\nAcknowledgements\nThe following authors contributed to this repository:\n\nPatrick van Kessel\nRegina Widjaya\nSkye Toor\nEmma Remy\nOnyi Lam\nBrian Broderick\nGalen Stocking\nDennis Quinn\n\nAbout Pew Research Center\nPew Research Center is a nonpartisan fact tank that informs the public about the issues, attitudes and trends shaping the world. It does not take policy positions. The Center conducts public opinion polling, demographic research, content analysis and other data-driven social science research. It studies U.S. politics and policy; journalism and media; internet, science and technology; religion and public life; Hispanic trends; global attitudes and trends; and U.S. social and demographic trends. All of the Center's reports are available at www.pewresearch.org. Pew Research Center is a subsidiary of The Pew Charitable Trusts, its primary funder.\nContact\nFor all inquiries, please email info@pewresearch.org. Please be sure to specify your deadline, and we will get back to you as soon as possible. This email account is monitored regularly by Pew Research Center Communications staff.\n","1":"scuole\n(It's Italian for \"schools.\")\nPublic Schools 3!\n\nSetup\nUpdating and deploying\n\nChanges to the code\nChanges to the data\n\nFor cohorts\nFor AskTED\nFor TAPR\n\n\n\n\nTroubleshooting\nWorkspace\nAdmin\nTo-dos\n\nSetup\nThis project assumes you are using the provided docker PostgreSQL database build. Make sure docker is up and running, then run:\nmake docker\/db\nThis will create the data volume and instance of PostgreSQL for Django.\nFire up pipenv:\npipenv --three\nInstall dependencies via pipenv:\npipenv install --dev\nRun pipenv:\npipenv shell\nIf there's a problem installing psycopg2 when installing pipenv dependencies, you may need to the pg_config path by running pg_config and then reinstalling it.\npg_config --bindir\nexport PATH=$PATH:path\/to\/pg\/config\npip3 install psycopg2\nLocally, we are using Pipfile and Pipfile.lock files to manage dependencies. On the staging and production servers, we are using a Dockerfile, which installs dependencies from a requirements.txt file.\nWe need to always make sure these are in sync. For instance, if you are pulling updates from dependabot, changes will be made to the Pipfile.lock file but will never make it over to the requirements.txt file, which is used on staging\/production. This is a big problem.\nA temporary workaround is to generate a requirements.txt file that's based on the Pipfile, using the pipenv-to-requirements package. You can do this simply by running:\npipenv run pipenv_to_requirements -f\nThis will generate a requirements.txt file in the root directory, which will be used by the Dockerfile when creating\/editing the containers for staging and production. It also creates a requirements-dev.txt file, which currently isn't being used.\nIn the future, we will be switching to using Docker locally to manage dependencies rather than relying on pipenv. Several files have been created to start this process:\nDockerfile.local\ndocker-entrypoint-local.sh\ndocker-compose.local.yml\nAt the moment, however, this is not working.\nNext, install npm packages:\nnpm install\nnpm run build\nIf this is your first time using the app, you'll probably want to load in last year's data to start off. These commands will drop your database (which doesn't exist yet), create a new one and run migrations before loading in the data:\nmake local\/reset-db\nsh bootstrap.sh\nbootstrap.sh is a compilation of commands from the Makefile that load in the latest data (for the state, regions, counties, districts, campuses, etc.) and create models from them.\nIf you're having trouble with the data, it might be because your .env file is not getting used. In that file is where we set up the DATA_FOLDER as explained in the setup doc. But you can also get around using that file by typing:\nexport DATA_FOLDER=~\/Documents\/tribune\/github\/scuole-house\/scuole-data\/\nThis should be run before running any commands that load in data.\nIf this is not your first time loading the app, you can run this to catch up with any outstanding migrations you might have:\npython manage.py migrate\nSome of the data we load will be pulled from the AskTed website. There may be data formatting errors with some of the data as its being pulled in. For instance, some of the phone numbers may be invalid. Right now, we have a phoneNumberFormat function in the updatedistrictsuperintendents, updatecampusdirectory and updatecampusprincipals. You may need to edit this function or create new ones if you're running into problems loading the data from AskTed.\nOnce the data is loaded, you can run the following command:\nsh docker-entrypoint.sh\nThis will collect static files, as well as fire up a local server.\nIf that doesn't work, try:\npython manage.py collectstatic --noinput\npython manage.py runserver\nIf you make changes to the styles, you'll need to run npm run build again to rebuild the main.css file in the assets\/ folder that the\ntemplates reference. You'll also need to do a hard refresh in whatever\nbrowser you're running the explorer in to fetch the new styles.\nAll good? Let's go! There are also other commands in scuole's Makefile at your disposal so check them out.\nUpdating and deploying\nEver year, we need to update cohorts, TAPR, district boundaries, campus coordinates, and the entities files for districts and campuses. Ideally, we would update AskTED every quarter.\nMore specific instructions for updating each dataset are in the scuole-data repo README.\nChanges to the code\nIf you're making changes to just the code and not the data, first you need to push all your changes locally to Github. Then you can run:\nssh schools-test\ncd scuole\ngit checkout master\ngit pull\nmake compose\/test-deploy\nOnce you run these, make sure everything is working on the test url. If so, then you'll need to repeat those steps on the two production servers: schools-prod and schools-prod-2. You must do both servers \u2014 if you don't, the published app will switch between new and old code.\nssh schools-prod\ncd scuole\ngit pull\nmake compose\/production-deploy\nCongrats, your changes are now live!\nChanges to the data\nThere are two types of data updates. One type is when you manually download the data, format it, and load it into the appropriate folder in scuole-data. You'll then need to deploy the latest data in scuole-data (instructions in the deploy section).\nAnother type is when you run a command to download the latest data directly from the website. Updating AskTED is an example of this. You'll need to ssh onto the appropriate server (test or production) and run a series of commands to pull the latest data before deploying it to the server.\nSee scuole-data for more descriptions about the data used in scuole. scuole-data also has instructions on how to clean and download TAPR and cohorts data.\nIf you're making changes to the data, we will first deploy to the test server and then production. This process will involve getting into the Docker container on the server, loading in the new data and deploying it live.\nIf you're not set up with the ssh yet, check out this doc for more info.\nFirst, make sure all of your changes are pushed to Github. Then log into the server and pull those changes:\nssh schools-test\ncd scuole-data\ngit pull\ncd ..\/scuole\ngit checkout master\ngit pull \nNow let's get into the Docker container:\ndocker run -it --rm --volume=\/home\/ubuntu\/scuole-data:\/usr\/src\/app\/data\/:ro --net=scuole_default --entrypoint=ash --env-file=env-docker schools\/web\nAnd run the update to your data, which would look something like:\npython manage.py loadallcohorts 2008\nFor cohorts\nYou'll need to add a line to data\/all-cohorts in the Makefile in scuole with the latest year. Then, run\npython manage.py loadallcohorts <latest year> during the update.\nYou will also need to change the latest_cohort_year variable in the scuole\/cohorts\/views.py file to reference the latest cohorts school year.\nLastly, make sure the scuole\/cohorts\/schema\/cohorts\/schema.py has the correct years (i.e. you'll need to change the year in 8th Grade (FY 2009) for the reference 'enrolled_8th': '8th Grade (FY 2009)', along with the rest of the references.)\nFor AskTED\nRun make data\/update-directories to update the data. You can also run each command in that block separately, your choice!\nIf you are doing this locally, make sure you have run pipenv shell and are in the shell \u2014 otherwise you won't have the dependencies you need to run this successfully.\nIf you run into any duplicate key errors during the AskTED update, refer to the Troublshooting section below for instructions on how to clear a table.\nYou'll need to clear the table that is throwing this error, and reload the data. This error happens because the update may be trying to insert something that has the same ID as something else.\nFor TAPR\nRefer to this Confluence document.\nNow exit out of the python shell and your Docker container with Ctrl + P + Q. If you have code changes as well, you can push them live by running:\nmake compose\/test-deploy\nYour changes should now be on the test server! Now we're ready for production a.k.a. the big time.\nFortunately, you only need to push data changes to one server. For schools-prod, we will need to pull down Github changes:\nssh schools-prod\ncd scuole-data\ngit pull\ncd ..\/scuole\ngit pull\nIf your making data changes, the command to get into the Docker containers on the prod servers will change a little bit. You won't need the --net parameter anymore:\ndocker run -it --rm --volume=\/home\/ubuntu\/scuole-data:\/usr\/src\/app\/data\/:ro --entrypoint=ash --env-file=env-docker schools\/web\nNow go ahead make your changes. Here's an example:\npython manage.py loadallcohorts 2008\nAnd deploy:\nmake compose\/production-deploy\nOnce that's done, check the live site. Your changes should be there! Now go home, your work here is done.\nFor the sitemap\nWhen we add new urls, we also need to update the sitemap (sitemap.xml) to include those paths. Fortunately, Django has functions that allow us to generate all of the urls associated with an object's views.\nTo see an example, view any of the sitemaps.py files. You'll need to add the sitemap to the config\/urls.py file, and view the updated sitemap locally at localhost:8000\/sitemap.xml.\nAfter verifying that the sitemap looks OK locally, copy the content starting from the <urlset> tag in sitemap.xml and paste it into scuole\/static_src\/sitemap.xml before deploying. You can also run python manage.py collectstatic --noinput on the test and production servers to get the updated sitemap.\nTroubleshooting\nWhat is the best place to view the data?\nTablePlus is recommended. Hook it up to the scuole PostgreSQL database!\nI'm seeing models being imported that are not supposed to be.\nThe easiest solution is to follow the instructions from the answer for the duplicate key error and selectively delete some models. There are probably better solutions for this ...\nWhy is my operation getting killed when I run make compose\/test-deploy and make compose\/production-deploy?\nThis may be due to memory issues. It typically gets killed when packages are being installed in the Docker container. If you try a couple more times (wait a little), the application should deploy.\nDjango doesn't know where the the database is.\nTo ensure Django knows where to look, you may need to set the DATABASE_URL. If you are not using the Docker provided database, use DATABASE_URL to tell the app what you've done instead. We haven't needed to set this lately.\nexport DATABASE_URL=postgres:\/\/docker:docker@docker.local:5432\/docker\nIf you get the error django.db.utils.OperationalError: could not translate host name \"docker.local\" to address: nodename nor servname provided, or not known, unset DATABASE_URL in your environment by running unset DATABASE_URL in the terminal. From there, the app should be runnable as normal.\nI'm seeing a duplicate key error when loading new data into the database.\nSometimes an update can throw a duplicate key error. A brute force solution is clearing out all of the objects from the table, and running the update again.\n\nRun ssh schools-test. ALWAYS do this on the test server first.\nRun docker run -it --rm --volume=\/home\/ubuntu\/scuole-data:\/usr\/src\/app\/data\/:ro --net=scuole_default --entrypoint=ash --env-file=env-docker schools\/web to get into the test server Docker environment.\nRun python manage.py shell to get into the Python shell to run Django commands.\nIf you want to clear Superintendents, for example, you'd need to import the District models by running from scuole.districts.models import Superintendent in the Python shell. You can get the syntax for the import by going to the Python file in scuole that updates that particular model and coping the line of code importing it.\nSave the models in a variable: super = Superintendent.objects.all().\nprint(super) to make sure you're clearing the right thing.\nsuper.delete() to delete them all.\nexit() to exit out of the Python shell.\nRun your update command again.\n\nYou may need to run this process when updating cohorts data \u2014 if the cohorts data upload for the latest year is failing because there are too many regional or county cohorts, it may be because you've tried to upload the data more than once and there are duplicates.\nYou'll need to filter for the latest year by filtering for the objects with the highest year_id so you can delete them. You can find the highest year_id by looking at the objects in Table Plus. Then, you'll run:\nfrom scuole.counties.models import CountyCohorts\nlatest = CountyCohorts.objects.filter(year_id=14)\nprint(latest) # to check if these are the objects we want to delete\nlatest.delete()\n\nIf you see the error django.db.utils.OperationalError: could not translate host name \"db\" to address: Name does not resolve when deploying\/updating data, it could mean that the app doesn't know where to look for the database. Running make compose\/test-deploy does some of the setup, and might fix the issue.\nMy deployment isn't working because there are \"active endpoints\".\nIf you run into ERROR: error while removing network: network <network-name> id <network-id> has active endpoints while deploying to test or production, it means you need to clear out some lingering endpoints.\n\nRun docker network ls to get a list of networks.\nGrab the id of scuole_default and run docker network inspect <scuole_default-id>\nYou'll see a bunch of objects in the Container property. Each of them should have an endpoint ID and a name. The name is the endpoint name.\nRun docker network disconnect -f <scuole_default-id> <endpoint-name> for each of the endpoints.\nTry test deploying again. It should work this time around!\n\nSource of this information\nWorkspace\nThe workspace directory is used for incorporating the schools database with other datasets we run across in our reporting. These include:\n\nA-F scores\n\nFor this, we merge the slugs for campuses in our schools app with their A-F scores from TEA. This is done so we can link to their pages in the schools app when showing them in our grade lookup tool. The spreadsheet with A-F scores from TEA gets put into the raw_data manually. The other spreadsheet you'll need is one with slugs for each campus in our schools app. It can be generated by running:\npython manage.py exportslugs\nAfter those files are in the raw_data directory, run everything inside of analysis.ipynb to spit out a merged spreadsheet in the output directory, which will then be loaded into a Google spreadsheet and used with the lookup tool.\nAdmin\nThis likely won't have an admin interface, but you are welcome to use it to check out how things are getting loaded. First, you'll need to create a super user. (If you ever blow away your database, you'll have to do it again!)\npython manage.py createsuperuser\nThen, after a python manage.py runserver, you can visit http:\/\/localhost:8000\/admin and use the credentials you setup to get access. Every thing will be set to read-only, so there's no risk of borking anything.\n","2":"thermometer\n","3":"\nwalls\nThis queries Salesforce for opportunity information, massages it into a nice JSON format and then stores it in S3 where it will ultimately be accessed by browsers\/web app(s).\ntesting\nmake test\nrun it\nConfigure env file with Salesforce anw AWS variables.\nmake\nLicense\nThis project is licensed under the terms of the MIT license.\n","4":"GeoIP2\nA super simple, Node.js-based deployment for getting the location of a user with an IP address.\nLicense\nMIT\n","5":"@texastribune\/queso-tools\n\nNode task runners for compiling CSS, creating SVGs, and more.\n\nThis repo accompanies our CSS+icons framework, queso-ui. Use the the task runners here, to compile the assets in that framework.\nInstall\nnpm install @texastribune\/queso-tools --save-dev\nor\nyarn add @texastribune\/queso-tools --dev\nTools\n\n\n\nmodules\nparams\n\n\n\n\nstyles\ndirs, manifest (manifest is optional if you want files with hashed names)\n\n\nicons\ndirs\n\n\namp\ndirs\n\n\n\nUsing these tasks in the wild\nMost of the tasks expect an array of directories or files with an input, in: key, and output, out: key.\nTo set this up, create a file called paths.js and declare your map of paths.\nExample:\n\/\/ paths.js\nconst SCSS_DIR = '.\/scss';\nconst CSS_OUTPUT_DIR = '.\/css\/';\nconst SVG_LIB_DIR = '.\/node_modules\/@texastribune\/queso-ui\/icons\/base';\nconst SVG_OUTPUT_DIR = '.\/templates\/includes';\n\nconst CSS_MAP = [\n  {\n    in: `${SCSS_DIR}\/styles.scss`,\n    out: CSS_OUTPUT_DIR,\n  },\n  {\n    in: `${SCSS_DIR}\/styles2.scss`,\n    out: CSS_OUTPUT_DIR,\n  },\n];\n\/\/ The \"in\" key for icons should be an array; you can mix and match icons from @texastribune\/queso-ui and some stored locally\nconst SVG_MAP = [\n  {\n    in: [\n      `${SVG_LIB_DIR}\/twitter.svg`,\n      `${SVG_LIB_DIR}\/facebook.svg`,\n      '.\/icons\/custom-icon.svg',\n      '.\/icons\/other-icon.svg'\n    ],\n    out: `${SVG_OUTPUT_DIR}\/my-svg-sprite.html`,\n  },\n];\n\n\nconst AMP_MAP = [\n  {\n    in: `${SCSS_DIR}\/styles.scss`,\n    out: 'amp-include.html',\n  },\n];\n\n\/\/ use if you'd like the outputted CSS to have hashed file names\nconst MANIFEST_FILE = `${CSS_OUTPUT_DIR}styles.json`;\n\n\nmodule.exports = {\n  CSS_MAP,\n  SVG_MAP,\n  AMP_MAP,\n  MANIFEST_FILE,\n};\nNow create a build.js file in that same folder where you'll reference these paths and begin to call the various tasks in this package.\nThat could look something like the following:\n\/\/ build.js\nconst { styles, icons, amp } = require('@texastribune\/queso-tools');\nconst { CSS_MAP, MANIFEST_FILE, SVG_MAP, AMP_MAP } = require('.\/paths');\n\nasync function build() {\n  await styles(CSS_MAP, MANIFEST_FILE);\n  \/\/ OR (use await if you had to glob to get your map)\n  \/\/ const stylesArr = await CSS_MAP();\n  \/\/ await styles(stylesArr, MANIFEST_FILE);\n  await icons(SVG_MAP);\n  await amp(AMP_MAP);\n}\n\nbuild()\n  .catch((err) => {\n    \/\/ eslint-disable-next-line no-console\n    console.error(err.message);\n    process.exit(1);\n  });\nNow run node build.js in your local environment to fire the build script.\nPublishing\nMake sure you're authenticated for npm publishing.\n\nnpm login - then follow the prompts\nnpm run release\n\n","6":"=======================================\nConcurrency control with django-locking\nDjango has seen great adoption in the content management sphere, especially among the newspaper crowd. One of the trickier things to get right, is to make sure that nobody steps on each others toes while editing and modifying existing content. Newspaper editors might not always be aware of what other editors are up to, and this goes double for distributed teams. When different people work on the same content, the one who saves last will win the day, while the other edits are overwritten.\ndjango-locking provides a system that makes concurrent editing impossible, and informs users of what other users are working on and for how long that content will remain locked. Users can still read locked content, but cannot modify or save it.\ndjango-locking makes sure no two users can edit the same content at the same time, preventing annoying overwrites and lost time. Find the repository and download the code at http:\/\/github.com\/stdbrouw\/django-locking\ndjango-locking has only been tested on Django 1.2 and 1.3, but probably works from 1.0 onwards.\nDocumentation\nForked from the Django Locking plugin at stdbrouw\/django-locking, this code features the cream of the crop for django-locking combining features from over 4 repos!\nNew features added to this fork\nChanges on change list pages\nUnlock content object from change list page by simply clicking on the lock icon\n\n\nHover over the lock icon to see when the lock expires\n\n\nHover over the username by the lock icon to see the full name of the person who has locked the content object\n\n\nConsolidated username and lock icon into one column on change list page\nChanges in settings:\nAdded Lock warning and expiration flags in terms of seconds\nLock messages:\nAdded options to reload or save the object when lock expiration message is shown\n\nImproved look and feel for the lock messages\nLock messages fade in and out seamlessly\nAdded much more detail to let users know who the content object was locked by providing the username, first name and last name\nAdded lock expiration warnings\nShows how much longer the object is locked for in minutes\nLocking:\nAdded hard locking support using Django's validation framework\n\nSet hard and soft locking as the default to ensure the integrity of locking\nAdded seamless unlocking when lock expires\n\nArchitecture:\n1 model tracks lock information and that's it!  No messy migrations for each model that needs locking.\nRefactored and cleaned up code for easier maintainability\nSimplified installation by coupling common functionality into base admin\/form\/model classes\n10 Minute Install\n\n\nGet the code:\ngit clone git@github.com:RobCombs\/django-locking.git\n\n\nInstall the django-locking python egg:\ncd django-locking\nsudo python setup.py install\n\n\nAdd locking to the list of INSTALLED_APPS in project settings file:\nINSTALLED_APPS = ('locking',)\n\n\nAdd the following url mapping to your urls.py file:\nurlpatterns = patterns('',\n(r'^admin\/ajax\/', include('locking.urls')),\n)\n\n\nAdd locking to the admin files that you want locking for:\nfrom locking.admin import LockableAdmin\nclass YourAdmin(LockableAdmin):\nlist_display = ('get_lock_for_admin')\n\n\nAdd warning and expiration time outs to your Django settings file:\nLOCKING = {'time_until_expiration': 120, 'time_until_warning': 60}\n\n\nBuild the Lock table in the database:\ndjango-admin.py\/manage.py migrate locking (For south users. Recommended approach) OR\ndjango-admin.py\/manage.py syncdb (For non south users)\n\n\nInstall django-locking media:\ncp -r django-locking\/locking\/media\/locking $your static media directory\n\n\nNote: This is the step where people usually get lost.\nJust start up your django server and look for the 200\/304s http responses when the server attempts to load the media\nas you navigate to a model change list\/view page where you've enabled django-locking. If you see 404s, you put the media in the wrong directory!\nYou should see something like this in the django server console:\n[02\/May\/2012 15:33:20] \"GET \/media\/static\/locking\/css\/locking.css HTTP\/1.1\" 304 0\n[02\/May\/2012 15:33:20] \"GET \/media\/static\/web\/common\/javascript\/jquery-1.4.4.min.js HTTP\/1.1\" 304 0\n[02\/May\/2012 15:33:20] \"GET \/media\/static\/locking\/js\/jquery.url.packed.js HTTP\/1.1\" 304 0\n[02\/May\/2012 15:33:21] \"GET \/admin\/ajax\/variables.js HTTP\/1.1\" 200 114\n[02\/May\/2012 15:33:21] \"GET \/media\/static\/locking\/js\/admin.locking.js?v=1 HTTP\/1.1\" 304 0\n[02\/May\/2012 15:33:21] \"GET \/admin\/ajax\/redirects\/medleyobjectredirect\/14\/is_locked\/?_=1335987201245 HTTP\/1.1\" 200 0\n[02\/May\/2012 15:33:21] \"GET \/admin\/ajax\/redirects\/medleyobjectredirect\/14\/lock\/?_=1335987201295 HTTP\/1.1\" 200 0\nYou can also hit the media directly for troubleshooting your django-locking media installation:\nhttp:\/\/www.local.wsbradio.com:8000\/media\/static\/locking\/js\/admin.locking.js\nIf the url resolves, then you've completed this step correctly!\nBasically, the code refers to the media like so.  That's why you needed to do this step.\nclass Media:\njs = ( 'http:\/\/ajax.googleapis.com\/ajax\/libs\/jquery\/1.4.2\/jquery.min.js', \n     'static\/locking\/js\/jquery.url.packed.js',\n     \"\/admin\/ajax\/variables.js\",\n     \"static\/locking\/js\/admin.locking.js?v=1\")\ncss = {\"all\": (\"static\/locking\/css\/locking.css\",)\n}\n\nThat's it!\nChecking the installation\nSimulate a lock situation -> Open 2 browsers and hit your admin site with one user logged into the 1st browser and\nother user logged into the other.  Go to the model in the admin that you've installed locking for with one browser.\nOn the other browser, go to the change list\/change view pages of the model that you've installed django-locking for.\nYou'll see locks in the interface similar to the screen shots above.\nYou can also look at your server console and you'll see the client making ajax calls to the django server checking for locks like so:\n[04\/May\/2012 15:15:09] \"GET \/admin\/ajax\/redirects\/medleyobjectredirect\/14\/is_locked\/?_=1336158909826 HTTP\/1.1\" 200 0\n[04\/May\/2012 15:15:09] \"GET \/admin\/ajax\/redirects\/medleyobjectredirect\/14\/lock\/?_=1336158909858 HTTP\/1.1\" 200 0\n\nOptional\nIf you'd like to enforce hard locking(locking at the database level), then add the LockingForm class to the same admin pages\nExample:\nfrom locking.forms import LockingForm\nclass YourAdmin(LockableAdmin):\n list_display = ('get_lock_for_admin')\n form = LockingForm\n\nNote: if you have an existing form and clean method, then call super to invoke the LockingForm's clean method\nExample:\nfrom locking.forms import LockingForm\nclass YourFormForm(LockingForm):\n  def clean(self):\n    self.cleaned_data = super(MedleyRedirectForm, self).clean()\n    ...some code\n    return self.cleaned_data\n\nCREDIT\nThis code is basically a composition of the following repos with a taste of detailed descretion from me. Credit goes out to the following authors and repos for their contributions\nand my job for funding this project:\nhttps:\/\/github.com\/stdbrouw\/django-locking\nhttps:\/\/github.com\/runekaagaard\/django-locking\nhttps:\/\/github.com\/theatlantic\/django-locking\nhttps:\/\/github.com\/ortsed\/django-locking\n","7":"\n  data-visuals-create\n\n\n\n\n\n\nA tool for generating the scaffolding needed to create a graphic or feature the Data Visuals way.\nKey features\n\n\ud83d\udcd0 HTML templating with a familiar, easy Jinja2-esque format via a modified instance of a Nunjucks environment that comes with all the functionality of journalize by default.\n\ud83c\udfa8 Supports SCSS syntax for styles compiled with the super fast reference implementation of Sass via dart-sass. All CSS is passed through autoprefixer and minified with clean-css in production.\n\ud83d\udce6 A configured instance of Webpack ready to go and optimized for a two-path modern\/legacy bundle approach. Ship lean ES2015+ code to modern browsers, and a functional polyfilled\/transpiled bundle to the rest!\n\ud83d\udcd1 Full-support for ArchieML formatted Google Docs and key\/value or table formatted Google Sheets. Use data you've collaborated on with reporters and editors directly in your templates.\n\ud83c\udf8a And so, so, so much more!\n\nGetting started\nnpx @data-visuals\/create feature my-great-project\ncd feature-my-great-project-YYYY-MM # the four digit year and two digit month\nnpm start\n\nWhile you can install @data-visuals\/create globally and use the data-visuals-create command, we recommend using the npx method instead to ensure you are always using the latest version.\n\nTable of contents\n\nInstallation\nUsage\nDevelopment and testing\nFolder structure\n\nconfig\/\ndata\/\nworkspace\/\nproject.config.js\napp\/\napp\/index.html, app\/static.html\napp\/templates\/\napp\/scripts\/\napp\/styles\/\napp\/assets\/\nOther directories you may see\n\n.tmp\/\ndist\/\n\n\n\n\nHow to work with Google Doc and Google Sheet files\n\nGoogle Docs\nGoogle Sheets\n\n\nSupported browsers\nHow do JavaScript packs work?\n\nCreating a new entrypoint\nConnecting an entrypoint to an HTML file\n\n\nAvailable commands\n\nnpm start or npm run serve\nnpm run deploy\nnpm run data:fetch\nnpm run assets:push\nnpm run assets:pull\nnpm run workspace:push\nnpm run workspace:pull\n\n\nEnvironment variables and authentication\n\nAWS\nGoogle\n\nCLIENT_SECRETS_FILE\nGOOGLE_TOKEN_FILE\n\n\n\n\nLicense\n\nInstallation\nWhile we recommend using the npx method, you can also install the tool globally. If you do, replace all instances of npx @data-visuals\/create you see with data-visuals-create.\nnpm install -g @data-visuals\/create\nUsage\nnpx @data-visuals\/create <project-type> <slug>\nCurrently there are two project types available \u2014 graphic and feature.\nnpx @data-visuals\/create graphic school-funding\nThis will create a directory for you, copy in the files, install the dependencies, and do your first git commit.\nThe directory name will be formatted like this:\n<project-type>-<slug>-<year>-<month>\n\nUsing the example command above, it would be the following:\ngraphic-school-funding-2018-01\n\nThis is to ensure consistent naming of our directories!\nDevelopment and testing\nIf you make changes locally to @data-visuals\/create and want to test them, you can run data-visuals-create\/bin\/data-visuals-create <project-type> <slug> to generate a graphic or feature and see if your changes were included. Run the command one level above this repo, or you'll create a graphic or feature within data-visuals-create.\nFolder structure\nAfter creation, your project directory should look something like this:\nyour-project\/\n  README.md\n  node_modules\/\n  config\/\n  data\/\n  workspace\/\n  package.json\n  project.config.js\n  app\/\n    index.html\n    templates\/\n    styles\/\n    scripts\/\n    assets\/\n\nHere are the highlights of what each file\/directory represents:\nconfig\/\nThis is the directory of all the configuration and tasks that power the kit. You probably do not need to ever go in here! (And eventually this will be abstracted away.)\ndata\/\nWhere data downloaded and processed with npm run data:fetch ends up. You are also free to manually (or via your own scripts!) put data files here - they will get pulled in too! Be aware that the only compatible data files that belong here are ones that quaff knows how to consume, otherwise it will ignore them.\nworkspace\/\nThe workspace directory is for storing all of your analysis, production and raw data files. It's important to use this directory for these files (instead of app\/assets\/ or data\/) so we can keep them out of GitHub and away from other parts of the kit. You interact with it using the npm run workspace:push and npm run workspace:pull commands.\nproject.config.js\nWhere all the configuration for a project belongs. This is where you can change the S3 deploy parameters, manage the Google Drive documents that sync with this project, set up a bespoke API or add custom filters to Nunjucks.\napp\/\nWhere you'll spend most of your time! Here are where all the assets that go into building your project live.\napp\/index.html, app\/static.html\nThe starter HTML pages provided by the kit. index.html is for scripted graphics that require additional JavaScript, and static.html is for graphics that do not, like Illustrator embeds. Feel free to rename them!\nIf your project is only a single page (or graphic), you can pick one of them where you do all your HTML work. No special configuration is required to create new HTML files - just creating a new .html file in in the app directory (but not within app\/scripts\/ or \/app\/templates\/ - HTML files have special meanings in those directories) is enough to tell the kit about new pages it should compile.\nWhen embedding graphics other than index.html, remember to add the name of the template to the end of the embed link. The default link points to index.\napp\/templates\/\nWhere all the Nunjucks templates (including the base.html template that app\/index.html inherits from), includes and macros live.\napp\/scripts\/\nWhere all of our JavaScript files live. Within this folder there are a number of helpful utilities and scripts we've created across tons of projects. JavaScript imports work as you'd expect, but the app\/scripts\/packs\/ directory is special - learn more about it in the How do JavaScript packs work? section.\napp\/styles\/\nAll the SCSS files that are used to compile the CSS files live here. This includes all of our house styles and variables (app\/styles\/_variables.scss). app\/styles\/main.scss is the primary entrypoint - any changes you make will either need to be in this file or be imported into it.\napp\/assets\/\nWhere all other assets should live. This includes images, font files, any JSON or CSV files you want to directly interact with in your JavaScript - these files are post-processed and deployed along with the other production files. Be aware, anything in this directory will technically be public on deploy. Use workspace\/ or data\/ instead for things that shouldn't be public.\nOther directories you may see\n.tmp\/\nThis is a temporary folder where files compiled during development will be placed. You can safely ignore it.\ndist\/\nThis is the compiled project and the result of running npm run build.\nHow to work with Google Doc and Google Sheet files\n@data-visuals\/create projects support downloading ArchieML-formatted Google Docs and correctly-formatted Google Sheets directly from Google Drive for use within your templates. All files you want to use in your projects should be listed in project.config.js under the files key. You are not limited to one of each, either! (Our current record is seven Google Docs and two Google Sheets in a single project.)\n{ \/\/ ...\n  \/**\n    * Any Google Doc and Google Sheet files to be synced with this project.\n    *\/\n  files: [\n    {\n      fileId: '<the-document-id-from-the-url>',\n      type: 'doc',\n      name: 'text',\n    },\n    {\n      fileId: '<the-sheet-id-from-the-url>',\n      type: 'sheet',\n      name: 'data',\n    },\n  \/\/ ...\n}\nEach object representing a file needs three things:\nfileId\nThe fileId key represents the ID of a Google Doc or Google Sheet. This is most easily found in the URL of a document when you have it open in your browser.\ntype\nThe type key is used to denote whether this is a Google Doc (doc) or a Google Sheet (sheet). This controls how it gets processed.\nname\nThe name key controls what filename it will receive once it's put in the data\/ directory. So if the name is hello, it'll be saved to data\/hello.json.\nGoogle Docs\nArchieML Google Docs work as documented on the ArchieML site. This includes the automatic conversion of links to <a> tags!\nOur kit can display variables pulled in from Google Docs in the template. This is helpful when we want to show data in our text that is in the data\/ folder. Nunjucks finds the variable syntax (anything in curly braces) in our Google Doc text and displays the corresponding value.\nBy default, Nunjucks has access to every file in our data\/ folder as an object. For example, if there are two files in the data\/ folder named data.json and text.json respectively, it will be structured as:\n{\n  \"text\": {\n    \"title\": \"Phasellus venenatis dapibus ante, vel sodales sem blandit sed.\",\n  },\n  \"data\": {\n    \"keyvalue_sheet\": {\n      \"key1\": \"value1\",\n    }\n  }\n}\nYou can then reference values in this data object as a variable, i.e. {{ data.keyvalue_sheet.key1 }} in the Google Doc.\nYou can also pass in your own data object for Nunjucks to reference to the prose, raw and text macros. This will override any values in the default data object.\nGoogle Sheets\nGoogle Sheets processed by @data-visuals\/create may potentially require some additional configuration. Each sheet (or tab) in a Google Sheet is converted separately by the kit, and keyed-off in the output object by the name of the sheet.\nBy default it treats every sheet in a Google Sheet as being formatted as a table. In other words, every row is considered an item, and the header row determines the key of each value in a column.\nThe Google Sheets processor also supports a key-value format as popularized by copytext (and its Node.js counterpart). This treats everything in the first column as the key, and everything in the second column as the value matched to its key. Every other column is ignored.\nTo activate the key-value format, add :kv to the end of a sheet's filename. (For consistency you can also use :table to tell the processor to treat a sheet as a table, but it is not required due to it being the default.)\nIf there are any sheets you want to exclude from being processed, you can do it via two ways: hide them using the native hide mechanism in Google Sheets, or add :skip to the end of the sheet name.\nSupported browsers\n@data-visuals\/create projects use a two-prong JavaScript bundling method to ship a lean, modern bundle for evergreen browsers and and a polyfilled, larger bundle for legacy browsers. It uses the methods promoted in Philip Walton's Deploying ES2015+ Code in Production Today blog post and determines browser support based on whether a browser understands ES Module syntax. If a browser does, it gets the modern bundle. If it doesn't, it gets the legacy bundle.\nIn practice this means you mostly do not have to worry about it - as long as you're using the JavaScript packs correctly everything should just work. In terms of actual browsers, while we do still currently do a courtesy check of how things look in Internet Explorer 11, it's not considered a dealbreaker if a complicated feature or graphic does not work there and would require extensive work to ensure compatibility.\nFor CSS we currently pass the following to autoprefixer.\n\"browserslist\": [\"> 0.5%\", \"last 2 versions\", \"Firefox ESR\", \"not dead\"]\nHow do JavaScript packs work?\nProjects created with @data-visuals\/create borrow a Webpack approach from rails\/webpacker to manage JavaScript entrypoints without configuration. To get the right scripts into the right pages, you have to do two things.\nCreating a new entrypoint\nBy default every project will come with an entrypoint file located at app\/scripts\/packs\/main.js, but you're not required to only use that if it makes sense to have different sets of scripts for different pages. Any JavaScript file that exists within app\/scripts\/packs\/ is considered a Webpack entrypoint.\ntouch app\/scripts\/packs\/maps.js\n# Now the build task will create a new entrypoint called `maps`! Don't forget to add your code.\nConnecting an entrypoint to an HTML file\nBecause there's a lot more going on behind the scenes than just adding a <script> tag, you have to set a special variable in a template in order to get the right entrypoint into the right HTML file.\nSet jsPackName anywhere in the HTML file to the name of your entrypoint (without the extension) to route the right JavaScript files to it.\n{% set jsPackName = 'map' %}\n{# This is now using the new entrypoint we created above #}\nPack entrypoints can be used multiple times across multiple pages, so if your code allows for it feel free to add an entrypoint to multiple pages. (You can also add jsPackName to the base app\/templates\/base.html file and have it inserted in every page that inherits from it).\nAvailable commands\nAll project templates share the same build commands.\nnpm start or npm run serve\nThe main command for development. This will build your HTML pages, prepare your SCSS files and compile your JavaScript. A local server is set up so you can view the project in your browser.\nnpm run deploy\nThe main command for deployment. It will always run npm run build first to ensure the compiled version is up-to-date. Use this when you want to put your project online. This will use the bucket and folder values in the project.config.js file to determine where it should be deployed on S3. Make sure those are set the appropriate values!\nnpm run data:fetch\nThis command uses the array of files listed under the files key in project.config.js to download data to the project. This data will be processed and made available in the data folder in the root of the project.\nYou can also set dataDir in project.config.js to change the location of that directory if necessary.\nnpm run assets:push\nThis pushes all the raw files found in the app\/assets directory to S3 to a raw_assets directory. This makes it possible for collaborators on the project to sync up with your assets when they run npm run assets:pull. This prevents potentially large assets like photos and audio clips from ending up in GitHub. This also runs automatically when npm run deploy is used.\nnpm run assets:pull\nPulls any raw assets that have been pushed to S3 back down to the project's app\/assets directory. Good for ensuring you have the same files as anyone else who is working on the project.\nnpm run workspace:push\nThe workspace directory is for storing all of your analysis, production and raw data files. It's important to use this directory for these files (instead of assets or data) so we can keep them out of GitHub. This command will push the contents of the workspace directory to S3.\nnpm run workspace:pull\nPulls any workspace files that have been pushed to S3 back down to the project's local workspace directory. This is helpful for ensuring you're in sync with another developer.\nEnvironment variables and authentication\nAny projects created with data-visuals-create assume you're working within a Texas Tribune environment, but it is possible to point AWS (used for deploying the project and assets to S3) and Google's API (used for interfacing with Google Drive) at your own credentials.\nAWS\nProjects created with data-visuals-create support two of the built-in ways that aws-sdk can authenticate. If you are already set up with the AWS shared credentials file (and those credentials are allowed to interact with your S3 buckets), you're good to go. aws-sdk will also recognize the AWS credential environmental variables.\nGoogle\nThe interface with Google Drive within data-visuals-create projects currently only supports using Oauth2 credentials to speak to the Google APIs. This requires a set of OAuth2 credentials that will be used to generate and save an access token to your computer. data-visuals-create projects have hardcoded locations for the credential file and token file, but you may override those with environmental variables.\nCLIENT_SECRETS_FILE\ndefault: ~\/.tt_kit_google_client_secrets.json\nGOOGLE_TOKEN_FILE\ndefault: ~\/.google_drive_fetch_token\nLicense\nMIT\n","8":"\ud83c\udf2e TacoBots  \ud83c\udf2e\nStyleBot\n\nThere once was a bot who knew,\nExactly what to do.\nWhen it got a command,\nIt explained what was banned,\nSo confusion would not ensue.\n--Avriana Allen\n\nStyleBot is a custom slash command that runs as a serverless function on AWS Lambda. Built on the Slash Commands App, the method can be as easy to inpliment as setting four variables in Lambda.\nHoedown Helper\n\nA bot wore a hat with style.\nIt never lacked a smile.\nPerhaps this is why,\nThe mistakes it would spy,\nIt always announced without guile.\n--Avriana Allen\n\nThe Hoedown Helper is a Slack App. It runs as a serverless function on AWS Lambda which only triggers when a message is posted to a channel that Hoedown Helper is part of.\nLicense\nMIT\n","9":"ValidatesTimeliness\n\n\n\nDescription\nComplete validation of dates, times and datetimes for Rails 3.x and\nActiveModel.\nThis is a Rails 4.2-compatible fork of\n[ jc-validates_timeliness gem] by [johncarney].\nwhich is a fork of the\noriginal validates_timeliness gem by Adam Meehan.\nFeatures\n\nAdds validation for dates, times and datetimes to ActiveModel\nHandles timezones and type casting of values for you\nOnly Rails date\/time validation plugin offering complete validation (See\nORM\/ODM support)\nUses extensible date\/time parser (Using\ntimeliness gem. See Plugin Parser)\nAdds extensions to fix Rails date\/time select issues (See Extensions)\nSupports I18n for the error messages\nSupports all the Rubies (that any sane person would be using in production).\n\nInstallation\n# in Gemfile\ngem 'jc-validates_timeliness'\n\n# Run bundler\n$ bundle install\n\nThen run\n$ rails generate validates_timeliness:install\n\nThis creates configuration initializer and locale files. In the initializer,\nthere are a number of config options to customize the plugin.\nNOTE: You may wish to enable the plugin parser and the extensions to start.\nPlease read those sections first.\nExamples\nvalidates_datetime :occurred_at\n\nvalidates_date :date_of_birth, :before => lambda { 18.years.ago },\n                               :before_message => \"must be at least 18 years old\"\n\nvalidates_datetime :finish_time, :after => :start_time # Method symbol\n\nvalidates_date :booked_at, :on => :create, :on_or_after => :today # See Restriction Shorthand.\n\nvalidates_time :booked_at, :between => ['9:00am', '5:00pm'] # On or after 9:00AM and on or before 5:00PM\nvalidates_time :booked_at, :between => '9:00am'..'5:00pm' # The same as previous example\nvalidates_time :booked_at, :between => '9:00am'...'5:00pm' # On or after 9:00AM and strictly before 5:00PM\n\nvalidates_time :breakfast_time, :on_or_after => '6:00am',\n                                :on_or_after_message => 'must be after opening time',\n                                :before => :lunchtime,\n                                :before_message => 'must be before lunch time'\n\nUsage\nTo validate a model with a date, time or datetime attribute you just use the\nvalidation method\nclass Person < ActiveRecord::Base\n  validates_date :date_of_birth, :on_or_before => lambda { Date.current }\n  # or\n  validates :date_of_birth, :timeliness => {:on_or_before => lambda { Date.current }, :type => :date}\nend\n\nor even on a specific record, per ActiveModel API.\n@person.validates_date :date_of_birth, :on_or_before => lambda { Date.current }\n\nThe list of validation methods available are as follows:\nvalidates_date     - validate value as date\nvalidates_time     - validate value as time only i.e. '12:20pm'\nvalidates_datetime - validate value as a full date and time\nvalidates          - use the :timeliness key and set the type in the hash.\n\nThe validation methods take the usual options plus some specific ones to\nrestrict the valid range of dates or times allowed\nTemporal options (or restrictions):\n:is_at        - Attribute must be equal to value to be valid\n:before       - Attribute must be before this value to be valid\n:on_or_before - Attribute must be equal to or before this value to be valid\n:after        - Attribute must be after this value to be valid\n:on_or_after  - Attribute must be equal to or after this value to be valid\n:between      - Attribute must be between the values to be valid. Range or Array of 2 values.\n\nRegular validation options:\n:allow_nil    - Allow a nil value to be valid\n:allow_blank  - Allows a nil or empty string value to be valid\n:if           - Execute validation when :if evaluates true\n:unless       - Execute validation when :unless evaluates false\n:on           - Specify validation context e.g :save, :create or :update. Default is :save.\n\nSpecial options:\n:ignore_usec  - Ignores microsecond value on datetime restrictions\n:format       - Limit validation to a single format for special cases. Requires plugin parser.\n\nThe temporal restrictions can take 4 different value types:\n\nDate, Time, or DateTime object value\nProc or lambda object which may take an optional parameter, being the record\nobject\nA symbol matching a method name in the model\nString value\n\nWhen an attribute value is compared to temporal restrictions, they are\ncompared as the same type as the validation method type. So using\nvalidates_date means all values are compared as dates.\nConfiguration\nORM\/ODM Support\nThe plugin adds date\/time validation to ActiveModel for any ORM\/ODM that\nsupports the ActiveModel validations component. However, there is an issue\nwith most ORM\/ODMs which does not allow 100% date\/time validation by default.\nSpecifically, when you assign an invalid date\/time value to an attribute, most\nORM\/ODMs will only store a nil value for the attribute. This causes an issue\nfor date\/time validation, since we need to know that a value was assigned but\nwas invalid. To fix this, we need to cache the original invalid value to know\nthat the attribute is not just nil.\nEach ORM\/ODM requires a specific shim to fix it. The plugin includes a shim\nfor ActiveRecord and Mongoid. You can activate them like so\nValidatesTimeliness.setup do |config|\n\n  # Extend ORM\/ODMs for full support (:active_record, :mongoid).\n  config.extend_orms = [ :mongoid ]\n\nend\n\nBy default the plugin extends ActiveRecord if loaded. If you wish to extend\nanother ORM then look at the wiki page for more information.\nIt is not required that you use a shim, but you will not catch errors when the\nattribute value is invalid and evaluated to nil.\nError Messages\nUsing the I18n system to define new defaults:\nen:\n  errors:\n    messages:\n      invalid_date: \"is not a valid date\"\n      invalid_time: \"is not a valid time\"\n      invalid_datetime: \"is not a valid datetime\"\n      is_at: \"must be at %{restriction}\"\n      before: \"must be before %{restriction}\"\n      on_or_before: \"must be on or before %{restriction}\"\n      after: \"must be after %{restriction}\"\n      on_or_after: \"must be on or after %{restriction}\"\n\nThe %{restriction} signifies where the interpolation value for the\nrestriction will be inserted.\nYou can also use validation options for custom error messages. The following\noption keys are available:\n:invalid_date_message\n:invalid_time_message\n:invalid_datetime_message\n:is_at_message\n:before_message\n:on_or_before_message\n:after_message\n:on_or_after_message\n\nNote: There is no :between_message option. The between error message should be\ndefined using the :on_or_after and :on_or_before (:before in case when\n:between argument is a Range with excluded high value, see Examples) messages.\nIt is highly recommended you use the I18n system for error messages.\nPlugin Parser\nThe plugin uses the timeliness gem as a fast, configurable and\nextensible date and time parser. You can add or remove valid formats for\ndates, times, and datetimes. It is also more strict than the Ruby parser,\nwhich means it won't accept day of the month if it's not a valid number for\nthe month.\nBy default the parser is disabled. To enable it:\n# in the setup block\nconfig.use_plugin_parser = true\n\nEnabling the parser will mean that strings assigned to attributes validated\nwith the plugin will be parsed using the gem. See the wiki\nfor more details about the parser configuration.\nRestriction Shorthand\nIt is common to restrict an attribute to being on or before the current time\nor current day. To specify this you need to use a lambda as an option value\ne.g. `lambda { Time.current }. This can be tedious noise amongst your\nvalidations for something so common. To combat this the plugin allows you to\nuse shorthand symbols for often used relative times or dates.\nJust provide the symbol as the option value like so:\nvalidates_date :birth_date, :on_or_before => :today\n\nThe :today symbol is evaluated as lambda { Date.today }. The :now and :today\nsymbols are pre-configured. Configure your own like so:\n# in the setup block\nconfig.restriction_shorthand_symbols.update(\n  :yesterday => lambda { 1.day.ago }\n)\n\nDefault Timezone\nThe plugin needs to know the default timezone you are using when parsing or\ntype casting values. If you are using ActiveRecord then the default is\nautomatically set to the same default zone as ActiveRecord. If you are using\nanother ORM you may need to change this setting.\n# in the setup block\nconfig.default_timezone = :utc\n\nBy default it will be UTC if ActiveRecord is not loaded.\nDummy Date For Time Types\nGiven that Ruby has no support for a time-only type, all time type columns are\nevaluated as a regular Time class objects with a dummy date value set. Rails\ndefines the dummy date as 2000-01-01. So a time of '12:30' is evaluated as a\nTime value of '2000-01-01 12:30'. If you need to customize this for some\nreason you can do so as follows\n# in the setup block\nconfig.dummy_date_for_time_type = [2009, 1, 1]\n\nThe value should be an array of 3 values being year, month and day in that\norder.\nTemporal Restriction Errors\nWhen using the validation temporal restrictions there are times when the\nrestriction option value itself may be invalid. This will add an error to the\nmodel such as 'Error occurred validating birth_date for :before restriction'.\nThese can be annoying in development or production as you most likely just\nwant to skip the option if no valid value was returned. By default these\nerrors are displayed in Rails test mode.\nTo turn them on\/off:\n# in the setup block\nconfig.ignore_restriction_errors = true\n\nExtensions\nStrict Parsing for Select Helpers\nWhen using date\/time select helpers, the component values are handled by\nActiveRecord using the Time class to instantiate them into a time value. This\nmeans that some invalid dates, such as 31st June, are shifted forward and\ntreated as valid. To handle these cases in a strict way, you can enable the\nplugin extension to treat them as invalid dates.\nTo activate it, uncomment this line in the initializer:\n# in the setup block\nconfig.enable_multiparameter_extension!\n\nDisplay Invalid Values in Select Helpers\nThe plugin offers an extension for ActionView to allowing invalid date and\ntime values to be redisplayed to the user as feedback, instead of a blank\nfield which happens by default in Rails. Though the date helpers make this a\npretty rare occurrence, given the select dropdowns for each date\/time\ncomponent, but it may be something of interest.\nTo activate it, uncomment this line in the initializer:\n# in the setup block\nconfig.enable_date_time_select_extension!\n\nContributors\nTo see the generous people who have contributed code, take a look at the\ncontributors list.\nMaintainers\n\nJohn Carney\n\nLicense\nCopyright (c) 2008 Adam Meehan, released under the MIT license\n","10":"rbenv cookbook\nInstalls and manages your versions of Ruby and Gems in Chef with rbenv and ruby_build\n\nrbenv\nruby_build\n\nRequirements\n\nChef 10\nCentos \/ Redhat \/ Fedora \/ Ubuntu \/ Debian\nRuby >= 1.9\n\nUsage\nAdd a dependency on rbenv to your cookbook's metadata.rb\ndepends 'rbenv'\n\nInstalling rbenv and ruby_build\nTo install rbenv and ruby_build; Include each recipe in one of your cookbook's recipes\ninclude_recipe \"rbenv::default\"\ninclude_recipe \"rbenv::ruby_build\"\n\nInstalling rbenv-vars\nTo install rbenv-vars; Include this recipe in one of your cookbook's recipes\ninclude_recipe \"rbenv::rbenv_vars\"\n\nInstalling a Ruby\nAnd now to install a Ruby use the rbenv_ruby LWRP\nrbenv_ruby \"1.9.3-p194\"\n\nInstalling Gems for rbenv managed Rubies\nIf you'd like a specific Ruby installed by rbenv to include a Gem, say bundler, use the rbenv_gem LWRP\nrbenv_gem \"bundler\" do\n  ruby_version \"1.9.3-p194\"\nend\n\nBe sure to include a value for the ruby_version attribute so the gem is installed for the correct Ruby\nAttributes\nrbenv\n\nrbenv[:group_users]     - Array of users belonging to the rbenv group\nrbenv[:git_repository]  - Git url of the rbenv repository to clone\nrbenv[:git_revision]    - Revision of the rbenv repository to checkout\nrbenv[:install_prefix]  - Path prefix rbenv will be installed into\n\nruby_build\n\nruby_build[:git_repository] - Git url of the ruby_build repository to clone\nruby_build[:git_revision]   - Revision of the ruby_build repository to checkout\nruby_build[:prefix]         - Path prefix where ruby_build will be installed to\n\nRecipes\ndefault\nConfigures a node with a system wide rbenv accessible by users in the rbenv group\nruby_build\nInstalls ruby_build to a node which enables the rbenv_ruby LWRP to install Rubies to the node\nohai_plugin\nInstalls an rbenv Ohai plugin onto the node to automatically populate attributes about the rbenv installation\nResources \/ Providers\nrbenv_ruby\nInstall specified version of Ruby to be managed by rbenv\nActions\n\n\n\nAction\nDescription\nDefault\n\n\n\n\ninstall\nInstall the version of Ruby\nYes\n\n\n\nAttributes\n\n\n\nAttribute\nDescription\nDefault\n\n\n\n\nruby_version\nthe ruby version and patch level you wish to install\nname\n\n\nforce\ninstall even if this version is already present (reinstall)\nfalse\n\n\nglobal\nset this ruby version as the global version\nfalse\n\n\npatch\na url for rbenv install --patch to pull in\n\n\n\n\nUsing the patch attribute requires that patchutils is installed, so that it can use filterdiff to remove parts of the patch (the Changelog which usually causes merge conflicts).\nExamples\nInstalling Ruby 1.9.2-p290\nrbenv_ruby \"1.9.2-p290\"\n\nForcefully install Ruby 1.9.3-p0\nrbenv_ruby \"Ruby 1.9.3\" do\n  ruby_version \"1.9.3-p0\"\n  force true\nend\n\nInstall Ruby 2.1.1 with a patch for ubuntu 14.04 support\nrbenv_ruby \"2.1.1\" do\n  global true\n  patch \"https:\/\/bugs.ruby-lang.org\/projects\/ruby-trunk\/repository\/revisions\/45225\/diff?format=diff\"\nend\n\nrbenv_gem\nInstall specified RubyGem for the specified ruby_version managed by rbenv\nActions\n\n\n\nAction\nDescription\nDefault\n\n\n\n\ninstall\nInstall the gem\nYes\n\n\nupgrade\nUpgrade the gem to the given version\n\n\n\nremove\nRemove the gem\n\n\n\npurge\nPurge the gem and configuration files\n\n\n\n\nAttributes\n\n\n\nAttribute\nDescription\nDefault\n\n\n\n\npackage_name\nName of given to modify\nname\n\n\nruby_version\nRuby of version the gem belongs to\n\n\n\nversion\nVersion of the gem to modify\n\n\n\nsource\nSpecified if you have a local .gem file to install\n\n\n\ngem_binary\nOverride for path to gem command\n\n\n\nresponse_file\n\n\n\n\noptions\nAdditional options to the underlying gem command\n\n\n\n\nExamples\nInstalling Bundler for Ruby 1.9.2-p290\nrbenv_gem \"bundler\" do\n  ruby_version \"1.9.2-p290\"\nend\n\nrbenv_execute\nSafely execute shell commands with RBENV and a particular Ruby activated\nActions\n\n\n\nAction\nDescription\nDefault\n\n\n\n\nrun\nRun the command\nYes\n\n\n\nAttributes\n\n\n\nAttribute\nDescription\n\n\n\n\ncommand\nThe name of the command to be executed\n\n\ncreates\nIndicates that a command to create a file will not be run when that file already exists\n\n\ncwd\nThe current working directory from which a command is run\n\n\nenvironment\nA hash of environment variables. These will be automatically merged with the required RBENV environment variables\n\n\ngroup\nThe group name or group ID that must be changed before running a command\n\n\npath\nAn array of paths to use when searching for a command. These paths will be added to the command's environment $PATH and the required RBENV path variables\n\n\nreturns\nThe return value for a command. This may be an array of accepted values. An exception is raised when the return value(s) do not match\n\n\nruby_version\nThe version of Ruby to activate when running the command\n\n\ntimeout\nThe amount of time (in seconds) a command will wait before timing out\n\n\nuser\nThe user name or user ID that should be changed before running a command\n\n\numask\nThe file mode creation mask, or umask\n\n\n\nReleasing\n\n\nInstall the prerequisite gems\n $ bundle install\n\n\n\nIncrement the version number in the metadata.rb file\n\n\nRun the Thor release task to create a tag and push to the community site\n $ bundle exec thor release\n\n\n\nAuthors and Contributors\n\nJamie Winsor (jamie@vialstudios.com)\n\n","11":"Geogle\n\n\nRuby wrapper for the Geocoding and Directions services provided from the Google Maps API.\nInstallation\nAdd this line to your application's Gemfile:\ngem 'geogle'\n\nAnd then execute:\n$ bundle\n\nOr install it yourself as:\n$ gem install geogle\n\nUsage\nGeocoding\nGoogle geocoding documentation:\nhttps:\/\/developers.google.com\/maps\/documentation\/geocoding\/\nSetting parameters\nWhen creating the Geogle::Geocoder these are the setting parameters:\n\n\nsensor:\n\ntrue\nfalse (default)\n\n\n\nlanguage: ar, eu, bg, bn, ca, cs, da, de, el, en, en-AU, en-GB, es, eu, fa, fi, fil, fr, gl, gu, hi, hr, hu, id, it, iw, ja, kn, ko, lt, lv, ml, mr, nl, no, pl, pt, pt-BR, pt-PT, ro, ru, sk, sl, sr, sv, tl, ta, te, th, tr, uk, vi, zh-CN, zh-TW\n\n\nraw:\n\ntrue: returns the raw json that comes in the body from the response.\nfalse (default): returns the object created with auxiliar funtions.\n\n\n\nclient_id (required for business API): ID of the client. It starts with \"gme-\" prefix.\n\n\ncrypto_key (required for business API): Criptographic key.\n\n\nHere's more information about Google Maps API for Business:\nhttps:\/\/developers.google.com\/maps\/documentation\/business\/webservices\nData model\nBoth methods return an array of Geogle::Model::Place. Each place is composed by:\n\n\nCoordinates:\n\nlat: Float\nlng: Float\n\n\n\nArea:\n\nnortheast: Coordinates\nsouthwest: Coordinates\n\n\n\nGeometry:\n\nlocation: Coordinates\nlocation_type: String\nbounds: Area\nviewport: Area\n\n\n\naddress:\n\nstreet_number: String\nstreet: String\nlocality: String\narea_level_1: String\narea_level_1_code: String\narea_level_2: String\narea_level_2_code: String\ncountry: String\ncountry_code: String\nformatted: String\n\n\n\nBy address without an account\nclient = Geogle::Geocoder.new({ sensor: false, language: \"es\" })\nclient.address(\"Blasco Iba\u00f1ez, Valencia\")\nBy address making use of the components\nclient = Geogle::Geocoder.new({ sensor: false, language: \"es\" })\ncomponents = { country: 'ES' }\nclient.address(\"Blasco Iba\u00f1ez, Valencia\", components)\nAvailable components to be used can be found here:\nhttps:\/\/developers.google.com\/maps\/documentation\/geocoding\/#ComponentFiltering\nReverse geocoding (by latitude and longitude)\nGeogle::Geocoder.new.latlng(39.5073225, -0.2914778)\nUsing a business account\nclient = Geogle::Geocoder.new({ client_id: \"gme-client-id\", crypto_key: \"crypto-key\" })\nclient.latlng(39.5073225, -0.2914778)\nThe signature required to do the request will be appended in the URL.\nDirections\nGoogle directions documentation: https:\/\/developers.google.com\/maps\/documentation\/directions\/\nSetting parameters\nThe same as with Geocode.\nOptions\n\nmode: supporterd modes of transport (bicycling|walking|driving).\nwaypoints: specifies an array of waypoints to alter a route by routing it through the specified location.\nalternatives: if true returns more than one route.\navoid:\n\ntolls: the route should avoid toll roads\/bridges.\nhighways: the route should avoid highways,\n\n\nunits:\n\nmetric: usage of metric system, returning distances in meters and kilometers.\nimperial: usage of imperial system (British), returning distances in miles and feet.\n\n\nregion: the code of the country to search in.\n\nData model\nReturn an array of Geogle::Model::Route. Each route is composed by:\n\n\nTime:\n\nvalue: Integer\ntext: String\ntime_zone: String\n\n\n\nTextValue:\n\nvalue: Integer\ntext: String\n\n\n\nLeg:\n\nsteps: Step\ndistance: TextValue\nduration: TextValue\narrival_time: Time\ndeparture_time: Time\nstart_address: String\nend_address: String\nstart_location: Coordinates\nend_location: Coordinates\n\n\n\nRoute:\n\nsummary: String\nlegs: Array[Leg]\nwaypoint_order: Array[Integer]\nbounds: Area\ncopyrights: String\nwarnings: Array[String]\nhelper methods:\n\nduration: Total duration of all legs.\ndistance: Total distance of all legs.\norigin: Name of starting address.\ndestination: Name of destination address.\npath: Array of geo-location points that represents the entire path of all legs.\n\n\n\n\n\nSearch using address names for origin and destination\noptions = { region: \"de\", mode: \"driving\" }\nclient = Geogle::Directions.new\nclient.routes(\"Berlin\", \"Munich\", options)\nSearch using geo-locations for origin and destination\nclient = Geogle::Directions.new\nclient.routes(\"39.4699889,-0.3759178\", \"40.4167158,-3.7037799\")\nContributing\n\nFork it\nCreate your feature branch (git checkout -b my-new-feature)\nCommit your changes (git commit -am 'Add some feature')\nPush to the branch (git push origin my-new-feature)\nCreate new Pull Request\n\n","12":"pewtils\nPewtils is a package of useful programming utilities developed at the Pew Research Center \nover the years. Most of the functions in Pewtils can be found in the root module, while a \nhandful of submodules contain more specialized utilities for working with files, web \nresources, and regular expressions.\nInstallation\nTo install, you can use pip:\npip install git+https:\/\/github.com\/pewresearch\/pewtils#egg=pewtils\n\nOr you can install from source:\ngit clone https:\/\/github.com\/pewresearch\/pewtils.git\ncd pewtils\npython setup.py install\n\nInstallation Troubleshooting\nUsing 64-bit Python\nSome of our libraries require the use of 64-bit Python. If you encounter errors during installation \nthat are related to missing libraries, you may be using 32-bit Python. We recommend that you uninstall \nthis version and switch to a 64-bit version instead. On Windows, these will be marked with x86-64; you \ncan find the latest 64-bit versions of Python here.\nInstalling ssdeep\nssdeep is an optional dependency that can be used by the get_hash function in Pewtils. \nInstallation instructions for various Linux distributions can be found in the library's \ndocumentation. The ssdeep \nPython library is not currently compatible with Windows. \nInstalling ssdeep on Mac OS may involve a few additional steps, detailed below:\n\nInstall Homebrew\nInstall xcode\nxcode-select --install\n\n\nInstall system dependencies\nbrew install pkg-config libffi libtool automake\nln -s \/usr\/local\/bin\/glibtoolize \/usr\/local\/bin\/libtoolize\n\n\nInstall ssdeep with an additional flag to build the required libraries\nBUILD_LIB=1 pip install ssdeep\n\n\nIf step 4 fails, you may need to redirect your system to the new libraries by setting the following flags:\nexport LIBTOOL=`which glibtool`\nexport LIBTOOLIZE=`which glibtoolize`\n\nDo this and try step 4 again.\nNow you should be able to run the main installation process detailed above.\n\nDocumentation\nPlease refer to the official documentation for information on how to use this package.\nUse Policy\nIn addition to the license, Users must abide by the following conditions:\n\nUser may not use the Center's logo\nUser may not use the Center's name in any advertising, marketing or promotional materials.\nUser may not use the licensed materials in any manner that implies, suggests, or could otherwise be perceived as attributing a particular policy or lobbying objective or opinion to the Center, or as a Center endorsement of a cause, candidate, issue, party, product, business, organization, religion or viewpoint.\n\nIssues and Pull Requests\nThis code is provided as-is for use in your own projects. You are free to submit issues and pull requests with any questions or suggestions you may have. We will do our best to respond within a 30-day time period.\nRecommended Package Citation\nPew Research Center, 2020, \"pewtils\" Available at: github.com\/pewresearch\/pewtils\nAcknowledgements\nThe following authors contributed to this repository:\n\nPatrick van Kessel\nRegina Widjaya\nSkye Toor\nEmma Remy\nOnyi Lam\nBrian Broderick\nGalen Stocking\nDennis Quinn\n\nAbout Pew Research Center\nPew Research Center is a nonpartisan fact tank that informs the public about the issues, attitudes and trends shaping the world. It does not take policy positions. The Center conducts public opinion polling, demographic research, content analysis and other data-driven social science research. It studies U.S. politics and policy; journalism and media; internet, science and technology; religion and public life; Hispanic trends; global attitudes and trends; and U.S. social and demographic trends. All of the Center's reports are available at www.pewresearch.org. Pew Research Center is a subsidiary of The Pew Charitable Trusts, its primary funder.\nContact\nFor all inquiries, please email info@pewresearch.org. Please be sure to specify your deadline, and we will get back to you as soon as possible. This email account is monitored regularly by Pew Research Center Communications staff.\n","13":"Search Sampler\nThis is a package for collecting and analyzing Google Health API data using a large, rolling sample, which can be beneficial when making precise calculations. Instead of taking just one sample of all data points, this package gives users the option of retrieving several samples for each data point, which can later be computed as a single data point. It is a modified version of the script researchers used to collect data for Pew Research Center's report on the Flint water crisis, published on April 27, 2017. For more information on using this tool, see this post.\nAbout the Report\nThis repository contains a generalized version of code used for collecting and analyzing data from the Google Health API for Pew Research Center's project, \"Searching for News: The Flint Water Crisis\", published on April 27, 2017.\nThe project explored what aggregated search behavior can tell us about how news spreads and how public attention shifts in today's fractured information environment, using the water crisis in Flint, Michigan, as a case study.\nThe study delves into the kinds of searches that were most prevalent as a proxy for public interest, concerns and intentions about the crisis, and tracks the way search activity ebbed and flowed alongside real world events and their associated news coverage.\nResearchers collected the data via Google's Health API, to which the Center requested and gained special access for this project. For more information, read our Medium post on how we used Google Trends data to conduct our research. Note that this requires access to the Health API; to apply, click here.\nRequirements\n\nPython 2.7.x\nSee requirements.txt for required pip packages.\n\nInstallation\nInstall via pip:\npip install search_sampler\n\nInstructions\nNOTE: Use of this tool requires an API key from Google, with special access for the Health API. To request access, please contact the Google News Lab via this form.\nInitialization\nTo use this tool, initialize the class with the API Key and a set of search parameters, which include the search term, region, start and end of the search period, and the unit of time to search for (day, week, month). Every search also requires a name (search_name), which is used as a suffix to output files. Using the same search_name multiple times can let you concatenate new results to existing output when you call the save function.\nSearch parameters should be passed as a dictionary. For example:\napikey = ''\n\noutput_path = '' # Folder name in your current directory to save results. This will be created.\n\n# search params\nparams = {\n    # Can be any number of search terms, using boolean logic. See report methodology for more info.\n    'search_term':['cough'],\n\n    # Can be country, state, or DMA. States are US-CA. DMA are a 3 digit code; see Nielsen for info.\n    'region':'US-DC',\n\n    # Must be in format YYYY-MM-DD\n    'period_start':'2014-01-01',\n    'period_end':'2014-02-15',\n\n    # Options are day, week, month. WARNING: This has been extensively tested with week only.\n    'period_length':'week'\n}\n\nsample = SearchSampler(apikey, search_name, params, output_path=output_path)\n\nGetting Data\nThis package provides either a single sample of data or a set of rolling window samples (see Medium post for details).\nTo retrieve a single sample:\ndf_results = sample.pull_data_from_api()\n\nTo retrieve a rolling set of samples:\ndf_results = sample.pull_rolling_window(num_samples=num_samples)\n\nSaving Results\nTo save results, run the built-in save command:\nsample.save_file(df_results)\n\nSearchSampler also allows you to run the same search multiple times. When done on different days, the Health API returns a slightly different sample, giving you more observations and increasing your analytical power (see this Medium post for more information). These new results can then be appended to any previously saved results by adding the append parameter to save_file. If append is not set to True, existing results will be overwritten.\nsample.save_file(df_results, append=True)\n\nOutput\nThe results are saved in a CSV format in the folder in the output path\/region specified. The file name reflects the region and the specified search name. For example, if the output path is 'data', the region is 'US-CA', and the search name is 'flu', the file will be found in 'data\/US-CA\/US-CA-flu.csv.' This file can be opened by spreadsheet programs like Microsoft Excel and a range of statistical and computational tools. Note that if opened in Excel, the date fields may not be recognized, but this should not be a problem in statistical or computational tools, such as R or Python's pandas. Fields in the output file are:\n\nquery_time: time query was run\nsample: the number of this individual sample. Zero-indexed.\nterm: the list of terms searched on\ntimestamp: the specific period being searched\nvalue: the value from the Health API\n\nMethodological Note\nThis project, the first foray by the Center into the Google Health API, was as much an exploration of how analyses of search data can shed light on the public's response to news and events as it was a study of the Flint water crisis. The detailed methodology is an effort to openly share what we learned through this process.\nAcknowledgments\nThis report was made possible by The Pew Charitable Trusts. Pew Research Center is a subsidiary of The Pew Charitable Trusts, its primary funder. This report is a collaborative effort based on the input and analysis of a number of individuals and experts at Pew Research Center. Google's data experts provided valuable input during the course of the project, from assistance in understanding the structure of the data to consultation on methodological decisions. While the analysis was guided by our consultations with the advisers, Pew Research Center is solely responsible for the interpretation and reporting of the data.\nUse Policy\nIn addition to the license, Users must abide by the following conditions:\n\nUser may not use the Center's logo\nUser may not use the Center's name in any advertising, marketing or promotional materials.\nUser may not use the licensed materials in any manner that implies, suggests, or could otherwise be perceived as attributing a particular policy or lobbying objective or opinion to the Center, or as a Center endorsement of a cause, candidate, issue, party, product, business, organization, religion or viewpoint.\n\nRecommended Report Citation\nPew Research Center, April, 2017, \"Searching for News: The Flint Water Crisis\"\nRecommended Package Citation\nPew Research Center, September 2018, \"Search Sampler\" Available at: github.com\/pewresearch\/search_sampler\nRelated Pew Research Center Publications\n\n\nSeptember 13, 2018 \"Sharing the code we used to study the public's interest in the Flint water\u00a0crisis\"\n\n\nApril 27, 2017  \"Searching for News: The Flint Water Crisis\"\n\n\nApril 27, 2017  \"Using Google Trends data for research? Here are 6 questions to ask\"\n\n\nApril 27, 2017  \"Q&A: Using Google search data to study public interest in the Flint water crisis\"\n\n\nIssues and Pull Requests\nThis code is provided as-is for use in your own projects.  You are free to submit issues and pull requests with any questions or suggestions you may have. We will do our best to respond within a 30-day time period.\nAbout Pew Research Center\nPew Research Center is a nonpartisan fact tank that informs the public about the issues, attitudes and trends shaping the world. It does not take policy positions. The Center conducts public opinion polling, demographic research, content analysis and other data-driven social science research. It studies U.S. politics and policy; journalism and media; internet, science and technology; religion and public life; Hispanic trends; global attitudes and trends; and U.S. social and demographic trends. All of the Center's reports are available at www.pewresearch.org. Pew Research Center is a subsidiary of The Pew Charitable Trusts, its primary funder.\nContact\nFor all inquiries, please email info@pewresearch.org. Please be sure to specify your deadline, and we will get back to you as soon as possible. This email account is monitored regularly by Pew Research Center Communications staff.\n","14":"Kinja\nRuby gem for posting to Kinja (currently only supports Burner accounts)\nInstallation\nAdd this line to your application's Gemfile:\ngem 'kinja'\nAnd then execute:\n$ bundle\n\nOr install it yourself as:\n$ gem install kinja\n\nUsage\n# Create an instance of the kinja client\nclient = Kinja.new(\n  user: \"username\",\n  password: \"password\"\n)\n\n# Create a post\npost = client.create_post(\n  headline: '',                       # required\n  body: '<p>This is a post<\/p>',      # required\n  status: 'PUBLISHED',                # optional (default is \"DRAFT\")\n  replies: false                      # optional (default is true)\n)\n\n# Get a post\npost = client.get_post(\"http:\/\/gawker.com\/lapd-claims-the-jinx-had-nothing-to-do-with-robert-durs-1691730232\")\n\n# ...OR...\n\npost = client.get_post(\"1691730232\")\nPublishing update\n\nBump version in version.rb\nrake build\nrake release\n\nContributing\n\nFork it ( https:\/\/github.com\/adampash\/kinja\/fork )\nSetup .env file with a test burner account\nCreate your feature branch (git checkout -b my-new-feature)\nCommit your changes with tests (git commit -am 'Add some feature')\nPush to the branch (git push origin my-new-feature)\nCreate a new Pull Request\n\n","15":"chef-bcpc\nchef-bcpc is a set of Chef cookbooks that\nbuild a highly-available OpenStack cloud.\nThe cloud consists of head nodes (OpenStack controller services, Ceph Mons,\netc.) and work nodes (hypervisors).\nEach head node runs all of the core services in a highly-available manner. Each\nwork node runs the relevant services (nova-compute, Ceph OSDs, etc.).\nGetting Started\nThe following instructions will get chef-bcpc up and running on your local\nmachine for development and testing purposes.\nSee the [Hardware Deployment][Hardware Deployment] section for notes on how to\ndeploy the chef-bcpc on hardware.\nPrerequisites\n\nOS X or Linux\nQuad-core CPU that supports VT-x or AMD-V virtualization extensions\n32 GB of memory\n128 GB of free disk space\nVagrant 2.1+\nVirtualBox 5.2+\ngit, curl, rsync, ssh, jq, make, ansible\n\nNOTE: It is likely possible to build an environment with 16GB of RAM or less\nif one is willing to make slight modifications to the\nvirtual topology and\/or change some of the\nbuild settings and overrides.  However, we've opted to spec the minimum\nrequirements slightly more aggressively and target hosts with 32GB RAM or more\nto provide the best out-of-the-box experience.\nLocal Build\n\nReview virtual\/topology\/topology.yml for the topology you will build and\nmake changes as required, e.g. assign more or less RAM based on your topology\nand your build environment. Other topologies exist in the same directory.\nTo make changes to the virtual topology without dirtying the tree, copy the\nhardware.yml and\ntopology.yml to files named\nhardware.overrides.yml and topology.overrides.yml, respectively, and make\nchanges to them instead.\nIf a proxy server is required for internet access, set the variables TBD\nIf additional CA certificates are required (e.g. for a proxy), set the variables TBD\nFrom the root of the chef-bcpc git repository run the following command:\n\nCreate a Python virtual environment (virtualenv) and activate it\npython3 -mvenv venv\nsource venv\/bin\/activate\npip install PyYaml ansible netaddr pyOpenSSL pycryptodome\nTo create a virtualbox build (the default):\nmake generate-chef-databags\nmake create all\nTo create a libvirt build:\nvagrant plugin install vagrant-libvirt vagrant-mutate\nvagrant box add bento\/ubuntu-18.04\nvagrant mutate bento\/ubuntu-18.04 libvirt\nexport VAGRANT_DEFAULT_PROVIDER=libvirt\nmake create all\nYou may also want to change cpu model from qemu64 to kvm64 in\nansible\/playbooks\/roles\/common\/defaults\/main\/chef.yml\nchef_environment:\n  name: virtual\n  override_attributes:\n    bcpc:\n       nova:\n         cpu_config:\n           cpu_mode: custom\n           cpu_model: kvm64\n\nHardware Deployment\nTBD\nContributing\nCurrently, most development is done by a team at Bloomberg L.P. but we would\nlike to build a community around this project. PRs and issues are welcomed. If\nyou are interested in joining the team at Bloomberg L.P. please see available\nopportunities at the Bloomberg L.P. careers site.\nLicense\nThis project is licensed under the Apache 2.0 License - see the\nLICENSE.txt file for details.\nBuilt With\nchef-bcpc is built with the following open source software:\n\nAnsible\nApache HTTP Server\nBIRD\nCalico\nCeph\nChef\nConsul\netcd\nHAProxy\nMemcached\nOpenStack\nPercona XtraDB Cluster\nPowerDNS\nRabbitMQ\nUbuntu\nUnbound\nVagrant\nVirtualBox\n\nThanks to all of these communities for producing this software!\n","16":"Description\nThe locking resource cookbook provides resources to lock various other Chef resources. This can be used to  prevent a distributed environment from having multiple machines simultaneously executing a particular resource.\nUse cases envisioned:\n\nPrevent stampeding herds knocking over a particularly fragile end-point by serializing access\nPrevent all instances of a service going down en mass during configuration updates\nCommunicate state if a particular service is not coming back up (preventing a toxic-configuration from causing cascading failures) and still verifying if a process is restarted outside of Chef using process start time.\n\nRequirements\nThe Zookeeper gem and a Zookeeper cluster is required. While envisioned to use a generic synchronous state engineer, today Zookeeper is used for all lock coordination.\nAttributes\n\nnode[:locking_resource][:restart_lock][:root] - Zookeeper namespace under which all locks are created\nnode[:locking_resource][:restart_lock_acquire][:sleep_time] - Sleep time in (fractions-of) seconds between tries to acquire a lock for restart\nnode[:locking_resource][:restart_lock_acquire][:timeout] - Timeout in seconds before failing to acquire lock\nnode[:locking_resource][:skip_restart_coordination] - Flag to skip attempting lock coordination (will just assume lock was acquired and not block)\nnode[:locking_resource][:zookeeper_servers] - The default zookeeper quorum\n\nResources\n\nlocking_resource - The HWRP for achieving locking\n\nActions\n\n:serialize - Will run the requested action every Chef run as long as a lock can be acquired\n:serialize_process - Is like :serialize except that should it leave a stale lock, it also verifies if the specified process has restarted since the lock was acquired and if so, cleans-up the lock not restarting again\n\n:serialize\nWill run the requested action every Chef run as long as a lock can be acquired\nRelevant Attributes\n\n:resource - String name of resource to control (uses the same 'resource[name]' syntax as :notifies\/:subscribes)\n:perform - Action to perform on :resource when called (locked resources are often set to :nothing to avoid running outside of lock)\n:timeout - Optional timeout override for resources which might hold a lock particularly long\n:lock_data - Optional data to put in lock; (envisioned to provide ability to lock on a topology grouping e.g. a rack)\n\n:serialize_process\nWill run the requested action every Chef run as long as a lock can be acquired; provides extra features for a process affecting resource. Will verify that if the machine is found to be holding a stale lock and the process as restarted since the lock was taken out, the lock will be released with no action. This provides an ability for a service to fail restarting (e.g. due to an exogenous resource failure; like a disk), to take out a lock to prevent other like processes going down electively and to be cleared by an administrator (e.g. disk replaced) and for Chef to clear the condition automatically.\n\n:process_pattern - Takes a block of options to define the process to keep an eye on:\nfull_cmd - Boolean as to if ps(1) should use -ww and read the contents of cmd for a full command string search (otherwise it reads the contents of comm)\ncommand_string - The process string to search for (e.g. java)\nuser - The numeric UID or ASCII username string of the user running the process\nOnly user or command_string need be supplied (both may be)\n\nContributing\nContributions are welcomed! This cookbook tries to have rigorous testing to verify that locks are held and released as expected. The current process for kicking these off on a machine with ChefDK is:\n$ export PATH=\/opt\/chefdk\/embedded\/bin:$PATH\n$ bundler package --path=vendor\/cache\n$ berks vendor\n$ bundler exec rspec\n$ kitchen converge '.*'\n\n","17":"collectd-cookbook\n\n\n\n\nApplication cookbook which installs and configures the\ncollectd monitoring daemon.\nThis cookbook provides a dead-simple installation and configuration of\nthe collectd monitoring daemon. It provides two resources: the first\nis for managing the collectd system service, and the second is for\nconfiguring the daemon's plugins. Additionally, the\ncollectd_plugins cookbook may be used to configure many of the\ncommon plugins that ship with the daemon.\nIt is very important to note that distributions may ship different\nmajor versions of the package, but the following platforms are tested\nusing the integration tests via Test Kitchen.\n\nUbuntu ~> 10.04, 12.04, 14.04\nCentOS ~> 5.8, 6.4, 7.1\nRHEL ~> 5.8, 6.4, 7.1\n\nBasic Usage\nThe default recipe in this cookbook simply\nconfigures the monitoring daemon to run as a system service. The\nconfiguration for this service can be tuned using the\nnode attributes. Additionally, a resource is\nprovided to configure plugins for the daemon. After a plugin has been\nconfigured the daemon should be restarted.\nEnabling Syslog\nOne of the simplest plugins to enable is the collectd Syslog plugin\nwhich receives log messages from the daemon and dispatches them to the\nto syslog. This allows the daemon's logs to easily integrate with\nexisting UNIX utilities.\ncollectd_plugin 'syslog' do\n  options do\n    log_level 'info'\n    notify_level 'OKAY'\n  end\nend\nAdvanced Usage\nIn order to enable the full functionality of some of the more\nintrusive collectd plugins the daemon will need to run as the root\nuser. Since this is obviously a security risk it is not the default.\nTo achieve this behavior you're required to write a\nwrapper cookbook which overrides the service user with the proper\nroot user.\nnode.default['collectd']['service_user'] = node['root_user']\nnode.default['collectd']['service_group'] = node['root_group']\ninclude_recipe 'collectd::default'\n","18":"Nginx Cookbook\n\nApplication cookbook which installs and configures the nginx monitoring daemon. Currently it defaults to ubuntu.\nUsage\nSupports\n\nUbuntu\n\nCurrently this does not support sentinal but it will in due time.\nDependencies\n\n\n\nName\nDescription\n\n\n\n\npoise\nLibrary cookbook built to aide in writing reusable cookbooks.\n\n\npoise-service\nLibrary cookbook built to abstract service management.\n\n\n\nAttributes\nAll nginx default shipped settings are built directly into the resource and most if not all have default settings attached(same that come packaged with nginx). You can view all default settings\/attributes here Nginx Service Nginx Site Nginx Module.\nResources\/Providers\nnginx_service\nThis provider will setup install and setup nginx. It will not configure any sites just the service and default nginx.conf.\nnginx_service \"example\"\nIf you would like additional settings outside of the basic attributes listed here Nginx Service you would add them with the same syntax below:\nnginx_service \"www\" do\n  additional_options do\n    option1 'value1'\n    option2 'value2'\n  end\nend\nYou also have the ability your own nginx.conf entirely by specifying the source option:\nnginx_service \"www\" do\n  source \"somefile.erb\"\nend\n####nginx_site\nThis provider will create and enable(symbolic link to sites-enabled) by default for all nginx sites specified in the block. Ideally you will always pass a servername otherwise the default is example.com:\nnginx_site \"wwww\" do\n  servername \"www.company.com\"\n  notifies :restart, \"nginx_service[#{instance}]\", :immediately # This will work only when the site instance name and service instance name are alike. \nend\nIf you would like additional settings outside of the basic attributes listed here Nginx Site you would add them with the same syntax below:\nnginx_site \"www\" do\n  servername \"www.company.com\"\n  additional_options do\n    option1 'value1'\n    option2 'value2'\n  end\nend\nYou can also enable SSL and\/or force redirect for anything listening on 80:\nnginx_site \"www\" do\n  servername \"www.company.com\"\n  enable_ssl true\n  ssl_force_redirect true\nend\nIf you would like to add additional SSL options only that won't be apart of the main http service you can use ssl_additional_options block.\nLast but not least you can also bring your own config file and do whatever you like with that.\nnginx_site \"www\" do\n  source \"www.company.com.erb\"\nend\n####nginx_module\nThis provider will give you the ability to setup the config for a module. If the module needs to be pulled down from the internet or installed somehow you will need to add that code. The default erb template is simple and just accepts a hash. You will likely want to pass it a source other than my empty one. However you can view the options in the library file. Nginx Module.\nnginx_module \"new-module\" do \n  module_config do\n    option1 'value1'\n    option2 'value2'\n  end\nend\nLicense & Authors\n\nAuthor:: Anthony Caiafa (acaiafa1@bloomberg.net)\n\nCopyright 2015 Bloomberg Finance L.P.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n\n","19":"consul-cluster cookbook\nCluster cookbook which installs and configures a Consul cluster\nwith TLS certificates.\nThis cookbook utilizies the Consul application cookbook to\nbootstrap and maintain Consul server infrastructure. It is intended as\nan example usage of the wrapper cookbook pattern and should\nhopefully serve as a basis for production infrastructure.\nUsage\nThe default recipe in this cookbook is designed to install and\nconfigure the Consul agent to bootstrap a cluster. The minimum number\nof nodes necessary for the infrastructure is three. After three nodes\nhave been setup and configured Consul will bootstrap the Cluster.\nChef Vault and TLS\nBy default, the Consul Cluster cookbook uses Chef Vault to encrypt the\nTLS certificates and keys that are deployed to the nodes. This data\nshould only be capable of being decrypted by the nodes the Consul\nCluster itself.\n{\n    \"ca_certificate\": \"-----BEGIN CERTIFICATE-----\\nMIIE [...]\",\n    \"certificate\": \"-----BEGIN CERTIFICATE-----\\nMIIE [...]\",\n    \"private_key\": \"-----BEGIN RSA PRIVATE KEY-----\\nMIIE [...]\"\n}\nPreparing the Instance\nThere are many configuration options that are exposed using the\nConsul application cookbook that can be set in either a\nwrapper cookbook, a Chef environment or a\nPolicyfile. But it is good form to not keep\nsecret information inside of a repository (or not encrypted in the\nChef Server).\nThe following configuration file can be written to the instances to\nset cluster's datacenter and encryption key for the gossip\nprotocol. This file should be written to\n\/etc\/consul\/conf.d\/server.json on each of the nodes.\n{\n    \"datacenter\": \"us-east-1\",\n    \"encrypt\": \"RxNBbMMl3m\/cSOdjKDhATg==\"\n}\nUsing Policyfiles\nIt is extremely simple to use Chef Policyfiles to configure your\nthree nodes which formulates your Consul cluster. Using the chef update and chef push commands you can push the below policy to a\nChef Server. A more detailed explanation of how to use these commands\nis for the chef CLI documentation.\nname consul-cluster'\ndefault_source :community\ncookbook 'consul-cluster', '~> 2.0'\nrun_list 'consul-cluster::default'\n\noverride['consul']['version'] = '0.6.4'\noverride['consul']['config']['bootstrap_expect'] = 3\noverride['consul']['config']['start_join'] = %w{10.0.10.2 10.0.10.3 10.0.10.4}\n\nTest Kitchen\nIf you are looking to execute the integration tests you must first configure\nthe policyfile that ships with the test fixtures.\n~\/Projects\/consul-cluster-cookbook % chef install test\/fixtures\/policies\/default.rb\n~\/Projects\/consul-cluster-cookbook % kitchen converge ubuntu-1404\n","20":"cobbler-cookbook\nInstalls and configures Cobbler and Cobbler Web.\nSupported Platforms\n\nCentOS 6.5, 5.10\nUbuntu 12.04, 14.04\n\nAttributes\n\n\nKey\nType\nDescription\nDefault\n\n\n['cobbler']['root_password']\nString\nRoot password for Kickstart templates.\n`echo 'root' | shasum -a 512 -p`\n\n\n['cobbler']['user']['password']\nString\nRoot password for Kickstart templates.\n`echo 'cloud' | shasum -a 512 -p`\n\n\n['cobbler']['user']['name']\nString\nUNIX username\ncloud\n\n\n['cobbler']['user']['uid']\nInteger\nUNIX uid\n900\n\n\nUsage\ncobbler::default\nInclude cobblerd in your node's run_list:\n{\n  \"run_list\": [\n    \"recipe[cobblerd::default]\"\n  ]\n}\nThis installs Cobbler on your machine\ncobbler::source\nInclude cobblerd::source in your node's run_list:\n{\n  \"run_list\": [\n    \"recipe[cobblerd::default]\"\n  ]\n}\nThis builds Cobbler from source code\ncobbler::web\nInclude cobblerd::web in your node's run_list:\n{\n  \"run_list\": [\n    \"recipe[cobblerd::web]\"\n  ]\n}\nThis provides the Cobbler web interface\ncobbler::centos\nInclude cobblerd::centos in your node's run_list:\n{\n  \"run_list\": [\n    \"recipe[cobblerd::centos]\"\n  ]\n}\nThis provides a CentOS image via Cobbler\ncobbler::ubuntu\nInclude cobblerd::ubuntu in your node's run_list:\n{\n  \"run_list\": [\n    \"recipe[cobblerd::ubuntu]\"\n  ]\n}\nThis provides an Ubuntu image via Cobbler\nMaintainers\nAuthor:: Bloomberg Compute Architecture Group (compute@bloomberg.net)\nMaintainers\nTo build and test, one can run the following (this done using ChefDK binaries):\n\nbundler package\nkitchen verify '.*'\n\n","21":"bfd\nInstalls and configures OpenBFDD.\nSupported Platforms\n\nUbuntu 12.04, 14.04\nChef 11, 12\n\nAttributes\n\n\nKey\nType\nDescription\nDefault\n\n\n['bfd']['repo']['url']\nString\nGit repo for OpenBFDD.\nhttps:\/\/github.com\/dyninc\/OpenBFDD.git\n\n\n['bfd']['repo']['sha']\nString\nGit revision of the OpenBFD repo to pull down.\n895cfb523bb96b3ef199fc5916578482ccd528ee\n\n\n['bfd']['version']\nString\nv0.5.3\nVersion of OpenBFD to use in the package naming\n\n\n['bfd']['package']['short_name']\nString\nopenbfdd\nShort name of the package name\n\n\n['bfd']['package']['name']\nString\nopenbfdd_v0.5.3_amd64.pkg\nFull name of the package\n\n\n['bfd']['package']['dependencies']\nString\n\nDependencies of the package\n\n\n['bfd']['install_dir']\nString\n\/usr\/local\nLocation where package files install\n\n\n['bfd']['bin_dir']\nString\n\/home\/vagrant\/chef-bcpc\/bins\nLocation where package should be created\n\n\n['bfd']['owner']\nString\nroot\nOwner of package files\n\n\n['bfd']['group']\nString\nroot\nGroup ownership of package files\n\n\nUsage\nbfd::default\nInclude bfd in your node's run_list:\n{\n  \"run_list\": [\n    \"recipe[bfd::default]\"\n  ]\n}\ncobbler::install\nInclude bfd in your node's run_list:\n{\n  \"run_list\": [\n    \"recipe[bfd::install]\"\n  ]\n}\nMaintainers\nAuthor:: Bloomberg Compute Architecture Group (compute@bloomberg.net)\nTesting\nTo test the Chef 11 compatibility one needs the kitchen-chef_zero_berks_env gem\nThis can be installed via chef gem install kitchen-chef_zero_berks_env\nTest kitchen and Server Spec test the operation of this cookbook; one can run the tests with:\nkitchen verify\n","22":"\nKaminari   \u00b6 \u2191\nA Scope & Engine based, clean, powerful, customizable and sophisticated paginator for modern web app frameworks and ORMs\nFork details\u00b6 \u2191\nForked to allow passing arbitrary html attributes to pagination helpers as described: github.com\/leckylao\/kaminari\/commit\/fa3a7475c4569bfcda87c0aa91939a87e17384c5\nFeatures\u00b6 \u2191\nClean\u00b6 \u2191\nDoes not globally pollute Array, Hash, Object or AR::Base.\nEasy to use\u00b6 \u2191\nJust bundle the gem, then your models are ready to be paginated. No configuration required. Don't have to define anything in your models or helpers.\nSimple scope-based API\u00b6 \u2191\nEverything is method chainable with less \u201cHasheritis\u201d. You know, that's the Rails 3 way. No special collection class or anything for the paginated values, instead using a general AR::Relation instance. So, of course you can chain any other conditions before or after the paginator scope.\nCustomizable engine-based I18n-aware helper\u00b6 \u2191\nAs the whole pagination helper is basically just a collection of links and non-links, Kaminari renders each of them through its own partial template inside the Engine. So, you can easily modify their behaviour, style or whatever by overriding partial templates.\nORM & template engine agnostic\u00b6 \u2191\nKaminari supports multiple ORMs (ActiveRecord, DataMapper, Mongoid, MongoMapper) multiple web frameworks (Rails, Sinatra, Grape), and multiple template engines (ERB, Haml, Slim).\nModern\u00b6 \u2191\nThe pagination helper outputs the HTML5 <nav> tag by default. Plus, the helper supports Rails 3 unobtrusive Ajax.\nSupported versions\u00b6 \u2191\n\nRuby 1.8.7, 1.9.2, 1.9.3, 2.0.0, 2.1.1\n\nRails 3.0, 3.1, 3.2, 4.0, 4.1\n\nHaml 3+\n\nMongoid 2+\n\nMongoMapper 0.9+\n\nDataMapper 1.1.0+\n\nInstall\u00b6 \u2191\nPut this line in your Gemfile:\ngem 'kaminari'\n\nThen bundle:\n% bundle\nUsage\u00b6 \u2191\nQuery Basics\u00b6 \u2191\n\nthe page scope\nTo fetch the 7th page of users (default per_page is 25)\nUser.page(7)\n\n\nthe per scope\nTo show a lot more users per each page (change the per_page value)\nUser.page(7).per(50)\n\nNote that the per scope is not directly defined on the models but is just a method defined on the page scope. This is absolutely reasonable because you will never actually use per_page without specifying the page number.\nKeep in mind that per utilizes internally limit and so it will override any limit that was set previously\nUser.count                  # => 1000\na = User.limit(5).count     # => 5\nb = a.page(1).per(20).size  # => 20\n\n\nthe padding scope\nOccasionally you need to pad a number of records that is not a multiple of the page size.\nUser.page(7).per(50).padding(3)\n\nNote that the padding scope also is not directly defined on the models.\n\nGeneral configuration options\u00b6 \u2191\nYou can configure the following default values by overriding these values using Kaminari.configure method.\ndefault_per_page  # 25 by default\nmax_per_page      # nil by default\nmax_pages         # nil by default\nwindow            # 4 by default\nouter_window      # 0 by default\nleft              # 0 by default\nright             # 0 by default\npage_method_name  # :page by default\nparam_name        # :page by default\n\nThere's a handy generator that generates the default configuration file into config\/initializers directory. Run the following generator command, then edit the generated file.\n% rails g kaminari:config\n\nchanging page_method_name\nYou can change the method name page to bonzo or plant or whatever you like, in order to play nice with existing page method or association or scope or any other plugin that defines page method on your models.\n\nConfiguring default per_page value for each model\u00b6 \u2191\n\npaginates_per\nYou can specify default per_page value per each model using the following declarative DSL.\nclass User < ActiveRecord::Base\n  paginates_per 50\nend\n\n\nConfiguring max per_page value for each model\u00b6 \u2191\n\nmax_paginates_per\nYou can specify max per_page value per each model using the following declarative DSL. If the variable that specified via per scope is more than this variable, max_paginates_per is used instead of it. Default value is nil, which means you are not imposing any max per_page value.\nclass User < ActiveRecord::Base\n  max_paginates_per 100\nend\n\n\nControllers\u00b6 \u2191\n\nthe page parameter is in params[:page]\nTypically, your controller code will look like this:\n@users = User.order(:name).page params[:page]\n\n\nViews\u00b6 \u2191\n\nthe same old helper method\nJust call the paginate helper:\n<%= paginate @users %>\nThis will render several ?page=N pagination links surrounded by an HTML5 <nav> tag.\n\nHelpers\u00b6 \u2191\n\nthe paginate helper method\n<%= paginate @users %>\nThis would output several pagination links such as \u00ab First \u2039 Prev ... 2 3 4 5 6 7 8 9 10 ... Next \u203a Last \u00bb\n\nspecifying the \u201cinner window\u201d size (4 by default)\n<%= paginate @users, :window => 2 %>\nThis would output something like ... 5 6 7 8 9 ... when 7 is the current page.\n\nspecifying the \u201couter window\u201d size (0 by default)\n<%= paginate @users, :outer_window => 3 %>\nThis would output something like 1 2 3 4 ...(snip)... 17 18 19 20 while having 20 pages in total.\n\nouter window can be separately specified by left, right (0 by default)\n<%= paginate @users, :left => 1, :right => 3 %>\nThis would output something like 1 ...(snip)... 18 19 20 while having 20 pages in total.\n\nchanging the parameter name (:param_name) for the links\n<%= paginate @users, :param_name => :pagina %>\nThis would modify the query parameter name on each links.\n\nextra parameters (:params) for the links\n<%= paginate @users, :params => {:controller => 'foo', :action => 'bar'} %>\nThis would modify each link's url_option. :controller and :action might be the keys in common.\n\nAjax links (crazy simple, but works perfectly!)\n<%= paginate @users, :remote => true %>\nThis would add data-remote=\"true\" to all the links inside.\n\nspecifying an alternative views directory (default is kaminari\/)\n<%= paginate @users, :views_prefix => 'templates\/' %>\nThis would search for partials in app\/views\/templates\/kaminari. This option makes it easier to do things like A\/B testing pagination templates\/themes, using new\/old templates at the same time as well as better intergration with other gems sush as cells.\n\nthe link_to_next_page and link_to_previous_page helper method\n<%= link_to_next_page @items, 'Next Page' %>\nThis simply renders a link to the next page. This would be helpful for creating a Twitter-like pagination feature.\n\nthe page_entries_info helper method\n<%= page_entries_info @users %>\nThis renders a helpful message with numbers of displayed vs. total entries.\n\nI18n and labels\u00b6 \u2191\nThe default labels for 'first', 'last', 'previous', '\u2026' and 'next' are stored in the I18n yaml inside the engine, and rendered through I18n API. You can switch the label value per I18n.locale for your internationalized application. Keys and the default values are the following. You can override them by adding to a YAML file in your Rails.root\/config\/locales directory.\nen:\n  views:\n    pagination:\n      first: \"&laquo; First\"\n      last: \"Last &raquo;\"\n      previous: \"&lsaquo; Prev\"\n      next: \"Next &rsaquo;\"\n      truncate: \"&hellip;\"\n  helpers:\n    page_entries_info:\n      one_page:\n        display_entries:\n          zero: \"No %{entry_name} found\"\n          one: \"Displaying <b>1<\/b> %{entry_name}\"\n          other: \"Displaying <b>all %{count}<\/b> %{entry_name}\"\n      more_pages:\n        display_entries: \"Displaying %{entry_name} <b>%{first}&nbsp;-&nbsp;%{last}<\/b> of <b>%{total}<\/b> in total\"\nCustomizing the pagination helper\u00b6 \u2191\nKaminari includes a handy template generator.\n\nto edit your paginator\nRun the generator first,\n% rails g kaminari:views default\nthen edit the partials in your app's app\/views\/kaminari\/ directory.\n\nfor Haml users\nHaml templates generator is also available by adding the -e haml option (this is automatically invoked when the default template_engine is set to Haml).\n% rails g kaminari:views default -e haml\n\nthemes\nThe generator has the ability to fetch several sample template themes from the external repository (github.com\/amatsuda\/kaminari_themes) in addition to the bundled \u201cdefault\u201d one, which will help you creating a nice looking paginator.\n% rails g kaminari:views THEME\nTo see the full list of avaliable themes, take a look at the themes repository, or just hit the generator without specifying THEME argument.\n% rails g kaminari:views\n\nmultiple themes\nTo utilize multiple themes from within a single application, create a directory within the app\/views\/kaminari\/ and move your custom template files into that directory.\n% rails g kaminari:views default (skip if you have existing kaminari views)\n% cd app\/views\/kaminari\n% mkdir my_custom_theme\n% cp _*.html.* my_custom_theme\/\nNext, reference that directory when calling the paginate method:\n<%= paginate @users, :theme => 'my_custom_theme' %>\nCustomize away!\nNote: if the theme isn't present or none is specified, kaminari will default back to the views included within the gem.\n\nPaginating a generic Array object\u00b6 \u2191\nKaminari provides an Array wrapper class that adapts a generic Array object to the paginate view helper. However, the paginate helper doesn't automatically handle your Array object (this is intentional and by design). Kaminari::paginate_array method converts your Array object into a paginatable Array that accepts page method.\n@paginatable_array = Kaminari.paginate_array(my_array_object).page(params[:page]).per(10)\n\nYou can specify the total_count value through options Hash. This would be helpful when handling an Array-ish object that has a different count value from actual count such as RSolr search result or when you need to generate a custom pagination. For example:\n@paginatable_array = Kaminari.paginate_array([], total_count: 145).page(params[:page]).per(10)\n\nCreating friendly URLs and caching\u00b6 \u2191\nBecause of the page parameter and Rails 3 routing, you can easily generate SEO and user-friendly URLs. For any resource you'd like to paginate, just add the following to your routes.rb:\nresources :my_resources do\n  get 'page\/:page', :action => :index, :on => :collection\nend\n\nIf you are using Rails 4 or later, you can simplify route definitions by using `concern`:\nconcern :paginatable do\n  get '(page\/:page)', :action => :index, :on => :collection, :as => ''\nend\n\nresources :my_resources, :concerns => :paginatable\n\nThis will create URLs like \/my_resources\/page\/33 instead of \/my_resources?page=33. This is now a friendly URL, but it also has other added benefits\u2026\nBecause the page parameter is now a URL segment, we can leverage on Rails page caching!\nNOTE: In this example, I've pointed the route to my :index action. You may have defined a custom pagination action in your controller - you should point :action => :your_custom_action instead.\nSinatra\/Padrino support\u00b6 \u2191\nSince version 0.13.0, kaminari started to support Sinatra or Sinatra-based frameworks experimentally.\nTo use kaminari and its helpers with these frameworks,\nrequire 'kaminari\/sinatra'\n\nor edit gemfile:\ngem 'kaminari', :require => 'kaminari\/sinatra'\n\nThis line just enables model-side features, such as Model#page and Model#per. If you want to use view helpers, please explicitly register helpers in your Sinatra or Padrino app:\nregister Kaminari::Helpers::SinatraHelpers\n\nOr, you can implement your own awesome helper :)\nMore features are coming, and again, this is still experimental. Please let us know if you found anything wrong with the Sinatra support.\nFor more information\u00b6 \u2191\nCheck out Kaminari recipes on the GitHub Wiki for more advanced tips and techniques. github.com\/amatsuda\/kaminari\/wiki\/Kaminari-recipes\nQuestions, Feedback\u00b6 \u2191\nFeel free to message me on Github (amatsuda) or Twitter (@a_matsuda)  \u2607\u2607\u2607  :)\nContributing to Kaminari\u00b6 \u2191\nFork, fix, then send a pull request.\nTo run the test suite locally against all supported frameworks:\n% bundle install\n% rake spec:all\nTo target the test suite against one framework:\n% rake spec:active_record_40\nYou can find a list of supported spec tasks by running rake -T. You may also find it useful to run a specific test for a specific framework. To do so, you'll have to first make sure you have bundled everything for that configuration, then you can run the specific test:\n% BUNDLE_GEMFILE='gemfiles\/active_record_40.gemfile' bundle install\n% BUNDLE_GEMFILE='gemfiles\/active_record_40.gemfile' bundle exec rspec .\/spec\/requests\/users_spec.rb\nCopyright\u00b6 \u2191\nCopyright \u00a9 2011 Akira Matsuda. See MIT-LICENSE for further details.\n","23":"Chef-BCS\n\nDESCRIPTION\nInstalls and configures Ceph, a distributed network storage and filesystem designed to provide excellent performance, reliability, and scalability.\nThe current version is focused on installing and configuring Ceph for CentOS and RHEL.\nPrerequisites\n\nVagrant - https:\/\/www.vagrantup.com\/downloads.html  (for development or just spinning up VM version of cluster - not needed for bare metal cluster)\nVirtualBox - https:\/\/www.virtualbox.org\/wiki\/Downloads\nGit\n\nInstructions\n\nFork\/clone repo\nNavigate to [whatever path]\/ceph-bcs\/bootstrap\/vms\/vagrant directory\nLaunch Vagrant version to see how it works and to do development and testing by issuing .\/CEPH_UP command (in \/bootstrap\/vms\/vagrant directory)\n\nProcess (Vagrant)\nAssuming you're in the path mentioned in #2 above.\nTo start a normal build simply do the following (no proxy):\n\n.\/CEPH_UP\n\nNB: If you want to test the upstream ceph-chef cookbook then clone that repo, make your changes, copy your cloned repo into the cookbooks section of the is cloned repo and then run the following command to start the build and test:\n\n.\/CEPH_UP -d 0   <-- Run in debug mode\n\nNB: Behind firewall:\n\n.\/CEPH_UP -p [whatever your http(s) proxy url]\n\n\nOR\n\n\n.\/CEPH_UP -d 0 -p [whatever your http(s) proxy url]  <-- Run in debug mode\n\nWhat happens...\n\nDownload CentOS 7.1 box version from Chef Bento upstream (7.2 and 7.3 versions of the bento\/centos have sshd issues)\nDownload required cookbooks including ceph-chef which is the most important\nIssue vagrant up that creates 4 VMs (dynamic and part of yaml file in \/bootstrap\/vms directory)\nSpins down VMs and adds network adapters and interfaces, sets up folder sharing and start VMs again\nMounts shared folders (makes it easy to move cookbooks etc to VMs) and sets network and then setups up the bootstrap node ceph-bootstrap as a Chef Server\nSets up chef-client on all other VMs\nAdds roles for specific Ceph types such as ceph-mon and ceph-osd etc for the given VM\nUpdates the environment json file (contains all of your override values of the defaults - different one for vagrant.json, staging.json and\/or production.json) [Only vagrant.json is used in this repo. You will need to create the specific environment json file for your targeted environment]\nCreates the Ceph Monitors first (ceph-mon role)\nCreates the Ceph OSD nodes (ceph-osd role)\nCreates the Ceph RGW node (ceph-radosgw role)\nCreates the Ceph restapi node (ceph-restapi role)\nFinishes the cluster simply by enabling the services\n\nNodes (Vagrant) - Creates an S3 Ceph Object Store Example Cluster\nThese are the default names. You can can call them anything you want. The main thing is to keep them numbered and not named like a pet but instead, named like cattle :)\nceph-bootstrap - Bootstrap node that acts as the Chef Server, Repo Mirror (in some cases) and Cobbler Server\nceph-vm1 - VM that has the ceph-mon, ceph-osd and ceph-radosgw roles applied\nceph-vm2 - VM that has the ceph-mon and ceph-osd roles applied\nceph-vm3 - VM that has the ceph-mon and ceph-osd roles applied\nNOTE: ceph-bootstrap does NOT contain any ceph functionality\nRADOS Gateway (RGW) uses civetweb as the embedded web server. You can login to any VM and issue a simple curl command (i.e., curl localhost or curl ceph-vm1.ceph.example.com or curl ceph-vm1). The hosts file is updated on all three VMs to support FQDN and short names.\nLogin to VMs (Vagrant)\nMust be located in the [wherever root dir]\/bootstrap\/vms\/vagrant directory (vagrant keeps a .vagrant directory with node information in it)\n\nCommand(s):\nvagrant ssh ceph-bootstrap\nvagrant ssh ceph-vm1\nvagrant ssh ceph-vm2\nvagrant ssh ceph-vm3\nNOTE: These names can be changed in the [wherever root dir]\/bootstrap\/vms\/servers_config.yaml file.\n\nSidebar: Vagrant uses port forwarding on the first network adapter of a given VM it manages. It then uses ssh port on the localhost to make it simple on itself.\nHelper Scripts (used in development to break tasks into smaller units of work)\n\/bootstrap\/common\n\/bootstrap\/vms\n\/bootstrap\/vms\/vagrant\nNote: The only one you must call is CEPH_UP which starts the whole process from creation of VMs to running Ceph cluster\nFor documentation on how to use this cookbook, refer to the USAGE section.\nNote: The documentation is a WIP along with a few other features. This repo is actively managed.\nIf there are issues then please go to the ISSUES section in this repo.\nREQUIREMENTS\nChef\n>= 12.8+\nPlatform\nTested as working:\n\nUbuntu Trusty (16.04) [Still verifying updates work]\nCentOS (7.3)\nRHEL (7.3)\n\nCookbooks\n[IMPORTANT - Cookbook that everything else is based on]\nhttps:\/\/github.com\/ceph\/ceph-chef\nThe ceph cookbook requires the following cookbooks from Chef:\nhttps:\/\/supermarket.chef.io\/\n\npoise\npoise-service\napt\napache2\nyum\nceph-chef\nchef-client\nchef-handler\nchef-sugar\ncollectd\ncollectd_plugins\ncron\nfirewall\nlogrotate\nntp\nsudo\nwindows\nohai\nyum-epel\ncompat_resource\n\nGEMS\nThe following two GEMS will need to be pulled down and loaded onto the production nodes for envrionments that can't reach the outside. The bootstrap_prereqs.sh does this automatically.\n\nnetaddr-1.5.1\nchef-sugar-3.4.0\n\nTEMPLATES\nThe following templates are Jinja2 based templates. The jinja_render.py found in bootstrap\/templates reads the production yaml data files and runs through these files and builds the production.json, kickstart, linux grub and operations key files. The erb are Chef templates but the jinja_render script builds and puts those erb files in the template\/default area of the cookbook as part of the preprocess.\n\nbase_environment.json.j2\nbcs_bootstrap_rhel.ks.j2\nbcs_node_rhel_nonosd.ks.erb.j2\nbcs_node_rhel_osd.ks.erb.j2\nlinux.cfg.j2\noperations.pub.j2\n\nUSAGE\nCeph cluster design is beyond the scope of this README, please turn to the\npublic wiki, mailing lists, visit our IRC channel, or contact Red Hat:\nhttp:\/\/ceph.com\/docs\/master\nhttp:\/\/ceph.com\/resources\/mailing-list-irc\/\nThis cookbook can be used to implement a chosen cluster design. Most of the configuration is retrieved from node attributes, which can be set by an environment or by a wrapper cookbook. A basic cluster configuration will need most of the following attributes:\n\nnode['ceph']['config']['fsid'] - the cluster UUID\nnode['ceph']['config]'['global']['public network'] - a CIDR specification of the public network\nnode['ceph']['config]'['global']['cluster network'] - a CIDR specification of a separate cluster replication network\nnode['ceph']['config]'['global']['rgw dns name'] -  the main domain of the radosgw daemon\n\nMost notably, the configuration does NOT need to set the mon initial members, because the cookbook does a node search to find other mons in the same environment.\nThe other set of attributes that this recipe needs is node['ceph']['osd_devices'], which is an array of OSD definitions, similar to the following:\n\n{'device' => '\/dev\/sdb'} - Use a full disk for the OSD, with a small partition for the journal\n{'type' => 'directory', 'device' => '\/src\/node\/sdb1\/ceph'} - Use a directory, and have a small file for the journal\n{'device' => '\/dev\/sde', 'dmcrypt' => true} - Store the data encrypted by passing --dmcrypt to ceph-disk-prepare\n{'device' => '\/dev\/sdc', 'journal' => '\/dev\/sdd2'} - use a full disk for the OSD with a custom partition for the journal\n\nUsing a Policy Wrapper Cookbook\nTo automate setting several of these node attributes, it is recommended to use a policy wrapper cookbook. This allows the ability to use Chef Server cookbook versions along with environment version restrictions to roll out configuration changes in an ordered fashion.\nIt also can help with automating some settings. For example, a wrapper cookbook could peek at the list of harddrives that ohai has found and populate node['ceph']['osd_devices'] accordingly, instead of manually typing them all in:\nnode.override['ceph']['osd_devices'] = node['block_device'].each.reject{ |name, data| name !~ \/^sd[b-z]\/}.sort.map { |name, data| {'journal' => \"\/dev\/#{name}\"} }\nFor best results, the wrapper cookbook's recipe should be placed before the Ceph cookbook in the node's runlist. This will ensure that any attributes are in place before the Ceph cookbook runs and consumes those attributes.\nCeph Monitor\nCeph monitor nodes should use the ceph-mon role.\nIncludes:\n\nceph-chef::default\n\nCeph Metadata Server\nCeph metadata server nodes should use the ceph-mds role.\nIncludes:\n\nceph-chef::default\n\nCeph OSD\nCeph OSD nodes should use the ceph-osd role\nIncludes:\n\nceph-chef::default\n\nCeph RADOS Gateway\nCeph RADOS Gateway nodes should use the ceph-radosgw role\nATTRIBUTES\nGeneral\n\n\nnode['ceph']['search_environment'] - a custom Chef environment to search when looking for mon nodes. The cookbook defaults to searching the current environment\n\n\nnode['ceph']['branch'] - selects whether to install the stable, testing, or dev version of Ceph\n\n\nnode['ceph']['version'] - install a version of Ceph that is different than the cookbook default. If this is changed in a wrapper cookbook, some repository urls may also need to be replaced, and they are found in attributes\/repo.rb. If the branch attribute is set to dev, this selects the gitbuilder branch to install\n\n\nnode['ceph']['extras_repo'] - whether to install the ceph extras repo. The tgt recipe requires this\n\n\nnode['ceph']['config']['fsid'] - the cluster UUID\n\n\nnode['ceph']['config']['global']['public network'] - a CIDR specification of the public network\n\n\nnode['ceph']['config']['global']['cluster network'] - a CIDR specification of a separate cluster replication network\n\n\nnode['ceph']['config']['config-sections'] - add to this hash to add extra config sections to the ceph.conf\n\n\nnode['ceph']['user_pools'] - an array of pool definitions, with attributes name, pg_num and create_options (optional), that are automatically created when a monitor is deployed\n\n\nCeph MON\n\nnode['ceph']['config']['mon'] - a hash of settings to save in ceph.conf in the [mon] section, such as 'mon osd nearfull ratio' => '0.70'\n\nCeph OSD\n\nnode['ceph']['osd_devices'] - an array of OSD definitions for the current node\nnode['ceph']['config']['osd'] - a hash of settings to save in ceph.conf in the [osd] section, such as 'osd max backfills' => 2\nnode['ceph']['config']['osd']['osd crush location'] - this attribute can be set on a per-node basis to maintain Crush map locations\n\nCeph MDS\n\nnode['ceph']['config']['mds'] - a hash of settings to save in ceph.conf in the [mds] section, such as 'mds cache size' => '100000'\nnode['ceph']['cephfs_mount'] - where the cephfs recipe should mount CephFS\nnode['ceph']['cephfs_use_fuse'] - whether the cephfs recipe should use the fuse cephfs client. It will default to heuristics based on the kernel version\n\nCeph RADOS Gateway (RGW)\nNote: Only supports the newer 'civetweb' version of RGW (not Apache)\n\nnode['ceph']['radosgw']['api_fqdn'] - what vhost to configure in the web server\nnode['ceph']['radosgw']['admin_email'] - the admin email address to configure in the web server\nnode['ceph']['radosgw']['port'] - if set, connects to the radosgw fastcgi over this port instead of a unix socket\nnode['ceph']['config']['global']['rgw dns name'] -  the main domain of the radosgw daemon, to calculate the bucket name from a subdomain\n\nResources\/Providers\nceph_client\nThe ceph_client LWRP provides an easy way to construct a Ceph client key. These keys are needed by anything that needs to talk to the Ceph cluster, including RGW, CephFS, and RBD access.\nActions\n\n:add - creates a client key with the given parameters\n\nParameters\n\n:name - name attribute. The name of the client key to create. This is used to provide a default for the other parameters\n:caps - A hash of capabilities that should be granted to the client key. Defaults to { 'mon' => 'allow r', 'osd' => 'allow r' }\n:as_keyring - Whether the key should be saved in a keyring format or a simple secret key. Defaults to true, meaning it is saved as a keyring\n:keyname - The key name to register in Ceph. Defaults to client.#{name}.#{hostname}\n:filename - Where to save the key. Defaults to \/etc\/ceph\/ceph.client.#{name}.#{hostname}.keyring if as_keyring and \/etc\/ceph\/ceph.client.#{name}.#{hostname}.secret if not as_keyring\n:owner - Which owner should own the saved key file. Defaults to root\n:group - Which group should own the saved key file. Defaults to root\n:mode - What file mode should be applied. Defaults to '00640'\n\nceph_cephfs\nThe ceph_cephfs LWRP provides an easy way to mount CephFS. It will automatically create a Ceph client key for the machine and mount CephFS to the specified location. If the kernel client is used, instead of the fuse client, a pre-existing subdirectory of CephFS can be mounted instead of the root.\nActions\n\n:mount - mounts CephFS\n:umount - unmounts CephFS\n:remount - remounts CephFS\n:enable - adds an fstab entry to mount CephFS\n:disable - removes an fstab entry to mount CephFS\n\nParameters\n\n:directory - name attribute. Where to mount CephFS in the local filesystem\n:use_fuse - whether to use ceph-fuse or the kernel client to mount the filesystem. ceph-fuse is updated more often, but the kernel client allows for subdirectory mounting. Defaults to true\n:cephfs_subdir - which CephFS subdirectory to mount. Defaults to '\/'. An exception will be thrown if this option is set to anything other than '\/' if use_fuse is also true\n\nceph_pool\nThe ceph_pool LWRP provides an easy way to create and delete Ceph pools.\nIt assumes that connectivity to the cluster is setup and that admin credentials are available from default locations, e.g. \/etc\/ceph\/ceph.client.admin.keyring.\nActions\n\n:add - creates a pool with the given number of placement groups\n:delete - deletes an existing pool\n\nParameters\n\n:name - the name of the pool to create or delete\n:pg_num - number of placement groups, when creating a new pool\n:create_options - arguments for pool creation (optional)\n:force - force the deletion of an exiting pool along with any data that is stored in it\n\nDEVELOPING\nStyle Guide\nThis cookbook requires a style guide for all contributions. Travis will automatically verify that every Pull Request follows the style guide.\n\nInstall ChefDK\nActivate ChefDK's copy of ruby: eval \"$(chef shell-init bash)\"\nbundle install\nbundle exec rake style\n\nTesting\nThis cookbook uses Test Kitchen to verify functionality. A Pull Request can't be merged if it causes any of the test configurations to fail.\n\nInstall ChefDK\nActivate ChefDK's copy of ruby: eval \"$(chef shell-init bash)\"\nbundle install\nbundle exec kitchen test aio-debian-74\nbundle exec kitchen test aio-ubuntu-1204\nbundle exec kitchen test aio-ubuntu-1404\n\nLICENSE\n\n\nAuthor: Chris Jones cjones303@bloomberg.net\n\n\nCopyright 2017, Bloomberg Finance L.P.\n\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\nhttp:\/\/www.apache.org\/licenses\/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n","24":"zookeeper-cluster-cookbook\n\n\n\n\nApplication cookbook which installs and configures\nApache Zookeeper.\nApache Zookeeper is a highly-available, centralized service which is\ncommonly used for maintaining configuration information, distributed\nservice discovery and providing coordination services. This cookbook\ntakes a simplified approach towards configuring Apache Zookeeper.\nBasic Usage\nThis cookbook was designed from the ground up to make it dead simple\nto install and configure a Zookeeper cluster using Chef. It also highlights\nseveral of our best practices for developing reusable Chef cookbooks\nat Bloomberg.\nThis cookbook provides node attributes which\ncan be used to fine tune the default recipe which installs and\nconfigures Zookeeper. The values from the node attributes are passed\ndirectly into the configuration and service resources.\nOut of the box the following platforms are certified to work and\nare tested using our Test Kitchen configuration. Additional platforms\nmay work, but your mileage may vary.\n\nCentOS (RHEL) 6.6, 7.1\nUbuntu 12.04, 14.04\n\nThe correct way to use this cookbook is to create a\nwrapper cookbook which configures all of the members of the\nZookeeper ensemble (cluster). We do this by using a data bag for each\nChef environment. The default recipe in your wrapper cookbook may\nlook something like the following block:\nbag = data_bag_item('config', 'zookeeper')[node.chef_environment]\nnode.default['zookeeper-cluster']['config']['instance_name'] = node['fqdn']\nnode.default['zookeeper-cluster']['config']['ensemble'] = bag\ninclude_recipe 'zookeeper-cluster::default'\nThe data bag for the above block should have an array of\nfully-qualified hostnames, the exact ones that appear in\nnode['fqdn'], which represent the members of the Zookeeper\nensemble. These hostnames are used when configuring the Zookeeper\nservice on each node.\n{\n  \"id\": \"zookeeper\",\n  \"development\": [\n    \"zk1.dev.inf.example.com\",\n    \"zk2.dev.inf.example.com\",\n    \"zk3.dev.inf.example.com\"\n  ],\n  \"production\": [\n    \"zk1.prod.inf.example.com\",\n    \"zk2.prod.inf.example.com\",\n    \"zk3.prod.inf.example.com\",\n    \"zk4.prod.inf.example.com\",\n    \"zk5.prod.inf.example.com\",\n  ]\n}\n","25":"confd-cookbook\n\n\n\n\nApplication cookbook which installs and configures confd.\nIt is often the case that application configuration files must be\ndynamically generated, distributed to a set of machines and a service\nmust be kicked to reload the changes. This faculty can be made to work\nwith Chef, but using confd offers the ability for an immediate\nconfiguration change and subsequent bounce of a service across your\nfleet.\nBasic Usage\nThe default recipe writes a basic configuration out for confd using\nnode attributes that can be modified by changing any of the keys and\nvalues in the node['confd']['config'] hash. If the configuration\nchanges on disk Chef will kick the confd service which is running\nas a service.\nThe confd_service custom resource\nprovides basic capabilities for a binary or a package\ninstallation. By default the binary is downloaded and installed from\nthe confd GitHub project's release page.\nAdvanced Usage\nThere are two additional custom resources which provide the means for\nwriting out confd templates and executing confd ad-hoc instead of\nas a service. The former custom resource assumes that confd has been\ninstalled separately as only the service resource performs the actual\npackage or binary installation.\nThe confd-iptables cookbook provides an excellent example of\nadvanced usage of the custom resource for writing out confd templates.\nAn example of using that custom resource can be seen below, but make\nsure to take a look at the confd-iptables cookbook default recipe\nfor the latest.\nconfd_template '\/etc\/iptables\/confd' do\n  template_source node['confd-iptables']['template_source']\n  prefix node['confd-iptables']['prefix']\n  keys node.tags.map { |t| \"\/groups\/#{t}\" }\n\n  check_command \"\/sbin\/iptables-restore -n -t < #{path}\"\n  reload_command \"\/sbin\/iptables-restore -n < #{path}\"\n\n  notifies :restart, 'confd_service[confd]', :delayed\nend\n","26":"collectd_plugins cookbook\n\n\n\n\nApplication cookbook which configures collectd plugins.\nBasic Usage\nThe default recipe installs the collectd daemon\nand configures the following plugins on the node.\n\nsyslog\ncpu\ndf\ninterface\nmemory\nswap\nload\nstatsd\nunixsock\nuptime\n\nAdvanced Configuration\nA wrapper cookbook can easily be used to fine tune specific settings\nfor any of the individual plugin recipes. For example, if you wanted to\nsimply change the syslog level you can do so from a wrapper recipe.\nnode.default['collectd-plugins']['syslog']['log_level'] = 'warn'\ninclude_recipe 'collectd_plugins::default'\n","27":"kubernetes-cluster cookbook\n\n\n\n\nApplication cookbook which installs and configures a Kubernetes cluster.\nSupported Platforms\n\nRHEL 7.1+ (CentOS 7.1+)\n\nBasic Usage\nSee detailed step by step instructions in the wiki here.\nConfigure masters using kubernetes-cluster::master\nConfigure minions using kubernetes-cluster::minion\nConfigure Docker registry using kubernetes-cluster::registry\nThe purpose of this cookbook is to install and configure the proper sevices to create application container clusters. This includes etcd, Kubernetes, Flannel, and Docker- specifically aimed at operating on Enterprise Linux platforms. The best method to using this cookbook is to create a wrapper with specific configurations for your project, adding this cookbook as a dependency. This cookbook assumes you have access to a chef server- however, the cookbook will work fine without it if you override node['kubernetes']['etcd']['members'] in attributes\/master.rb node['kubernetes']['master']['fqdn'] in attributes\/minion.rb in insecure mode. Secure mode requires more configuration.\n\nFirst, converge your masters, The first master converge may fail due to ETCD cluster sync oddities, I am working for a resolution for this. Just re-converge the masters that fail.\nSecond, converge your minions.\n\nExample solo.json for master\n{\n  \"kubernetes\": {\n    \"etcd\": {\n      \"members\": [\"master1.example.com\", \"master2.example.com\", \"master3.example.com\"]\n    }\n  },\n  \"run_list\": [\"recipe[kubernetes-cluster::master]\"]\n}\nExample solo.json for minion\n{\n  \"kubernetes\": {\n    \"master\": {\n      \"fqdn\": [\"master1.example.com\", \"master2.example.com\", \"master3.example.com\"]\n    }\n  },\n  \"run_list\": [\"recipe[kubernetes-cluster::minion]\"]\n}\nAdvanced Usage\nAs well as configuring a simple Kubernetes cluster, this cookbook also allows for far more advanced configurations. These configurations range from changing flannel network layout, to enabling secure communications, and adding additional Docker regestries. Secure mode will configure SSL and TLS connections for all endpoints for etcd and Kubernetes. This is HIGHLY recommended for production-like purposes. This will require large amounts of prep work. You can also set URLs for additional Docker registries for the minions to get container images from- as well as configuring said registry.\nFirst, set node['kubernetes']['secure']['enabled'] = 'true' and read below:\nI highly recommend you use a tool like CFSSL (CloudFlare SSL) to create your certificates, check out https:\/\/www.digitalocean.com\/community\/tutorials\/how-to-secure-your-coreos-cluster-with-tls-ssl-and-firewall-rules and start at \"Use CFSSL to Generate Self-Signed Certificates\"\nMasters:\n\nnode['kubernetes']['etcd']['peer']['ca'] - Certificate authority for managing authentication for master to master connections (etcd sync)\nnode['kubernetes']['etcd']['peer']['cert'] - Certificate the master identifies as for peer connections (Signed by peer CA)\nnode['kubernetes']['etcd']['peer']['key'] - Key matching peer certificate\nnode['kubernetes']['etcd']['client']['ca'] - Certificate authority for managing authentication for client to master connections (etcdctl\/kubectl\/kubelet)\nnode['kubernetes']['etcd']['client']['cert'] - Certificate the client identifies as for connections (Signed by client CA)\nnode['kubernetes']['etcd']['client']['key'] - Key matching client certificate\n\nMinions:\n\nnode['kubernetes']['etcd']['client']['ca'] - Certificate authority for verifying cert validity for client to master connections\nnode['kubernetes']['etcd']['client']['cert'] - Certificate the client identifies as for connections (Signed by client CA)\nnode['kubernetes']['etcd']['client']['key'] - Key matching client certificate\n\nNOTE: Peer and Client CA can be the same. This will allow for far simpler setup. However, you may use a different CA to more closely manage security if desired.\nNOTE: Additional exposed attributes contain notes for usage in the appropriate attributes file.\nExample solo.json for master\n{\n  \"kubernetes\": {\n    \"etcd\": {\n      \"members\": [\"master1.example.com\", \"master2.example.com\", \"master3.example.com\"],\n      \"basedir\": \"\/kube\/etcd\",\n      \"peer\": {\n        \"ca\": \"-----BEGIN CERTIFICATE-----\\ndatadata\\n-----END CERTIFICATE-----\",\n        \"cert\": \"-----BEGIN CERTIFICATE-----\\ndatadata\\n-----END CERTIFICATE-----\",\n        \"key\": \"-----BEGIN KEY-----\\ndatadata\\n-----END KEY-----\"\n      },\n      \"client\": {\n        \"ca\": \"-----BEGIN CERTIFICATE-----\\ndatadata\\n-----END CERTIFICATE-----\",\n        \"cert\": \"-----BEGIN CERTIFICATE-----\\ndatadata\\n-----END CERTIFICATE-----\",\n        \"key\": \"-----BEGIN KEY-----\\ndatadata\\n-----END KEY-----\"\n      }\n    },\n    \"secure\": {\n      \"enabled\": \"true\"\n    },\n    \"master\": {\n      \"podmaster-source\": \"registry.example.com:5000\/podmaster:1.1\",\n      \"scheduler-source\": \"registry.example.com:5000\/scheduler:1.0.3\",\n      \"controller-manager-source\": \"registry.example.com:5000\/controller-manager:1.0.3\"\n    }\n  },\n  \"docker\": {\n    \"environment\": {\n      \"docker-registry\": \"registry.example.com:5000\",\n      \"registry-insecure\": \"registry.example.com:5000\",\n      \"docker-basedir\": \"\/kube\/docker\"\n    }\n  },\n  \"run_list\": [\"recipe[kubernetes-cluster::master]\"]\n}\nExample solo.json for minion\n{\n  \"kubernetes\": {\n    \"etcd\": {\n      \"client\": {\n        \"ca\": \"-----BEGIN CERTIFICATE-----\\ndatadata\\n-----END CERTIFICATE-----\",\n        \"cert\": \"-----BEGIN CERTIFICATE-----\\ndatadata\\n-----END CERTIFICATE-----\",\n        \"key\": \"-----BEGIN KEY-----\\ndatadata\\n-----END KEY-----\"\n      }\n    },\n    \"master\": {\n      \"fqdn\": [\"master1.example.com\", \"master2.example.com\", \"master3.example.com\"]\n    },\n    \"secure\": {\n      \"enabled\": \"true\"\n    }\n  },\n  \"docker\": {\n    \"environment\": {\n      \"docker-registry\": \"registry.example.com:5000\",\n      \"registry-insecure\": \"registry.example.com:5000\",\n      \"docker-basedir\": \"\/kube\/docker\"\n    }\n  },\n  \"kubelet\": {\n    \"pause-source\": \"registry.example.com:5000\/pause:base\",\n    \"register\": \"true\"\n  },\n  \"run_list\": [\"recipe[kubernetes-cluster::minion]\"]\n}\nExample solo.json for registry\n{\n  \"kubernetes\": {\n    \"registry\": {\n      \"port\": \"5000\",\n      \"workers\": \"8\",\n      \"storage\": \"\/kube\/docker-storage\/\"\n    }\n  },\n  \"run_list\": [\"recipe[kubernetes-cluster::registry]\"]\n}\nTesting\nThis project will use Test Kitchen to execute the ChefSpec tests\non a clean virtual machine. By default Test Kitchen will use Vagrant\nand attempt to start a new virtual machine up from a default Opscode box.\nbin\/kitchen test default-centos-7.1\nHowever, no meaningful tests are currently written. This will be fixed.\n","28":"NYT Campfin\nA very basic Python client for the New York Times Campaign Finance API. You'll need an API key, which should be set as an environment variable to run the tests. The client returns JSON only, not full Python objects, and attempts to implement each response in The Times' API.\nInstall\n$ pip install nytcampfin\n\nOr download and run\n$ python setup.py install\n\nRequirements\nNYT Campfin uses the Kenneth Reitz's Requests library for retrieving API endpoints and Roman Haritonov's requests-cache library for local caching. The cache is preconfigured to use a local sqlite database and set to expire after 5 minutes.\nTests\nTo run the tests, set your API key as an environment variable NYT_CAMPFIN_API_KEY:\n$ export NYT_CAMPFIN_API_KEY=YOUR-API-KEY\n\nand then run the tests:\n$ python test.py\n\nThe use of caching is disabled in the tests.\nUsage\n>>> from nytcampfin import NytCampfin\n>>> finance = NytCampfin(YOUR_NYT_CAMPAIGN_FINANCE_API_KEY)\n\n# retrieve today's filings\n>>> today = finance.filings.today()\n>>> today[0]['filing_id']\n793150\n\n# retrieve a committee's details\n>>> cmte = finance.committees.get('C00490219',2012)\n>>> cmte['id']\nu'C00490219'\n\n# retrieve a candidate's details\n>>> cand = finance.candidates.get('H4NY11138')\n>>> cand['name']\nu'CLARKE, YVETTE D'\n\nSee the tests for plenty more examples.\nNote on Patches\/Pull Requests\n\nFork the project.\nMake your feature addition or bug fix.\nAdd tests for it.\nSend a pull request. Bonus points for topic branches.\n\nAuthors\nDerek Willis, dwillis@nytimes.com\nCopyright\nCopyright (c) 2012 The New York Times Company. See LICENSE for details.\n","29":"JsonPrinter allows you to convert arbitrarily nested Ruby data structures into \nhuman-and-machine-readable JSON output.  The input data can be any combination \nof arrays, hashes, symbols, strings, numbers, times, and false, true, and nil values.\n\n  data = \n   {\"attribute\" => \"value\",\n    \"blank\" => nil,\n    \"list\" => \n      [true,\n       2,\n       \"elem_number_three\"],\n    \"nested_hash\" =>\n      {\"key\" => 7,\n       \"other_key\" => 13.5}}\n  \n  JsonPrinter.render(data)\n  #=>\n  {\"nested_hash\":\n   {\"other_key\": 13.5,\n    \"key\": 7},\n   \"list\":\n    [true,\n     2,\n     \"elem_number_three\"],\n   \"blank\": null,\n   \"attribute\": \"value\"}\n\n  JSON.parse(JsonPrinter.render(data)) == data\n  #=> true\n\nThe printer recognizes instances of ActiveSupport::OrderedHash or other Hash-like objects responding to #keys and will render their attributes in order:\n\n  data = \n    ActiveSupport::OrderedHash.new(\n      [[\"foo\", \"bar\"], [\"biz\", \"bat\"], [\"cat\", \"hat\"]])\n      \n  JsonPrinter.render(data)\n  #=>\n  {\"foo\": \"bar\",\n   \"biz\": \"bat\",\n   \"cat\", \"hat\"}\n\n\nCopyright 2009 Mark McGranaghan and released under an MIT license.","30":"realgraph-listener\nRealgraph Listener listens for the Realgraph beacon installed on webpages.  It is a barebones Node.js app using Express 4.\nRunning Locally\nMake sure you have Node.js and the Heroku Toolbelt installed.\n$ git clone git@github.com:observermedia\/realgraph-listener.git \n$ cd realgraph-listener\n$ npm install\n$ npm start\nYour app should now be running on localhost:5000.\nDeploying to Heroku\n$ heroku create\n$ git push heroku master\n$ heroku open\n\nDocumentation\nRealgraph Listener\n\nListens for 'pings' to \/realgraph\/listen\nWrites { timestamp, url, hash(url) } to a PostgreSQL database\nResponds with a status code or with valid JSON that may be rendered by a client\n\nRealgraph Beacon\nInsert the following script onto any page you'd like to track using the Realgraph .\n<script type='text\/javascript' async=\"async\" src='http:\/\/realgraph-listener.herokuapp.com\/beacon.js'><\/script>\n\nIn order to display information about entities mentioned in the article, put the following div elements on your page:\n<div id=\"realgraph-buildings-data\"><\/div>\n<div id=\"realgraph-organizations-data\"><\/div>\n<div id=\"realgraph-people-data\"><\/div>\n<div id=\"realgraph-activities-data\"><\/div>\n\nEntities data will be inserted into those divs.\nFor more information about using Node.js on Heroku, see these Dev Center articles:\n\nGetting Started with Node.js on Heroku\nHeroku Node.js Support\nNode.js on Heroku\nBest Practices for Node.js Development\nUsing WebSockets on Heroku with Node.js\n\n$ heroku pg:psql\n\n> create table realgraph_pings (\n\tcreated_time TIMESTAMP WITH TIME ZONE NOT NULL DEFAULT current_timestamp, \n\turl VARCHAR(500), \n\thash TEXT\n);\n\n> insert into realgraph_pings (url, hash) values ('http:\/\/www.example.com\/some\/url', md5('http:\/\/www.example.com\/some\/url'));\n\nTo delete content from the table:\nDELETE FROM realgraph_pings;\n\n\n","31":"django-wordpress-rest\n\n\n\n\n\n\nWelcome to django-wordpress-rest!\n\nSummary\nDjango-wordpress-rest is a Django application that syncs content from a WordPress.com site to a Django site.\nThis is done using the WordPress.com REST API.\nA separate copy of the content data is stored on the Django side, which allows for loose coupling and extensability.\nFull documentation is available on Read The Docs.\n\nQuickstart\nInstall the module:\npip install django-wordpress-rest\n\nAdd \"wordpress\" to your INSTALLED_APPS setting:\nINSTALLED_APPS = (\n    # ...\n    \"wordpress\",\n    # ...\n)\n\nCreate the database tables that will persist the sync'd WordPress content:\n$ python manage.py migrate\n\nSync WordPress content using the management command. The <site_id> can be found using the \/me\/sites WordPress API call. This is useful for periodically updating the content with cron.\n$ python manage.py load_wp_api <site_id>\n\n\nAuthentication\nIf you'd like to synchronize private content, create an OAuth2 access token using the instructions provided by WordPress:\nhttps:\/\/developer.wordpress.com\/docs\/oauth2\/\nAdd this token to your Django settings.py file. Use an environment variable to keep things secure:\nWP_API_AUTH_TOKEN = os.getenv(\"WP_API_AUTH_TOKEN\")\n\n\nLoad Options\nBring the site content up to date:\n# first run gets everything\n$ python manage.py load_wp_api <site_id>\n\n# second run gets content modified since previous run\n$ python manage.py load_wp_api <site_id>\n\nDo a full sweep of the site content, inserting and updating as needed:\n# first run gets everything\n$ python manage.py load_wp_api <site_id>\n\n# second run gets\/updates all content again\n$ python manage.py load_wp_api <site_id> --full\n\nLoad everything modified after a given date:\n$ python manage.py load_wp_api <site_id> --modified_after=2015-01-01\n\nJust load posts, not pages, attachments, or reference data:\n$ python manage.py load_wp_api <site_id> --type=post\n\nLoad posts with a specific status (note this requires authentication):\n$ python manage.py load_wp_api <site_id> --status=draft\n\nPurge local content before loading -- careful!\n$ python manage.py load_wp_api <site_id> --purge --full\n\n\nWebhook\nIf you'd like to use the webhook to sync a post immediately after it's updated, include the urls into your project's urls.py, like so:\nfrom django.conf.urls import include\n\nurlpatterns = [\n    url(r'^wordpress\/', include('wordpress.urls'))\n]\n\nAdd \"after_response\" to your INSTALLED_APPS setting (this allows asynchronous processing):\nINSTALLED_APPS = (\n    # ...\n    \"after_response\",\n    \"wordpress\",\n    # ...\n)\n\nThe webhook looks for your <site_id> in Django settings. So add this your settings.py, and use an environment variable to keep things secure:\nWP_API_SITE_ID = os.getenv(\"WP_API_SITE_ID\")\n\nFinally from your WordPress.com site, submit a POST request with an ID data element in the body to trigger a sync of a single post. Note this should be the WordPress Post ID, not the Django one!\n$ curl -X POST --data \"ID=123456\" http:\/\/mydjangosite.com\/wordpress\/load_post\n\n\nRunning the Tests\n$ pip install detox\n$ detox\n\n","32":"Passgenerator\nPassgenerator is a Laravel5+ package that allows you to easily create passes compatible with Apple Wallet (former Passbook).\n\ud83d\udc49 Table of Contents \ud83d\udc48\n\n\ud83d\udc6e Requirements\n\ud83d\udcbe Installation\n\ud83c\udf4e Apple docs\n\ud83d\udcdd Configuration\n\ud83d\ude80 Usage\n\n\ud83d\udc6e Requirements\nOnly things needed are Laravel 6+ and to have the PHP Zip extension installed and enabled.\n\ud83d\udcbe Installation\nThe best and easiest way o install the package is using the Composer package manager. To do so, run this command in your project root:\ncomposer require thenextweb\/passgenerator\nThen, add the Thenextweb\\PassGeneratorServiceProvider provider to the providers array in config\/app.php:\n'providers' => [\n\/\/ ...\n    Thenextweb\\PassGeneratorServiceProvider::class,\n],\nThat's it!\n\ud83c\udf4e Apple docs\nFrom now on, some stuff is much better explained on the Apple docs, so when in doubt just check (if you haven't done so) the following documents:\n\nWallet Portal\nWallet Developer Guide\nCrypto Signatures\nPassKit Package Format Reference\n\n\ud83d\udcdd Configuration\nTo start using the package some Apple files are needed, as well as some action in order to convert them to more friendly formats:\n\n\nGo to the Apple Developer page \u27b5 Identifiers \u27b5 Pass Type IDs.\n\n\nNext, you need to create a pass type ID. This is similar to the bundle ID for apps. It will uniquely identify a specific kind of pass. It should be of the form of a reverse-domain name style string (i.e., pass.com.example.appname).\n\n\nAfter creating the pass type ID, click on Edit and follow the instructions to create a new Certificate.\n\n\nOnce the process is finished, the pass certificate can be downloaded. That's not it though, the certificate is downloaded as .cer file, which need to be converted to .p12 in order to work. If you are using a Mac you can import it into Keychain Access and export it from there. Make sure to remember the password you have given to the exported file since you'll have to use it later. You can also use other tools to convert the certificate but be sure it includes the private key on the exported PKCS12 file.\n\n\nIf you have made iOS development, you probably have already the Apple Worldwide Developer Relations Intermediate Certificate in your Mac\u2019s keychain. If not, it can be downloaded from the Apple Website (on .cer format). This one needs to be exported as .pem, you can also do so from Keychain Access (or whatever tool you use to manage certificates on your OS).\n\n\nOnce all this tedious process has been done, everything is almost ready to start using the package. The easiest now is to add to the following keys to your .env file:\n\nCERTIFICATE_PATH \u27aa The path to the .p12 pass certificate.\nCERTIFICATE_PASS \u27aa The password set to unlock the certificate when it was exported.\nWWDR_CERTIFICATE \u27aa The path to the Apple Worldwide Developer Relations Intermediate Certificate on .pem format.\n\nIn case there is a reason the config file must be modified (conflicting env keys, dynamic certificates required...), it can be published with the following command:\n\/\/ file will be at config\/passgenerator.php\nphp artisan vendor:publish --provider=\"Thenextweb\\PassGeneratorServiceProvider\"\n\ud83d\ude80 Usage\nTo create a pass for the first time, you have to first create the pass definition, either as a JSON file or as an array. It is really recommended to have already read the Apple docs as well as the PassKit Package Format Reference.\nuse Thenextweb\\PassGenerator;\n\n\/\/...\n\n$pass_identifier = 'somekindofid';  \/\/ This, if set, it would allow for retrieval later on of the created Pass\n\n$pass = new PassGenerator($pass_identifier);\n\n$pass_definition = [\n    \"description\"       => \"description\",\n    \"formatVersion\"     => 1,\n    \"organizationName\"  => \"organization\",\n    \"passTypeIdentifier\"=> \"pass.com.example.appname\",\n    \"serialNumber\"      => \"123456\",\n    \"teamIdentifier\"    => \"teamid\",\n    \"foregroundColor\"   => \"rgb(99, 99, 99)\",\n    \"backgroundColor\"   => \"rgb(212, 212, 212)\",\n    \"barcode\" => [\n        \"message\"   => \"encodedmessageonQR\",\n        \"format\"    => \"PKBarcodeFormatQR\",\n        \"altText\"   => \"altextfortheQR\",\n        \"messageEncoding\"=> \"utf-8\",\n    ],\n    \"boardingPass\" => [\n        \"headerFields\" => [\n            [\n                \"key\" => \"destinationDate\",\n                \"label\" => \"Trip to: BCN-SANTS\",\n                \"value\" => \"15\/09\/2015\"\n            ]\n        ],\n        \"primaryFields\" => [\n            [\n                \"key\" => \"boardingTime\",\n                \"label\" => \"MURCIA\",\n                \"value\" => \"13:54\",\n                \"changeMessage\" => \"Boarding time has changed to %@\"\n            ],\n            [\n                \"key\" => \"destination\",\n                \"label\" => \"BCN-SANTS\",\n                \"value\" => \"21:09\"\n            ]\n\n        ],\n        \"secondaryFields\" => [\n            [\n                \"key\" => \"passenger\",\n                \"label\" => \"Passenger\",\n                \"value\" => \"J.DOE\"\n            ],\n            [\n                \"key\" => \"bookingref\",\n                \"label\" => \"Booking Reference\",\n                \"value\" => \"4ZK6FG\"\n            ]\n        ],\n        \"auxiliaryFields\" => [\n            [\n                \"key\" => \"train\",\n                \"label\" => \"Train TALGO\",\n                \"value\" => \"00264\"\n            ],\n            [\n                \"key\" => \"car\",\n                \"label\" => \"Car\",\n                \"value\" => \"009\"\n            ],\n            [\n                \"key\" => \"seat\",\n                \"label\" => \"Seat\",\n                \"value\" => \"04A\"\n            ],\n            [\n                \"key\" => \"classfront\",\n                \"label\" => \"Class\",\n                \"value\" => \"Tourist\"\n            ]\n        ],\n        \"backFields\" => [\n            [\n                \"key\" => \"ticketNumber\",\n                \"label\" => \"Ticket Number\",\n                \"value\" => \"7612800569875\"\n            ], [\n                \"key\" => \"passenger-name\",\n                \"label\" => \"Passenger\",\n                \"value\" => \"John Doe\"\n            ], [\n                \"key\" => \"classback\",\n                \"label\" => \"Class\",\n                \"value\" => \"Tourist\"\n            ]\n        ],\n        \"locations\" => [\n            [\n                \"latitude\" => 37.97479,\n                \"longitude\" => -1.131522,\n                \"relevantText\" => \"Departure station\"\n            ]\n        ],\n        \"transitType\" => \"PKTransitTypeTrain\"\n    ],\n];\n\n$pass->setPassDefinition($pass_definition);\n\n\/\/ Definitions can also be set from a JSON string\n\/\/ $pass->setPassDefinition(file_get_contents('\/path\/to\/pass.json));\n\n\/\/ Add assets to the PKPass package\n$pass->addAsset(base_path('resources\/assets\/wallet\/background.png'));\n$pass->addAsset(base_path('resources\/assets\/wallet\/thumbnail.png'));\n$pass->addAsset(base_path('resources\/assets\/wallet\/icon.png'));\n$pass->addAsset(base_path('resources\/assets\/wallet\/logo.png'));\n\n$pkpass = $pass->create();\nNow, a valid ticket is already in place. Apple recommends a MIME type to serve it to its devices so something like the following should do:\nreturn new Response($pkpass, 200, [\n    'Content-Transfer-Encoding' => 'binary',\n    'Content-Description' => 'File Transfer',\n    'Content-Disposition' => 'attachment; filename=\"pass.pkpass\"',\n    'Content-length' => strlen($pkpass),\n    'Content-Type' => PassGenerator::getPassMimeType(),\n    'Pragma' => 'no-cache',\n]);\nLater on, if your users need to download the pass again, you don't need to create it again (wasting all those CPU cycles on crypto stuff), you can just do something like:\n\/\/ If the pass for that ID does not exist, you can then proceed to generate it as done above.\n$pkpass = PassGenerator::getPass($pass_identifier);\nif (!$pkpass) {\n    $pkpass = $this->createWalletPass();\n}\n\/\/ ...\nIt is also possible to retrieve the actual path to a pass on your filesystem. By default, Passgenerator  will copy your default filesystem config (usually rooted on storage_path('app') but you can always do getPassFilePath($pass_identifier) and retrieve the real path (in case it exists).\nDefinitions\nIt is also possible to programatically create\/modify a pass using the definitions objects. Eg.-\n$coupon = Thenextweb\\Definitions\\Coupon();\n$coupon->setDescription('Coupon description');\n$coupon->setSerialNumber('123456');\n\n$coupon->setUserInfo([\n    'email' => 'user@domain.com',\n]);\n$coupon->setExpirationDate(Carbon::now()->addMonths(6));\n\n$location = new Location();\n$location->setLatitude(40.4378698);\n$location->setLongitude(-3.819619);\n$coupon->addLocation($location);\n\n$coupon->setMaxDistance(50);\n$coupon->setRelevantDate(Carbon::now()->addDays(10));\n\n$coupon->addAuxiliaryField(new Field('key', 'value'));\n\n$coupon->addBackField(new Number('price', 13, [\n    'currencyCode' => 'EUR',\n    'numberStyle' => Number::STYLE_DECIMAL\n]));\n\n$coupon->addPrimaryField(new Date('created_at', Carbon::now(), [\n    'dateStyle' => Date::STYLE_FULL,\n]));\n\n$barcode = new Barcode('7898466321', Barcode::FORMAT_CODE128);\n$coupon->addBarcode($barcode);\n\n$passgenerator->setPassDefinition($coupon);\n\n","33":"AMP for WordPress\nOverview\nThis plugin adds support for the Accelerated Mobile Pages (AMP) Project, which is an open source initiative that aims to provide mobile optimized content that can load instantly everywhere.\nWith the plugin active, all posts on your site will have dynamically generated AMP-compatible versions, accessible by appending \/amp\/ to the end your post URLs. For example, if your post URL is http:\/\/example.com\/2016\/01\/01\/amp-on\/, you can access the AMP version at http:\/\/example.com\/2016\/01\/01\/amp-on\/amp\/. If you do not have pretty permalinks enabled, you can do the same thing by appending ?amp=1, i.e. http:\/\/example.com\/?p=123&amp=1\nNote #1: that Pages and archives are not currently supported.\nNote #2: this plugin only creates AMP content but does not automatically display it to your users when they visit from a mobile device. That is handled by AMP consumers such as Google Search. For more details, see the AMP Project FAQ.\nCustomization \/ Templating\nThe plugin ships with a default template that looks nice and clean and we tried to find a good balance between ease and extensibility when it comes to customization.\nYou can tweak small pieces of the template or the entire thing depending on your needs.\nWhere Do I Put My Code?\nThe code snippets below and any other code-level customizations should happen in one of the following locations.\nIf you're using an off-the-shelf theme (like from the WordPress.org Theme Directory):\n\nA child theme.\nA custom plugin that you activate via the Dashboard.\nA mu-plugin.\n\nIf you're using a custom theme:\n\nfunctions.php (or via a 'require' call to files that load from functions.php).\nAny of the options above.\n\nTheme Mods\nThe default template will attempt to draw from various theme mods, such as site icon, if supported by the active theme.\nSite Icon\nIf you add a site icon, we will automatically replace the WordPress logo in the template.\nIf you'd prefer to do it via code:\nadd_filter( 'amp_post_template_data', 'xyz_amp_set_site_icon_url' );\n\nfunction xyz_amp_set_site_icon_url( $data ) {\n\t\/\/ Ideally a 32x32 image\n\t$data[ 'site_icon_url' ] = get_stylesheet_directory_uri() . '\/images\/amp-site-icon.png';\n\treturn $data;\n}\nLogo Only\nIf you want to hide the site text and just show a logo, use the amp_post_template_css action. The following colors the title bar black, hides the site title, and replaces it with a centered logo:\nadd_action( 'amp_post_template_css', 'xyz_amp_additional_css_styles' );\n\nfunction xyz_amp_additional_css_styles( $amp_template ) {\n\t\/\/ only CSS here please...\n\t?>\n\theader.amp-wp-header {\n\t\tpadding: 12px 0;\n\t\tbackground: #000;\n\t}\n\theader.amp-wp-header a {\n\t\tbackground-image: url( 'https:\/\/example.com\/path\/to\/logo.png' );\n\t\tbackground-repeat: no-repeat;\n\t\tbackground-size: contain;\n\t\tdisplay: block;\n\t\theight: 28px;\n\t\twidth: 94px;\n\t\tmargin: 0 auto;\n\t\ttext-indent: -9999px;\n\t}\n\t<?php\n}\nNote: you will need to adjust the colors and sizes based on your brand.\nTemplate Tweaks\nYou can tweak various parts of the template via code.\nFeatured Image\nThe default template displays the featured image. If you don't want to display the featured image in your amp page, use the following code:\nadd_filter( 'amp_post_template_data', 'xyz_amp_remove_featured_image' );\n\nfunction xyz_amp_remove_featured_image( $data ) {\n    $data['featured_image'] = false;\n    return $data;\n}\nContent Width\nBy default, your theme's $content_width value will be used to determine the size of the amp content well. You can change this:\nadd_filter( 'amp_content_max_width', 'xyz_amp_change_content_width' );\n\nfunction xyz_amp_change_content_width( $content_max_width ) {\n\treturn 1200;\n}\nTemplate Data\nUse the amp_post_template_data filter to override default template data. The following changes the placeholder image used for iframes to a file located in the current theme:\nadd_filter( 'amp_post_template_data', 'xyz_amp_set_custom_placeholder_image' );\n\nfunction xyz_set_custom_placeholder_image( $data ) {\n\t$data[ 'placeholder_image_url' ] = get_stylesheet_directory_uri() . '\/images\/amp-iframe-placeholder.png';\n\treturn $data;\n}\nNote: The path must pass the default criteria set out by validate_file and must be somewhere in a subfolder of WP_CONTENT_DIR.\nSchema.org (JSON) Metadata\nThe plugin adds some default metadata to enable \"Rich Snippet\" support. You can modify this using the amp_post_template_metadata filter. The following changes the type annotation to NewsArticle (from the default BlogPosting) and overrides the default Publisher Logo.\nadd_filter( 'amp_post_template_metadata', 'xyz_amp_modify_json_metadata', 10, 2 );\n\nfunction xyz_amp_modify_json_metadata( $metadata, $post ) {\n\t$metadata['@type'] = 'NewsArticle';\n\n\t$metadata['publisher']['logo'] = array(\n\t\t'@type' => 'ImageObject',\n\t\t'url' => get_template_directory_uri() . '\/images\/my-amp-metadata-logo.png',\n\t\t'height' => 60,\n\t\t'width' => 600,\n\t);\n\n\treturn $metadata;\n}\nTemplate Meta (Author, Date, etc.)\nFor the meta section of the template (i.e. author, date, taxonomies, etc.), you can override templates for the existing sections, remove them, add new ones.\nExample: Override Author Template from Theme\nCreate a folder in your theme called amp and add a file called meta-author.php with the following:\n<li class=\"xyz-byline\">\n\t<span>Anonymous<\/span>\n<\/li>\nReplace the contents, as needed.\nExample: Override Taxonomy Template via filter\nThis will load the file t\/meta-custom-tax.php for the taxonomy section:\nadd_filter( 'amp_post_template_file', 'xyz_amp_set_custom_tax_meta_template', 10, 3 );\n\nfunction xyz_amp_set_custom_tax_meta_template( $file, $type, $post ) {\n\tif ( 'meta-taxonomy' === $type ) {\n\t\t$file = dirname( __FILE__ ) . '\/t\/meta-custom-tax.php';\n\t}\n\treturn $file;\n}\nIn t\/meta-custom-tax.php, you can add something like the following to replace the default category and tags with your custom author taxonomy:\n<li class=\"xyz-tax-authors\">\n\t<?php echo get_the_term_list( $this->get( 'post_id' ), 'xyz-author', '', ', ' ); ?>\n<\/li>\nExample: Remove Author from header_meta\nThis will completely remove the author section:\nadd_filter( 'amp_post_article_header_meta', 'xyz_amp_remove_author_meta' );\n\nfunction xyz_amp_remove_author_meta( $meta_parts ) {\n\tforeach ( array_keys( $meta_parts, 'meta-author', true ) as $key ) {\n\t\tunset( $meta_parts[ $key ] );\n\t}\n\treturn $meta_parts;\n}\nExample: Add Comment Count to footer_meta\nThis adds a new section to display the comment count:\nadd_filter( 'amp_post_article_footer_meta', 'xyz_amp_add_comment_count_meta' );\n\nfunction xyz_amp_add_comment_count_meta( $meta_parts ) {\n\t$meta_parts[] = 'xyz-meta-comment-count';\n\treturn $meta_parts;\n}\n\nadd_filter( 'amp_post_template_file', 'xyz_amp_set_comment_count_meta_path', 10, 3 );\n\nfunction xyz_amp_set_comment_count_meta_path( $file, $type, $post ) {\n\tif ( 'xyz-meta-comment-count' === $type ) {\n\t\t$file = dirname( __FILE__ ) . '\/templates\/xyz-meta-comment-count.php';\n\t}\n\treturn $file;\n}\nThen, in templates\/xyz-meta-comment-count.php:\n<li>\n\t<?php printf( _n( '%d comment', '%d comments', $this->get( 'post' )->comment_count, 'xyz-text-domain' ) ); ?>\n<\/li>\nCustom CSS\nRule Additions\nIf you want to append to the existing CSS rules (e.g. styles for a custom embed handler), you can use the amp_post_template_css action:\nadd_action( 'amp_post_template_css', 'xyz_amp_my_additional_css_styles' );\n\nfunction xyz_amp_my_additional_css_styles( $amp_template ) {\n\t\/\/ only CSS here please...\n\t?>\n\t.amp-wp-byline amp-img {\n\t\tborder-radius: 0; \/* we don't want round avatars! *\/\n\t}\n\t.my-custom-class {\n\t\tcolor: blue;\n\t}\n\t<?php\n}\nCompletely Override CSS\nIf you'd prefer to use your own styles, you can either:\n\nCreate a folder in your theme called amp and add a file called style.php with your custom CSS.\nSpecify a custom template using the amp_post_template_file filter for 'style' === $type. See the \"Override\" examples in the \"Meta\" section for examples.\n\nNote: the file should only include CSS, not the <style> opening and closing tag.\nHead and Footer\nIf you want to add stuff to the head or footer of the default AMP template, use the amp_post_template_head and amp_post_template_footer actions.\nadd_action( 'amp_post_template_footer', 'xyz_amp_add_pixel' );\n\nfunction xyz_amp_add_pixel( $amp_template ) {\n\t$post_id = $amp_template->get( 'post_id' );\n\t?>\n\t<amp-pixel src=\"https:\/\/example.com\/hi.gif?x=RANDOM\"><\/amp-pixel>\n\t<?php\n}\nAMP Endpoint\nIf you don't want to use the default \/amp endpoint, use the amp_query_var filter to change it to anything else.\nadd_filter( 'amp_query_var' , 'xyz_amp_change_endpoint' );\n\nfunction xyz_amp_change_endpoint( $amp_endpoint ) {\n\treturn 'foo';\n}\nCustom Template\nIf you want complete control over the look and feel of your AMP content, you can override the default template using the amp_post_template_file filter and pass it the path to a custom template:\nadd_filter( 'amp_post_template_file', 'xyz_amp_set_custom_template', 10, 3 );\n\nfunction xyz_amp_set_custom_template( $file, $type, $post ) {\n\tif ( 'single' === $type ) {\n\t\t$file = dirname( __FILE__ ) . '\/templates\/my-amp-template.php';\n\t}\n\treturn $file;\n}\nNote: there are some requirements for a custom template:\n\nYou must trigger the amp_post_template_head action in the <head> section:\n\ndo_action( 'amp_post_template_head', $this );\n\nYou must trigger the amp_post_template_footer action right before the <\/body> tag:\n\ndo_action( 'amp_post_template_footer', $this );\n\nWithin your amp-custom style tags, you must trigger the amp_post_template_css action:\n\ndo_action( 'amp_post_template_css', $this );\n\nYou must include all required mark-up that isn't already output via the amp_post_template_head action.\n\nHandling Media\nBy default, the plugin attempts to gracefully handle the following media elements in your content:\n\nimages (converted from img => amp-img or amp-anim)\nvideos (converted from video => amp-video; Note: Flash is not supported)\naudio (converted from audio => amp-audio)\niframes (converted from iframes => amp-iframes)\nYouTube, Instagram, Twitter, and Vine oEmbeds and shortcodes (converted from the embed to the matching amp- component)\n\nFor additional media content such as custom shortcodes, oEmbeds or manually inserted embeds, ads, etc. there are several customization options available and outlined below.\nDo Nothing\nIf your embeds\/media use standard iframes, you can choose to do nothing and let the plugin handle things. They should \"just work\" in most cases.\nthe_content filter\nAll existing hooks on the_content will continue to work. This can be a good or bad thing. Good, because existing plugin integrations will continue to work. Bad, because not all added content may make sense in an AMP context.\nYou can add additional callbacks to the_content filter to output additional content as needed. Use the is_amp_endpoint() function to check if an AMP version of a post is being viewed. However, we recommend using an Embed Handler instead.\nCaveat: with this method, if you add a custom component that requires inclusion of a script, you will need to add that script manually to the template using the amp_post_template_head action.\nUpdate Existing Shortcodes\nIn your existing shortcode or oEmbed callbacks, you can branch using the is_amp_endpoint() and output customized content for AMP content.\nThe same caveat about scripts for custom AMP components applies.\nCustom Embed Handler\nEmbed Handlers are helper classes to inject AMP-specific content for your oEmbeds and shortcodes.\nEmbed Handlers register the embeds they handle using standard WordPress functions such as add_shortcode. For working examples, check out the existing implementations for Instagram, Twitter, etc. as guides to build your own.\nWhile the primary purpose of Embed Handlers is for use with embeds, you can use them for adding AMP-specific the_content callbacks as well.\nStep 1: Build the Embed Handler\nYour Embed Handler class needs to extend the AMP_Base_Embed_Handler class.\nNote: make sure to set proper priorities or remove existing callbacks for your regular content.\nIn classes\/class-amp-related-posts-embed.php:\nclass XYZ_AMP_Related_Posts_Embed extends AMP_Base_Embed_Handler {\n\tpublic function register_embed() {\n\t\t\/\/ If we have an existing callback we are overriding, remove it.\n\t\tremove_filter( 'the_content', 'xyz_add_related_posts' );\n\n\t\t\/\/ Add our new callback\n\t\tadd_filter( 'the_content', array( $this, 'add_related_posts' ) );\n\t}\n\n\tpublic function unregister_embed() {\n\t\t\/\/ Let's clean up after ourselves, just in case.\n\t\tadd_filter( 'the_content', 'xyz_add_related_posts' );\n\t\tremove_filter( 'the_content', array( $this, 'add_related_posts' ) );\n\t}\n\n\tpublic function get_scripts() {\n        return array(\n            'amp-mustache' => 'https:\/\/cdn.ampproject.org\/v0\/amp-mustache-0.1.js',\n            'amp-list' => 'https:\/\/cdn.ampproject.org\/v0\/amp-list-0.1.js',\n        );\n\t}\n\n\tpublic function add_related_posts( $content ) {\n\t\t\/\/ See https:\/\/github.com\/ampproject\/amphtml\/blob\/master\/extensions\/amp-list\/amp-list.md for details on amp-list\n\t\t$related_posts_list = '\n<amp-list src=\"https:\/\/data.com\/articles.json?ref=CANONICAL_URL\" width=300 height=200 layout=responsive>\n\t<template type=\"amp-mustache\">\n\t\t<div>\n\t\t\t<amp-img src=\"{{imageUrl}}\" width=50 height=50><\/amp-img>\n\t\t\t{{title}}\n\t\t<\/div>\n\t<\/template>\n\t<div overflow role=button aria-label=\"Show more\" class=\"list-overflow\">\n\t\tShow more\n\t<\/div>\n<\/amp-list>';\n\n\t\t$content .= $related_posts_list;\n\n\t\treturn $content;\n\t}\n}\nStep 2: Load the Embed Handler\nadd_filter( 'amp_content_embed_handlers', 'xyz_amp_add_related_embed', 10, 2 );\n\nfunction xyz_amp_add_related_embed( $embed_handler_classes, $post ) {\n\trequire_once( dirname( __FILE__ ) . '\/classes\/class-amp-related-posts-embed.php' );\n\t$embed_handler_classes[ 'XYZ_AMP_Related_Posts_Embed' ] = array();\n\treturn $embed_handler_classes;\n}\nCustom Sanitizer\nThe name \"sanitizer\" is a bit of a misnomer. These are primarily used internally in the plugin to make your site's content compatible with the amp spec. This involves stripping unsupported tags and attributes and transforming media elements to their matching amp version (e.g. img => amp-img).\nSanitizers are pretty versatile and, unlike Embed Handlers -- which work with HTML content as a string -- they can be used to manipulate your post's AMP content using PHP's DOM library. We've included an example that shows you how to use a custom sanitizer to inject ads into your content. You can, of course, do many other things such as add related content.\nStep 1: Build the Sanitizer\nYour sanitizer needs to extend the AMP_Base_Sanitizer. In classes\/class-ad-inject-sanitizer.php:\nclass XYZ_AMP_Ad_Injection_Sanitizer extends AMP_Base_Sanitizer {\n\tpublic function sanitize() {\n\t\t$body = $this->get_body_node();\n\n\t\t\/\/ Build our amp-ad tag\n\t\t$ad_node = AMP_DOM_Utils::create_node( $this->dom, 'amp-ad', array(\n\t\t\t\/\/ Taken from example at https:\/\/github.com\/ampproject\/amphtml\/blob\/master\/builtins\/amp-ad.md\n\t\t\t'width' => 300,\n\t\t\t'height' => 250,\n\t\t\t'type' => 'a9',\n\t\t\t'data-aax_size' => '300x250',\n\t\t\t'data-aax_pubname' => 'test123',\n\t\t\t'data-aax_src' => '302',\n\t\t) );\n\n\t\t\/\/ Add a placeholder to show while loading\n\t\t$fallback_node = AMP_DOM_Utils::create_node( $this->dom, 'amp-img', array(\n\t\t\t'placeholder' => '',\n\t\t\t'layout' => 'fill',\n\t\t\t'src' => 'https:\/\/placehold.it\/300X250',\n\t\t) );\n\t\t$ad_node->appendChild( $fallback_node );\n\n\t\t\/\/ If we have a lot of paragraphs, insert before the 4th one.\n\t\t\/\/ Otherwise, add it to the end.\n\t\t$p_nodes = $body->getElementsByTagName( 'p' );\n\t\tif ( $p_nodes->length > 6 ) {\n\t\t\t$p_nodes->item( 4 )->parentNode->insertBefore( $ad_node, $p_nodes->item( 4 ));\n\t\t} else {\n\t\t\t$body->appendChild( $ad_node );\n\t\t}\n\t}\n}\nStep 2: Load the Sanitizer\nadd_filter( 'amp_content_sanitizers', 'xyz_amp_add_ad_sanitizer', 10, 2 );\n\nfunction xyz_amp_add_ad_sanitizer( $sanitizer_classes, $post ) {\n\trequire_once( dirname( __FILE__ ) . '\/classes\/class-ad-inject-sanitizer.php' );\n\t$sanitizer_classes[ 'XYZ_AMP_Ad_Injection_Sanitizer' ] = array(); \/\/ the array can be used to pass args to your sanitizer and accessed within the class via `$this->args`\n\treturn $sanitizer_classes;\n}\nExtracting Image Dimensions\nAMP requires images to have width and height attributes. When these attributes aren't present in an image tag, AMP-WP will\nattempt to determine them for the image.\nExtraction Methods\nConcurrent Dimension Extraction - PHP 5.3+ and cURL\nIf you're using PHP 5.3+ and have the cURL extension installed, AMP-WP will attempt to determine dimensions for all images\nthat need them concurrently. Only the minimum number of bytes required to determine the dimensions for a given image type\nare retrieved. Dimensions are then cached via transients for subsequent requests. This is the fastest and therefore recommended method.\nSequential Dimension Extraction - PHP 5.2 or no cURL\nIf you're using PHP 5.2 or do not have the cURL extension installed, AMP-WP will attempt to determine image dimensions\nsequentially. Only the minimum number of bytes required to determine the dimensions for a given image type are retrieved,\nbut the time it takes to retrieve each image's dimensions sequentially can still add up. Dimensions are then cached via transients for subsequent requests.\nCustom Dimension Extraction\nYou can implement your own image dimension extraction method by adding a callback to the amp_extract_image_dimensions_batch filter.\namp_extract_image_dimensions_batch callback functions take a single argument, $dimensions by convention, which is a map\/array of image urls to either an array containing the\ndimensions of the image at the url (if another callback for the filter was able to determine them), or false if the dimensions have yet to be determined, e.g.\narray(\n    'http:\/\/i0.wp.com\/placehold.it\/350x150.png' => array(\n        'width' => 350,\n        'height' => 150,\n     ),\n     'http:\/\/i0.wp.com\/placehold.it\/1024x768.png' => false,\n);\nYour custom dimension extraction callback would iterate through the mappings contained in this single argument, determining\ndimensions via your custom method for all image url keys whose values are not arrays of dimensions, e.g.\nfunction my_custom_dimension_extraction_callback( $dimensions ) {\n    foreach ( $dimensions as $url => $value ) {\n        \/\/ Skip if dimensions have already been determined for this image.\n        if ( is_array( $value ) ) {\n            continue;                \n        }\n        $width = <YOUR CUSTOM CODE TO DETERMINE WIDTH>\n        $height = <YOUR CUSTOM CODE TO DETERMINE HEIGHT>\n        $dimensions[ $url ] = array(\n            'width' => $width,\n            'height' => $height,\n         );\n    }\n\n    return $dimensions;\nYour callback needs to return $dimensions so that the value either cascades to the next callback that was added to the amp_extract_image_dimensions_batch filter or is\nreturned to the apply_filter() call (if there are no more unprocessed callbacks).\nThe default callback provided by WP-AMP described above, extract_by_downloading_images, will fire unless explicitly removed, so be sure\nto remove it from the callback chain if you don't want it to, e.g.\n\tremove_filter( 'amp_extract_image_dimensions_batch', array( 'AMP_Image_Dimension_Extractor', 'extract_by_downloading_images' ), 999, 1 );\nNote that if you previously added a custom dimension extraction callback to the amp_extract_image_dimensions filter,\nyou need to update it to hook into the amp_extract_image_dimensions_batch filter instead and iterate over the key value\npairs in the single argument as per the example above.\nAnalytics\nThere are two options you can follow to include analytics tags in your posts.\nPlugin Analytics Options\nThe plugin defines an analytics option to enable the addition of\namp-analytics in your posts. When the plugin is\nactive, an AMP top-level menu appears in the Dashboard with one inner sub-menu called 'Analytics':\n\nSelecting the Analytics sub-menu in the AMP options menu takes us to an Analytics Options entry page, where we can\ndefine the analytics tags we want to have, by specifying the vendor type (e.g. Parsely), and the associated  JSON\nconfiguration.\n\nNotice that the goal of this option of the plugin is to provide a simple mechanism to insert analytics tags;\nit provides very simple validation based solely on the validity of the JSON string provided. It is the users\nresponsibility to make sure that the values in the configuration string and the vendor type used are coherent with\nthe analytics requirements of their site . Please review the documentation in the AMP project  and in AMPByExample.\nThe AMP Analytics options entry form provides a very simple validation feedback mechanism: if the JSON configuration\nstring entered is invalid (i.e. not valid  JSON), an error message (in red) is displayed below the title of the\noptions window and the entry form is reloaded:\n\nAnd, if the configuration provided is actually a valid JSON string, a success message (in green) is displayed at the\ntop of the window below the title, and again the entry form is reloaded.\n\nManually\nAlaternatively, you can use the amp_post_template_analytics filter:\nadd_filter( 'amp_post_template_analytics', 'xyz_amp_add_custom_analytics' );\nfunction xyz_amp_add_custom_analytics( $analytics ) {\n\tif ( ! is_array( $analytics ) ) {\n\t\t$analytics = array();\n\t}\n\n\t\/\/ https:\/\/developers.google.com\/analytics\/devguides\/collection\/amp-analytics\/\n\t$analytics['xyz-googleanalytics'] = array(\n\t\t'type' => 'googleanalytics',\n\t\t'attributes' => array(\n\t\t\t\/\/ 'data-credentials' => 'include',\n\t\t),\n\t\t'config_data' => array(\n\t\t\t'vars' => array(\n\t\t\t\t'account' => \"UA-XXXXX-Y\"\n\t\t\t),\n\t\t\t'triggers' => array(\n\t\t\t\t'trackPageview' => array(\n\t\t\t\t\t'on' => 'visible',\n\t\t\t\t\t'request' => 'pageview',\n\t\t\t\t),\n\t\t\t),\n\t\t),\n\t);\n\n\t\/\/ https:\/\/www.parsely.com\/docs\/integration\/tracking\/google-amp.html\n\t$analytics['xyz-parsely'] = array(\n\t\t'type' => 'parsely',\n\t\t'attributes' => array(),\n\t\t'config_data' => array(\n\t\t\t'vars' => array(\n\t\t\t\t'apikey' => 'YOUR APIKEY GOES HERE',\n\t\t\t)\n\t\t),\n\t);\n\n\treturn $analytics;\n}\nEach analytics entry must include a unique array key and the following attributes:\n\ntype: (string) one of the valid vendors for amp-analytics.\nattributes: (array) any additional valid  attributes to add to the amp-analytics element.\nconfig_data: (array) the config data to include in the amp-analytics script tag. This is json_encode-d on output.\n\nCustom Post Type Support\nBy default, the plugin only creates AMP content for posts. You can add support for other post_types using the post_type parameter used when registering the custom post type (assume our post_type is xyz-review):\nadd_action( 'amp_init', 'xyz_amp_add_review_cpt' );\nfunction xyz_amp_add_review_cpt() {\n\tadd_post_type_support( 'xyz-review', AMP_QUERY_VAR );\n}\nYou'll need to flush your rewrite rules after this.\nIf you want a custom template for your post type:\nadd_filter( 'amp_post_template_file', 'xyz_amp_set_review_template', 10, 3 );\n\nfunction xyz_amp_set_review_template( $file, $type, $post ) {\n\tif ( 'single' === $type && 'xyz-review' === $post->post_type ) {\n\t\t$file = dirname( __FILE__ ) . '\/templates\/my-amp-review-template.php';\n\t}\n\treturn $file;\n}\nWe may provide better ways to handle this in the future.\nPlugin integrations\nJetpack\nJetpack integration is baked in. More support for things like Related Posts to come.\nParse.ly\nParse.ly's WordPress plugin automatically tracks AMP pages when enabled along with this plugin.\nYoast SEO\nIf you're using Yoast SEO, check out the companion plugin here: https:\/\/github.com\/Yoast\/yoastseo-amp\nCompatibility Issues\nThe following plugins have been known to cause issues with this plugin:\n\nCloudflare Rocket Loader (modifies the output of the AMP page, which breaks validation.)\n\n","34":"Elasticsearch for Laravel\nThis is a Laravel (4+) Service Provider for the official Elasticsearch low-level client:\nhttp:\/\/www.elasticsearch.org\/guide\/en\/elasticsearch\/client\/php-api\/current\/index.html\nVersion Matrix\nSince there are breaking changes in Elasticsearch 1.0, your version of Elasticsearch must match the version of this library, which matches the version of the Elasticsearch low-level client.\nIf you are using a version older than 1.0, you must install the 0.4 laravel-elasticsearch branch.  Otherwise, use the 1.0 branch.\nThe master branch will always track the latest version.\n\n\n\nElasticsearch Version\nlaravel-elasticsearch branch\n\n\n\n\n>= 1.0\n1.0, 2.0\n\n\n<= 0.90.*\n0.4\n\n\n\nSupport for v1.1.x of the Elasticsearch client has been added in v1.1 of laravel-elasticsearch.  We'll try to be consistent with this convention going forward.\nUsage\n\n\nRun composer require shift31\/laravel-elasticsearch:~2.0\n\n\nCreate app\/config\/elasticsearch.php, modifying the following contents accordingly:\n\n\n<?php\n\nreturn array(\n    'hosts' => array(\n                    'your.elasticsearch.server:9200'\n                    ),\n    'logPath' => 'path\/to\/your\/elasticsearch\/log',\n);\nThe keys of this array should be named according the parameters supported by Elasticsearch\\Client.\n\nIn the 'providers' array in app\/config\/app.php, if you are using Laravel 4.x, add 'Shift31\\LaravelElasticsearch\\LaravelElasticsearchServiceProvider'.\n\nIf you are using Laravel 5.x, add 'Shift31\\LaravelElasticsearch\\ElasticsearchServiceProvider'. The ServiceProvider will enable the 'Es' facade for you.\n\nUse the Es facade to access any method from the Elasticsearch\\Client class, for example:\n\n$searchParams['index'] = 'your_index';\n$searchParams['size'] = 50;\n$searchParams['body']['query']['query_string']['query'] = 'foofield:barstring';\n\n$result = Es::search($searchParams);\nA friendly reminder:  If you use the facade in a namespaced class (i.e. in a Laravel 5.x controller), you must add use Es; at the top of your file (after <?php of course), or add a backslash in front of any static calls (ex: \\Es::search(...)).\nDefault Configuration\nIf you return an empty array in the config file:\n'hosts' defaults to localhost:9200\n'logPath' defaults to storage_path() . '\/logs\/elasticsearch.log'\n","35":"Slack for Laravel\nThis package allows you to use Slack for PHP easily and elegantly in your Laravel 4 or 5 app. Read the instructions below to get setup, and then head on over to Slack for PHP for usage details.\nRequirements\nLaravel 4 or 5.\nInstallation\nYou can install the package using the Composer package manager. You can install it by running this command in your project root:\ncomposer require maknz\/slack-laravel\nThen create an incoming webhook for each Slack team you'd like to send messages to. You'll need the webhook URL(s) in order to configure this package.\nLaravel 5\nAdd the Maknz\\Slack\\Laravel\\ServiceProvider provider to the providers array in config\/app.php:\n'providers' => [\n  Maknz\\Slack\\Laravel\\ServiceProvider::class,\n],\nThen add the facade to your aliases array:\n'aliases' => [\n  ...\n  'Slack' => Maknz\\Slack\\Laravel\\Facade::class,\n],\nFinally, publish the config file with php artisan vendor:publish. You'll find it at config\/slack.php.\nLaravel 4\nAdd the Maknz\\Slack\\Laravel\\ServiceProvider provider to the providers array in app\/config.php:\n'providers' => [\n  ...\n  'Maknz\\Slack\\Laravel\\ServiceProvider',\n],\nThen add the facade to your aliases array:\n'aliases' => [\n  ...\n  'Slack' => 'Maknz\\Slack\\Laravel\\Facade',\n],\nFinally, publish the config file with php artisan config:publish maknz\/slack. You'll find the config file at app\/config\/packages\/maknz\/slack-laravel\/config.php.\nConfiguration\nThe config file comes with defaults and placeholders. Configure at least one team and any defaults you'd like to change.\nUsage\nThe Slack facade is now your interface to the library. Any method you see being called an instance of Maknz\\Slack\\Client is available on the Slack facade for easy use.\nNote that if you're using the facade in a namespace (e.g. App\\Http\\Controllers in Laravel 5) you'll need to either use Slack at the top of your class to import it, or append a backslash to access the root namespace directly when calling methods, e.g. \\Slack::method().\n\/\/ Send a message to the default channel\nSlack::send('Hello world!');\n\n\/\/ Send a message to a different channel\nSlack::to('#accounting')->send('Are we rich yet?');\n\n\/\/ Send a private message\nSlack::to('@username')->send('psst!');\nNow head on over to Slack for PHP for more examples, including attachments and message buttons.\n","36":"Craft \u2013\u00a0Admin Bar\nFront-end shortcuts for clients logged into Craft CMS.\nRequirements\n\nCraft 3.0 (beta 18)+\nPHP 7.0+\n\nInstallation\n\nDuring Craft 3 beta, follow these instructions for installation.\nRun composer update within the adminbar folder to downlaod dependencies.\nAdd one of the Twig tags to your template:\n\n\nAdmin Bar default tag\nAdmin Bar Twig tag\nEdit Link Twig tag\n\n\nAdmin Bar\n\nAdd the Default Admin Bar\nThe easiest way to add Admin Bar to your website is by adding the tag, {{ adminbar() }}, anywhere within your page template. Admin Bar will appear at the top of any page that includes this tag when someone\u2014who has the permission to view the CP\u2014is logged into your website.\nBecause Admin Bar is HTML, CSS, and Javascript added to your website's front-end, you may need to make some slight adjustments to override Admin Bar's CSS to make it fit your website.\nUsing the Admin Bar Twig Tag\nUsing the tag to add Admin Bar to your template is the same as using {{ adminbar() }}, but the Twig tag allows you more flexibility.\nUse the tag, {{ craft.adminbar.bar() }}, to add Admin Bar anywhere you'd like within your template.\nYou may pass in an array of arguments to make some changes on how Admin Bar looks and functions. In this example, you may pass in the entry that you'd like to appear when someone clicks the \"Edit\" link.\n{% set config = {\n  entry: entry,\n} %}\n\n{{ craft.adminbar.bar(config) }}\nHere is a list of available arguments:\n\n\n\nArgument\nDefault\nDescription\n\n\n\n\ncategory\nnull\nPass in a category object to add an edit link for that category\n\n\nbgColor\n'#000000'\nCSS hex color used for Admin Bar's background color\n\n\nhighlightColor\n'#d85b4b'\nCSS hex color used for rollovers and for the background of the mobile theme\n\n\ntextColor\n'#ffffff'\nCSS hex color for icons and link text\n\n\nentry\nnull\nPass in an entry object to add an edit link for that entry\n\n\nsticky\ntrue\nUses css to position: fixed; Admin Bar to the top of the page\n\n\nuseCss\ntrue\nAdd the default styles to Admin Bar or leave them off and style it your way\n\n\nuseJs\ntrue\nUse the Admin Bar's default Javascript\n\n\n\n\nIn-Content Edit Links\n\nEdit Links for Multiple Entries\nWhen looping through entries in an Element Criteria Model, entries in search results, or related entries to a page, you can now place edit links that make it easier to find and edit these entries.\nTo add an Edit Link, use the tag, {{ craft.Adminbar.edit(entry) }}, and pass in the entry you'd like the link to edit. You could also use Edit Links to add shortcuts to other areas of the CP by passing in a string, {{ craft.Adminbar.edit(craft.config.cpTrigger ~ '\/categories\/myCategories\/5-some-category-page') }}.\nBy default, Edit Links use Javascript to add the links to your page, so you can feel free to use {% cache %} tags around the Twig tag. The only thing a non-logged in user would see is this in the HTML markup: <div class=\"admin_edit\" data-id=\"0\"><\/div>.\nYou can also add developer notes to content editors, or pass along other arguments.\n{% set myNote %}{% spaceless %}\n  {% if loop.last %}\n    <p>The last entry always stays full width, even in the 2-column layout. See the <a href=\"{{ url('style-guide') }}\">Style Guide<\/a> for layout examples.<\/p>\n  {% endif %}\n{% endspaceless %}{% endset %}\n\n{{ craft.Adminbar.edit(entry, {\n  devNote: myNote,\n}) }}\n\nHere is a full list of available arguments:\n\n\n\nArgument\nDefault\nDescription\n\n\n\n\nbgColor\n'#000000'\nBackground color behind the Edit link\n\n\nhighlightColor\n'#d85b4b'\nColor used for rollovers and links\n\n\ntextColor\n'#FFFFFF'\nText color of the Edit link\n\n\ncontainerSelector\nnull\nOutline a parent element to show content editors the entirety of an entry or editable section. See below for an example\n\n\ndevNote\nnull\nDisplay information to content editors. You may use plain text or HTML markup\n\n\nshowEditInfo\ntrue\nIf set to true, the Edit Link will display the last updated date and the name of the author that last saved the entry\n\n\nuseCss\ntrue\nAdd the default styles to Edit Links or leave them off and style it your way\n\n\nuseJs\ntrue\nAdd the default Javascript used by Entry Edit Links. Setting this to false embeds the Entry Edit Link through Twig, instead\n\n\n\nInidcating What Will Change When Editing an Entry\nTo help a content editor realize what part of an Edit Link is editable, the containerSelector argument can select a containing parent HTML element of the Entry Edit Link Twig tag. For example, in the code below, by setting containerSelector to 'li', an outline would appear when a content editor rolls over the <li> element on the page.\n<ul class=\"my_sweet_content\">\n  {% for summary in mySweetContent %}\n    <li>\n      <h3>{{ summary.title }}<\/h3>\n      <p>{{ summary.teaser }}<\/p>\n      <a href=\"{{ summary.url }}\">Read more<\/a>\n      \n      {{ craft.Adminbar.edit(summary, {\n        containerSelector: 'li',\n      }) }}\n    <\/li>\n  {% endfor %}\n<\/ul>\n\n\nConfiguration settings\nThe config file gives you the ability to adjust how Admin Bar looks and functions in multiple environments. It also allows you to create additional links for the Admin Bar, and allows for plugin actions to be called through these additional links.\nHere are the settings you can change with the config file:\nAdmin Bar\n\n\n\nSetting\nDefault\nDescription\n\n\n\n\nadditionalLinks\n[]\nAdd links to Admin Bar using the properties found below\n\n\ncacheBar\ntrue\nEnable caching of Admin Bar links\n\n\ndisplayGreeting\ntrue\nDisplays the logged in user's photo (if it's set) and \"Hi, [friendlyname]\"\n\n\ndisplayDashboardLink\ntrue\nA link to the CP Dashboard\n\n\ndisplayDefaultEditSection\ntrue\nDisplay the name of the section in the default entry\/category edit link\n\n\ndisplaySettingsLink\ntrue\nA link to the CP Settings page that appears only to admins\n\n\ndisplayLogout\ntrue\nLogs you out of Craft CMS\n\n\nenableMobileMenu\ntrue\nEnables Admin Bar to display a separate mobile theme below a width of 600 pixels\n\n\nscrollLinks\ntrue\nEnable Admin Bar to scroll horizontally when the browser window doesn't have enough room for all of the links\n\n\n\nEntry Edit Links\n\n\n\nSetting\nDefault\nDescription\n\n\n\n\ndisplayEditDate\ntrue\nShows the date of the last time the entry was updated\n\n\ndisplayEditAuthor\ntrue\nShows the friendlyName of the person who last saved the entry\n\n\ndisplayRevisionNote\ntrue\nDisplays text added to the \"Notes about your changes\"\u2014a.k.a. Version Notes\u2014field found when editing an entry\n\n\n\nAdditional Links\nYou can add links to Admin Bar using the config file by passing properties into an array, called additionalLinks. There are examples commented out in the config.php file, and here are the properties you can use to create links.\n\n\n\nProperty\nValues\nDescription\n\n\n\n\ntitle\nstring\nAppears as the label for the link\n\n\nurl\nstring\nDepending on the type property, the url represents the location or action of the link\n\n\ntitle\n'url', 'cpUrl', 'action'\nIf the type is 'url', the url value should be an absolute URL or a path relative to the site root. If the type is 'cpUrl', the url value should be a path relative to your site's CP root. If the type is 'action', set the value for url to the path used by the Controller Action\n\n\nparams\nstring\nPasses along url parameters, as documented here. This only supports this string format: 'foo=1&bar=2'\n\n\nprotocol\nstring\nChanges the url protocol, as documented here\n\n\nmustShowScriptName\nstring\nAppends index.php, as documented here\n\n\npermissions\narray\nAn array of required permissions that are needed for this link to be displayed. All permissions in this array will be required for the link to appear\n\n\n\n\nTo Do\n\nNew icon \ud83d\udc34\nUpdate plugin to support Craft 3\nAdd a new type to be used within multiple entries.\nChange\u2014in Craft 3 version\u2014Embed Options in Embed Tag to array\nSetup upgrade migration from Craft 2 to Craft 3\nGet Plguin > Settings > Admin Bar working\n\n\nReleases\nRelease notes can be found at CHANGELOG.md\nPlease, let me know if this plugin is useful or if you have any suggestions or issues. @wbrowar\n","37":"Maintainers\n\nDMDev (this is an example project)\n\nContributors\nFrank is used by NPR as an example project for technical writers.\nLicense\nThe MIT License (MIT)\nCopyright (c) 2011-2012 Ioseb Dzmanashvili and Irakli Nadareishvili\nPermission is hereby granted, free of charge, to any person obtaining\na copy of this software and associated documentation files (the\n'Software'), to deal in the Software without restriction, including\nwithout limitation the rights to use, copy, modify, merge, publish,\ndistribute, sublicense, and\/or sell copies of the Software, and to\npermit persons to whom the Software is furnished to do so, subject to\nthe following conditions:\nThe above copyright notice and this permission notice shall be\nincluded in all copies or substantial portions of the Software.\nTHE SOFTWARE IS PROVIDED 'AS IS', WITHOUT WARRANTY OF ANY KIND,\nEXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\nMERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.\nIN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY\nCLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,\nTORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE\nSOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n","38":"#\u00a0Embed\n\n\n\n\n\n\n\n\nPHP library to get information from any web page (using oembed, opengraph, twitter-cards, scrapping the html, etc). It's compatible with any web service (youtube, vimeo, flickr, instagram, etc) and has adapters to some sites like (archive.org, github, facebook, etc).\nRequirements:\n\nPHP 5.4+\nCurl library installed\n\nIf you need PHP 5.3 support, use the 1.x version (but not maintained anymore)\nOnline demo\nhttp:\/\/oscarotero.com\/embed2\/demo\nInstallation\nThis package is installable and autoloadable via Composer as embed\/embed.\n$ composer require embed\/embed\n\nIf you don't use composer, you have to download the package and include the autoloader:\ninclude('Embed\/src\/autoloader.php');\nUsage\nuse Embed\\Embed;\n\n\/\/Load any url:\n$info = Embed::create('https:\/\/www.youtube.com\/watch?v=PP1xn5wHtxE');\n\n\/\/Get content info\n\n$info->title; \/\/The page title\n$info->description; \/\/The page description\n$info->url; \/\/The url as reported by OpenGraph, falling back to the tag <meta rel='canonical'>\n$info->canonicalUrl;  \/\/The canonical url, as reported by the tag <meta rel='canonical'>\n$info->type; \/\/The page type (link, video, image, rich)\n$info->tags; \/\/The page keywords (tags)\n\n$info->images; \/\/List of all images found in the page\n$info->image; \/\/The image choosen as main image\n$info->imageWidth; \/\/The width of the main image\n$info->imageHeight; \/\/The height of the main image\n\n$info->code; \/\/The code to embed the image, video, etc\n$info->width; \/\/The width of the embed code\n$info->height; \/\/The height of the embed code\n$info->aspectRatio; \/\/The aspect ratio (width\/height)\n\n$info->authorName; \/\/The (video\/article\/image\/whatever) author \n$info->authorUrl; \/\/The author url\n\n$info->providerName; \/\/The provider name of the page (youtube, twitter, instagram, etc)\n$info->providerUrl; \/\/The provider url\n$info->providerIcons; \/\/All provider icons found in the page\n$info->providerIcon; \/\/The icon choosen as main icon\n\n$info->publishedDate; \/\/The (video\/article\/image\/whatever) published date\nCustomization\nYou can set some options using an array as second argument. In this array you can configure the adapters, providers, resolvers, etc.\nThe adapter\nThe adapter is the class that get all information of the page from the providers and choose the best result for each value. For example, a page can provide multiple titles from opengraph, twitter cards, oembed, the <title> html element, etc, so the adapter get all this titles and choose the best one.\nEmbed has an generic adapter called \"Webpage\" to use in any web but has also some specific adapters for sites like archive.org, facebook, google, github, spotify, etc, that provides information using their own apis, or have any other special issue.\nYou can configure these adapters and even create your own adapter, that must implement the Embed\\Adapters\\AdapterInterface.\nThe available options for the adapters are:\n\nminImageWidth (int): Minimal image width used to choose the main image\nminImageHeight (int): Minimal image height used to choose the main image\nimagesBlacklist (array): Images that you don't want to be used. Could be plain text or Url match pattern.\ngetBiggerImage (bool): Choose the bigger image as the main image (instead the first found, that usually is the most relevant).\ngetBiggerIcon (bool): The same than getBiggerImage but used to choose the main icon\n\n$config = [\n\t'adapter' => [\n\t\t'class' => 'MyCustomClass', \/\/Your custom adapter\n\n\t\t'config' => [\n\t\t\t'minImageWidth' => 16,\n            'minImageHeight' => 16,\n            'imagesBlacklist' => null,\n            'getBiggerImage' => false,\n            'getBiggerIcon' => false,\n\t\t]\n    ]\n];\nThe providers\nThe providers get the data from different sources. Each source has it's own provider. For example, there is a provider for opengraph, other for twitter cards, for oembed, html, etc. The providers that receive options are:\noembed\nUsed to get data from oembed api if it's available. It accepts two options:\n\nparameters (array): Extra query parameters to send with the oembed request\nembedlyKey (string): If it's defined, use embed.ly api as fallback oembed provider.\niframelyKey (string): If it's defined, use iframe.ly api as fallback oembed provider.\n\nhtml\nUsed to get data directly from the html code of the page:\n\nmaxImages (int): Max number of images fetched from the html code (searching for the <img> elements). By default is -1 (no limit). Use 0 to no get images.\n\nfacebook\nThis provider is used only for facebook pages, to get information from the graph api\n\nkey (string): the key used\n\ngoogle\nThis provider is used only for google maps, to generate the embed code using the embed api\n\nkey (string): the key used\n\nsoundcloud\nUsed only for soundcloud pages, to get information using its api.\n\nkey (string): to get info from soundcloud API.\n\nThe request resolver\nEmbed uses the Embed\\RequestResolvers\\Curl class to resolve all requests using the curl library. You can set options to the curl request or use your custom resolver creating a class implementing the Embed\\RequestResolvers\\RequestResolverInterface.\nThe resolver configuration is defined under the \"resolver\" key and it has two options:\n\nclass: Your custom class name if you want to use your own implementation\nconfig: The options passed to the class. If you use the default curl class, the config are the same than the curl_setopt PHP function\n\n\/\/ CURL\n$config = [\n    'resolver' => [\n        'class' => 'Embed\\\\RequestResolvers\\\\Curl', \/\/ The default resolver used\n\n        'config' => [\n            CURLOPT_MAXREDIRS => 20,\n            CURLOPT_CONNECTTIMEOUT => 10,\n            CURLOPT_TIMEOUT => 10,\n            CURLOPT_SSL_VERIFYPEER => false,\n            CURLOPT_SSL_VERIFYHOST => false,\n            CURLOPT_ENCODING => '',\n            CURLOPT_AUTOREFERER => true,\n            CURLOPT_USERAGENT => 'Embed PHP Library',\n            CURLOPT_IPRESOLVE => CURL_IPRESOLVE_V4,\n        ]\n    ]\n];\n\n\/\/ Guzzle (5.x)\n$config = [\n    'resolver' => [\n        'class' => 'Embed\\\\RequestResolvers\\\\Guzzle5', \/\/ Guzzle5 resolver used\n\n        'config' => [\n            \/\/ optional: if you need to use your custom Guzzle instance\n            'client' => $myGuzzleClient,\n        ]\n    ]\n];\nYou can see here the RequestResolvers included.\nImage info\nTo check the images and get their mimetype and dimmensions, we have the class Embed\\ImageInfo\\Curl. This class uses curl to make request, get the first bytes to get the image type and dimmensions and close the connection. So the image wont be downloaded entirely, just until the downloaded data is enought to get this information.\nLike the resolver class, you can provide your own image class (it must implement the Embed\\ImageInfo\\ImageInfoInterface) and\/or change the configuration. The available options are the same:\n\nclass: Your custom class name if you want to use your own implementation\nconfig: The options passed to the class. If you use the default curl class, the config are the same than the curl_setopt PHP function\n\n\/\/CURL\n$config = [\n    'image' => [\n        'class' => 'Embed\\\\ImageInfo\\\\Curl', \/\/The default imageInfo used\n\n        'config' => [\n            CURLOPT_MAXREDIRS => 20,\n            CURLOPT_CONNECTTIMEOUT => 10,\n            CURLOPT_TIMEOUT => 10,\n            CURLOPT_SSL_VERIFYPEER => false,\n            CURLOPT_SSL_VERIFYHOST => false,\n            CURLOPT_ENCODING => '',\n            CURLOPT_AUTOREFERER => true,\n            CURLOPT_USERAGENT => 'Embed PHP Library',\n            CURLOPT_IPRESOLVE => CURL_IPRESOLVE_V4,\n        ]\n    ]\n];\n\n\/\/ Guzzle (5.x)\n$config = [\n    'image' => [\n        'class' => 'Embed\\\\ImageInfo\\\\Guzzle5',\n\n        'config' => [\n            'client' => $myGuzzleClient,\n        ]\n    ]\n];\nYou can see here the ImageInfo implementations included.\nFull configuration example\nuse Embed\\Embed;\n\n$config = [\n\t'adapter' => [\n\t\t'config' => [\n\t\t\t'minImageWidth' => 16,\n            'minImageHeight' => 16,\n            'imagesBlacklist' => [\n                'http:\/\/example.com\/full\/path\/to\/image.jpg',\n                'http?:\/\/test.*\/*.png\/',\n                '*\/bad_image.gif'\n            ]\n\t\t]\n\t],\n    'providers' => [\n        'oembed' => [\n            'parameters' => [],\n            'embedlyKey' => null\n        ],\n        'html' => [\n            'maxImages' => 3\n        ],\n        'facebook' => [\n            'key' => 'our-access-token'\n        ]\n    ],\n    'resolver' => [\n        'config' => [\n            CURLOPT_USERAGENT => 'My spider',\n            CURLOPT_MAXREDIRS => 3\n        ]\n    ]\n\t'image' => [\n\t\t'class' => 'App\\\\MyImageInfoClass'\n\t]\n];\n\n\/\/To use it:\n$info = Embed::create('https:\/\/www.youtube.com\/watch?v=PP1xn5wHtxE', $config);\nAccess to more data\nAs said before, the adapter get the data from all providers and choose the best values. But you can get the data returned by a specific provider:\nuse Embed\\Embed;\n\n\/\/Get the info\n$info = Embed::create('https:\/\/www.youtube.com\/watch?v=PP1xn5wHtxE');\n\n\/\/Get the oembed provider\n$oembed = $info->getProvider('oembed');\n\n\/\/Get the oembed title:\necho $oembed->getTitle();\n\n\/\/Get any value returned by oembed api\necho $oembed->bag->get('author_name');\n","39":"Oembed\nLaravel 4 - Retrieve page info using oembed, opengraph, etc.\n\nServer Requirements\nRequire PHP 5.4+ or higher.\n\nInstallation\nOpen your composer.json file, and add the new required package.\n\"pingpong\/oembed\": \"1.0.*\"\n\nNext, open a terminal and run.\ncomposer update\n\nAfter the composer updated. Add new service provider in app\/config\/app.php.\n    'Pingpong\\Oembed\\OembedServiceProvider'\nNext, Add new alias.\n    'Oembed'           => 'Pingpong\\Oembed\\Facades\\Oembed',\nDone.\nUsage\nBasic Usage\n$info = Oembed::get('https:\/\/www.youtube.com\/watch?v=PP1xn5wHtxE');\n\n\/\/Get content info\n\n$info->title; \/\/The page title\n$info->description; \/\/The page description\n$info->url; \/\/The canonical url\n$info->type; \/\/The page type (link, video, image, rich)\n\n$info->images; \/\/List of all images found in the page\n$info->image; \/\/The image choosen as main image\n$info->imageWidth; \/\/The with of the main image\n$info->imageHeight; \/\/The height  of the main image\n\n$info->code; \/\/The code to embed the image, video, etc\n$info->width; \/\/The with of the embed code\n$info->height; \/\/The height of the embed code\n$info->aspectRatio; \/\/The aspect ratio (width\/height)\n\n$info->authorName; \/\/The (video\/article\/image\/whatever) author \n$info->authorUrl; \/\/The author url\n\n$info->providerName; \/\/The provider name of the page (youtube, twitter, instagram, etc)\n$info->providerUrl; \/\/The provider url\n$info->providerIcons; \/\/All provider icons found in the page\n$info->providerIcon; \/\/The icon choosen as main icon\nCaching the results.\n$info = Oembed::cache($url, $options);\nDocumentation\nBecause this package using the library oscarotero\/Embed, please refer to its documentation for more info.\nLicense\nThis package is open-sourced software licensed under The BSD 3-Clause License\n","40":"Flatten\n\n\n\n\n\n\n\nFlatten is a powerful cache system for caching pages at runtime.\nWhat it does is quite simple : you tell him which page are to be cached, when the cache is to be flushed, and from there Flatten handles it all. It will quietly flatten your pages to plain HTML and store them. That whay if an user visit a page that has already been flattened, all the PHP is highjacked to instead display a simple HTML page.\nThis will provide an essential boost to your application's speed, as your page's cache only gets refreshed when a change is made to the data it displays.\nSetup\nInstallation\nFlatten installs just like any other package, via Composer : composer require anahkiasen\/flatten.\nThen if you're using Laravel, add Flatten's Service Provider to you config\/app.php file :\n'Flatten\\FlattenServiceProvider',\nAnd its facade :\n'Flatten' => 'Flatten\\Facades\\Flatten',\nConfiguration\nAll the options are explained in the config.php configuration file. You can publish it via artisan config:publish anahkiasen\/flatten.\nHere is a preview of the configuration options available in said file :\n\/\/ The default period during which a cached page should be kept (in minutes)\n\/\/ 0 means the page never gets refreshed by itself\n'lifetime'     => 0,\n\n\/\/ The different pages to be ignored when caching\n\/\/ They're all regexes so go crazy\n'ignore'       => array(),\n\n\/\/ List only specific pages to cache, useful if you have a lot of\n\/\/ pages you don't want to see cached\n\/\/ The ignored pages will still be substracted from this array\n'only'         => array(),\n\n\/\/ An array of string or variables to add to the salt being used\n\/\/ to differentiate pages\n'saltshaker'   => array(),\nUsage\nThe pages are cached according to two parameters : their path and their method. Only GET requests get cached as all the other methods are dynamic by nature.\nAll of the calls you'll make, will be to the Flatten\\Facades\\Flatten facade.\nQuery strings are taken into account in the cache and pages will different query strings will have different caches.\nBuilding\nFlatten can cache all authorized pages in your application via the artisan flatten:build command. It will crawl your application and go from page to page, caching all the pages you allowed him to.\nFlushing\nSometimes you may want to flush a specific page or pattern. If per example you cache your users's profiles, you may want to flush those when the user edit its informations.\nYou can do so via the following methods :\n\/\/ Manual flushing\nFlatten::flushAll();\nFlatten::flushPattern('users\/.+');\nFlatten::flushUrl('http:\/\/localhost\/users\/taylorotwell');\n\n\/\/ Flushing via an UrlGenerator\nFlatten::flushRoute('user', 'taylorotwell');\nFlatten::flushAction('UsersController@user', 'taylorotwell');\n\n\/\/ Flushing template sections (see below)\nFlatten::flushSection('articles');\nRuntime caching\nYou don't have to cache all of a page, you can fine-tune your cache in smaller cached sections.\nIn PHP you'd do it like this :\n<h1>This will always be dynamic<\/h1>\n<?php foreach ($articles as $article): ?>\n\t<?= $article->name ?>\n<?php endforeach; ?>\n\n<h1>This will not<\/h1>\n<?php Flatten::section('articles', function () use ($articles) { ?>\n\t<?php foreach ($articles as $article): ?>\n\t\t<?= $article->name ?>\n\t<?php endforeach; ?>\n<?php }); ?>\nYou can also specify for how long you want that section to be cached by adding an argument to section :\n<!-- This section will be cached for 10 minutes -->\n<?php Flatten::section('articles', 10, function () use ($articles) { ?>\n\t<?php foreach ($articles as $article): ?>\n\t\t<?= $article->name ?>\n\t<?php endforeach; ?>\n<?php }); ?>\nFlatten also hooks into the Blade templating engine for a leaner syntax. Let's rewrite our above example :\n<h1>This will always be dynamic<\/h1>\n@foreach($articles as $articles)\n\t{{ $article->name }}\n@endforeach\n\n<h1>This will not<\/h1>\n@cache('articles', 10)\n\t@foreach($articles as $article)\n\t\t{{ $article->name }}\n\t@endforeach\n@endcache\nKickstarting\nYou can speed up Flatten even more by using the Flatten::kickstart method. It requires a little more boilerplate code but can enhance performances dramatically.\nBasically you want to call that before everything else (ie. before even loading Composer). On Laravel you'd put that code at the top of bootstrap\/autoload.php.\nYou use it like that :\nrequire __DIR__.'\/..\/vendor\/anahkiasen\/flatten\/src\/Flatten\/Flatten.php';\n\nFlatten\\Flatten::kickstart();\nIf you have things in your saltshaker, you'll need to find faster raw methods to get these and pass the salts as arguments :\nrequire __DIR__.'\/..\/vendor\/anahkiasen\/flatten\/src\/Flatten\/Flatten.php';\n\n$salt = mysql_query('SOMETHING');\nFlatten\\Flatten::kickstart($salt);\nUsing outside of Laravel\nFlatten can easily be used outside of Laravel, for this you'll basically only ever use two methods. What you basically want to do is call Flatten::start at the top of the page, and Flatten::end at the bottom.\n<?php\nrequire 'vendor\/autoload.php';\n\nuse Flatten\\Facades\\Flatten;\n?>\n\n<?php Flatten::start() ?>\n<!DOCTYPE html>\n\t<head><\/head>\n\t<body><\/body>\n<\/html>\n<?php Flatten::end() ?>\n","41":"curl\nA basic CURL wrapper for PHP (see http:\/\/php.net\/curl for more information about the libcurl extension for PHP)\nInstallation\nClick the download link above or git clone git:\/\/github.com\/shuber\/curl.git\nUsage\nInitialization\nSimply require and initialize the Curl class like so:\nrequire_once 'curl.php';\n$curl = new Curl;\n\nPerforming a Request\nThe Curl object supports 5 types of requests: HEAD, GET, POST, PUT, and DELETE. You must specify a url to request and optionally specify an associative array or string of variables to send along with it.\n$response = $curl->head($url, $vars = array());\n$response = $curl->get($url, $vars = array()); # The Curl object will append the array of $vars to the $url as a query string\n$response = $curl->post($url, $vars = array());\n$response = $curl->put($url, $vars = array());\n$response = $curl->delete($url, $vars = array());\n\nTo use a custom request methods, you can call the request method:\n$response = $curl->request('YOUR_CUSTOM_REQUEST_TYPE', $url, $vars = array());\n\nAll of the built in request methods like put and get simply wrap the request method. For example, the post method is implemented like:\nfunction post($url, $vars = array()) {\n    return $this->request('POST', $url, $vars);\n}\n\nExamples:\n$response = $curl->get('google.com?q=test');\n\n# The Curl object will append '&some_variable=some_value' to the url\n$response = $curl->get('google.com?q=test', array('some_variable' => 'some_value'));\n\n$response = $curl->post('test.com\/posts', array('title' => 'Test', 'body' => 'This is a test'));\n\nAll requests return a CurlResponse object (see below) or false if an error occurred. You can access the error string with the $curl->error() method.\nThe CurlResponse Object\nA normal CURL request will return the headers and the body in one response string. This class parses the two and places them into separate properties.\nFor example\n$response = $curl->get('google.com');\necho $response->body; # A string containing everything in the response except for the headers\nprint_r($response->headers); # An associative array containing the response headers\n\nWhich would display something like\n<html>\n<head>\n<title>Google.com<\/title>\n<\/head>\n<body>\nSome more html...\n<\/body>\n<\/html>\n\nArray\n(\n    [Http-Version] => 1.0\n    [Status-Code] => 200\n    [Status] => 200 OK\n    [Cache-Control] => private\n    [Content-Type] => text\/html; charset=ISO-8859-1\n    [Date] => Wed, 07 May 2008 21:43:48 GMT\n    [Server] => gws\n    [Connection] => close\n)\n\nThe CurlResponse class defines the magic __toString() method which will return the response body, so echo $response is the same as echo $response->body\nCookie Sessions\nBy default, cookies will be stored in a file called curl_cookie.txt. You can change this file's name by setting it like this\n$curl->cookie_file = 'some_other_filename';\n\nThis allows you to maintain a session across requests\nBasic Configuration Options\nYou can easily set the referer or user-agent\n$curl->referer = 'http:\/\/google.com';\n$curl->user_agent = 'some user agent string';\n\nYou may even set these headers manually if you wish (see below)\nSetting Custom Headers\nYou can set custom headers to send with the request\n$curl->headers['Host'] = 12.345.678.90;\n$curl->headers['Some-Custom-Header'] = 'Some Custom Value';\n\nSetting Custom CURL request options\nBy default, the Curl object will follow redirects. You can disable this by setting:\n$curl->follow_redirects = false;\n\nYou can set\/override many different options for CURL requests (see the curl_setopt documentation for a list of them)\n# any of these will work\n$curl->options['AUTOREFERER'] = true;\n$curl->options['autoreferer'] = true;\n$curl->options['CURLOPT_AUTOREFERER'] = true;\n$curl->options['curlopt_autoreferer'] = true;\n\nTesting\nUses ztest, simply download it to path\/to\/curl\/test\/ztest (or anywhere else in your php include_path)\nThen run test\/runner.php\nContact\nProblems, comments, and suggestions all welcome: shuber@huberry.com\n","42":"php-image-optim\n\nThe purpose of this library is to help automate the optimisation of images via the command line in PHP,\nInstallation:\nThe library is PSR-0 compliant and the simplest way to install it is via composer, simply add:\n{\n    \"require\": {\n        \"bensquire\/php-image-optim\": \"dev-master\"\n    }\n}\n\ninto your composer.json, then run 'composer install' or 'composer update' as required.\nExample:\nThis example demonstrates the optimisation of a PNG file, by chaining several commands together.\n<?php\n    use PHPImageOptim\\Tools\\Png\\AdvPng;\n    use PHPImageOptim\\Tools\\Png\\OptiPng;\n    use PHPImageOptim\\Tools\\Png\\PngCrush;\n    use PHPImageOptim\\Tools\\Png\\PngOut;\n    use PHPImageOptim\\Tools\\Png\\PngQuant;\n\n    include('..\/..\/vendor\/autoload.php');\n\n    $advPng = new AdvPng();\n    $advPng->setBinaryPath('\/usr\/local\/bin\/advpng');\n\n    $optiPng = new OptiPng();\n    $optiPng->setBinaryPath('\/usr\/local\/bin\/optipng');\n\n    $pngOut = new PngOut();\n    $pngOut->setBinaryPath('\/usr\/bin\/pngout');\n\n    $pngCrush = new PngCrush();\n    $pngCrush->setBinaryPath('\/usr\/local\/bin\/pngcrush');\n\n    $pngQuant = new PngQuant();\n    $pngQuant->setBinaryPath('\/usr\/local\/bin\/pngquant');\n\n    $optim = new PHPImageOptim();\n    $optim->setImage('\/tmp\/lenna.png');\n    $optim  ->chainCommand($pngQuant)\n            ->chainCommand($advPng)\n            ->chainCommand($optiPng)\n            ->chainCommand($pngCrush)\n            ->chainCommand($pngOut);\n    $optim->optimise();\n\n","43":"Price of Admission\nThe Price of Admission series was published in March 2016. This website was built by Becca Aaronson using a variable of The Texas Tribune's app kit.\nQuickstart\nRun this command in your project's folder:\ncurl -fsSL https:\/\/github.com\/texastribune\/newsapps-app-kit\/archive\/price-of-admission-template.tar.gz | tar -xz --strip-components=1\nNext, npm install.\nIf this is your first time to ever use the kit, you need to follow the steps in your terminal for Google authorization, i.e. go to the given link and paste the token into the terminal.\nCreating Templates with Google Docs & Spreadsheets\nThis kit allows you to pull in content from Google Docs and Spreadsheets. To use the story.html template file, you'll need to set up a Google doc using our basic template and follow these steps.\nSet up config.js\nUpdate the config.js, and add the unique Google Tokens for your  document(s) and spreadsheet(s). For spreadsheets, you'll need to designate how to process the data, as either a keyvalue or objectlist. You'll also need to assign a \"name\" in the config.js for each doc\/sheet, which will become the name of a json file in your \/data folder with the data pulled in from that document. You'll then be able to refer to that name throughout your project to pull data from that file.\nUpdate project data with npm run data\/fetch\nTo pull or update the data from your docs\/sheets in your project, run the command npm run data\/fetch. You can double-check the \/data folder to make sure your files and data are updated.\nCreate index.html \/ additional story pages\nAll of the hard work to build your story page occurs in \/templates\/layouts\/story.html. To create a new page, add an HTMl file inside \/app, starting with index.html and insert the following code:\n{% extends 'layouts\/story.html' %}\n{% set context = data.story_one %}\n\n{% block styles %}\n{{ super() }}\n{% endblock %}\n\n{% block script %}\n{{ super()}}\n{% endblock %}\n\nThe line {% set context = data.story_one %} tells the project which data file to pull in to build the template. In this case, the file will look for the story_one.json file, which hopefully you just set up in your config.js with the name story_name and pulled in with the command npm run data\/fetch.\nAdditional HTML pages set up in your app will set the name of the file as the slug. For example, when you deploy, index.html will be bucket.org\/project-name and story-two.html will be bucket.org\/project-name\/story-two\/.\nSample Google Doc ArchieML template\n\nid \u2014 This is the Part Number for the series. It's used to set up the story navigation in the .nav__aside and .nav__footer modules found in \/app\/styles\/_nav.scss. It's also used to indicate which lead art class to use, which you define in \/app\/styles\/_utils\/ > @mixin story-header.\nscript \u2014 Sets which script file to load in the base.html. You don't need to include the extension .js. Default: main\nheadline - Your story headline. Appears in the .storytop, .nav__aside and .nav__footer\npub_date - Publish date. Use AP style :)\nslug - URL slug for your story. It must match the filename of the HTML file for that story.\nshort_head - Shorter headline, in case you need it.\n[authors] - Currently supports up 2 individual author names. If you include email, the name will be linked. See \/app\/templates\/macros\/authors.html\n{lead_art}\n\nphoto_url: test.jpg \/\/ file name with extension type. Looks for file in \/app\/assets\/images\nthumbnail_url: test.jpg \/\/ for thumbnail version in .nav__aside\ncaption: Vestibulum ullamcorper mauris at ligula. Sed hendrerit. \/\/ Photo caption\n-credit: Person McPhotographer \/\/ Credit\n\n\nfb_art - Specific image file for Facebook OG meta data. Include extension; looks for file in \/app\/assets\/images\nsummary - Summary description for Facebook OG meta description.\ntwitter_text - Text for Twitter share button\n[+prose] \u2014 ALL THE MAGIC STUFF. This will convert regular story text into HTML. Paragraphs in prose must be separated by an empty return space to be read as new <p>. Hyperlinked text will automatically become <a href=\"link.html\">. Inside [+prose] you can also add ad units, the nav__aside, and a variety of other components. See full list and make additional components in \/app\/template\/macros\/prose.html.\n\n{.ad} - Insert ad unit into prose.\n\ntype - Options: inside [468, 60], sidebar [350, 200], banner [728, 90].\nid - Define ad units in \/app\/scripts\/includes\/adLoader.js and set id to indicate which ad unit\nalignment - Optional. Will add alignment class. Options: right-lock, left-lock, basic.\n\n\nsubhead - Insert subheader into story text with bold styles.\n{.image} - Insert image.\n\nphoto_url - file.jpg \/\/ Include file type extension. Looks for file in \/app\/assets\/images\ncaption - Photo caption text\ncredit - Photo credit\nalignment - Options: basic (full width), right-lock (float right on Desktop), left-lock(float left on Desktop)\n\n\ndisclosure - Italicizes text \/ adds .disclosure styles.\nrepublish - Adds prompt\/link to additional republish page. Requires you to separately set up an HTML page with the story_republish.html template.\n\n\n\nWebpack\nThis kit uses the webpack module bundler. You can see an example of how to import files in app\/scripts\/main.js - as the kit comes preloaded with an import of the app\/scripts\/includes\/adLoader.js partial script. If you're using a big library like JQuery or D3, we recommend downloading the node module, and adding it to the webpack.config.js file as a plugin.\nHere's an example:\n  plugins: [\n    new webpack.ProvidePlugin({\n      $: 'jquery',\n      jQuery: 'jquery',\n      'window.jQuery': 'jquery',\n      d3: 'd3'\n    })\n  ],\n\nThen you can set global variables in your \/app\/scripts\/main.js and reference the libraries throughout your project. When you compile the project later for deployment, it will include the libraries.\nDevelopment\nRun the following command to start the development server:\nnpm run serve\nThen visit http:\/\/localhost:3000 to see your work.\nConnect to S3\nTo use the commands to deploy your project to Amazon S3, you'll need to add a profile to your ~\/.aws\/config. It should look something like this:\n[profile newsapps]\naws_access_key_id=YOUR_UNIQUE_ID\naws_secret_access_key=YOUR_SECRET_ACCESS_KEY\n\nDeployment\nRun these commands to build and deploy:\nnpm run build\nnpm run deploy\n\nThe project will deploy using the S3 bucket and slug found in your config.js.\nAssets\nThe graphics kit comes with an empty app\/assets folder for you to store images, fonts and data files. The kit works best if you add these files to app\/assets\/images, app\/assets\/fonts and app\/assets\/data. These files will automatically be ignored by git hub, if added to the proper folders, to prevent a storage overload and to keep files locally that may have sensitive information in an open source project.\nAvailable Commands\nnpm run data\/fetch\nPulls down the project's spreadsheet and\/or documents and creates data files in the data folder. Will authenticate you with your Google account if needed.\nnpm run serve\nStarts the development server.\nnpm run build\nBuild the project for production.\nnpm run deploy\/dev\nDeploys the project.\n","44":"The News Apps App Kit\nThis package helps jump start special Tribune features and series that are built outside the regular CMS. It's Tribune-centric, but easy to update and transform to fit your needs. If you're working on a graphic, use the News Apps Graphic Kit. It is similar to the App Kit, but comes with NPR's pym.js to help you embed with a responsive <iframe>.\nFeatures\n\nLive reloading and viewing powered by BrowserSync\nCompiling of Sass\/SCSS with Ruby Sass\nCSS prefixing with autoprefixer\nCSS sourcemaps with gulp-sourcemaps\nCSS compression with csso\nJavaScript linting with jshint\nJavaScript compression with uglifyjs\nTemplate compiling with nunjucks\nImage compression with gulp-imagemin\nAsset revisioning with gulp-rev and gulp-rev-replace\n\nQuickstart\nRun this command in your project's folder:\ncurl -fsSL https:\/\/github.com\/texastribune\/newsapps-app-kit\/archive\/master.tar.gz | tar -xz --strip-components=1\nNext, npm install.\nIf this is your first time to ever use the kit, you need to authorize your computer: npm run spreadsheet\/authorize\nAdd your Google sheet's ID to the config.json, and override any sheets that need to be processed differently. (keyvalue or objectlist)\nNow get to work!\nDevelopment\nRun the following command to start the development server:\ngulp serve\nThen visit http:\/\/localhost:3000 to see your work.\nConnect to S3\nTo use the commands to deploy your project to Amazon S3, you'll need to add a profile to your ~\/.aws\/config. It should look something like this:\n[profile newsapps]\naws_access_key_id=YOUR_UNIQUE_ID\naws_secret_access_key=YOUR_SECRET_ACCESS_KEY\n\nDeployment\nRun these commands to build and deploy:\ngulp\nnpm run deploy\n\nThe project will deploy using the S3 bucket and slug found in your package.json.\nAssets\nThe graphics kit comes with an empty app\/assets folder for you to store images, fonts and data files. The kit works best if you add these files to app\/assets\/images, app\/assets\/fonts and app\/assets\/data. These files will automatically be ignored by git hub, if added to the proper folders, to prevent a storage overload and to keep files locally that may have sensitive information in an open source project.\nAvailable Commands\nnpm run spreadsheet\/authorize\nAllows your computer to interact with the scraper. Only needs to be done once. Any future uses of the graphic kit can skip this.\nnpm run spreadsheet\/fetch\nPulls down the project's spreadsheet and processes it into the data.json file.\nnpm run spreadsheet\/edit\nOpens the project's spreadsheet in your browser.\nnpm run deploy\nDeploys the project.\nnpm run assets\/push\nPushes the raw assets to the S3 bucket.\nnpm run assets\/pull\nPulls the raw assets down to the local environment.\n","45":"A\/B\/n Testing with Google Tag Manager\nScripts that support A\/B\/n + Multivariate Testing (for [The Next Web]\n(http:\/\/thenextweb.com)), via Google Tag Manager.\nFeatures\n\nSupports: A\/B\/n testing and Multivariate testing.\nIntegration with Google Analytics, will send data for event to the dataLayer.\nSupported via Google Tag Manager but also easily configurable to run natively.\n\n\nUsage\nBoth scripts work in a similar order and require the use of Google Tag Manager\nto make sure it will work. You can add a new tag there to make sure your\nvariants are working.\n\nprefix - a string that is going to be used to set cookies + check for tests.\nrandomNumber - the randon number variable used in Google Tag Manager. Make\nsure you enabled Random Number as a variable in GTM.\nvariants - the code for the variants, you can add an unlimited amount of variants.\n\nWe'll check if a user is in a certain variant to make sure that the user doesn't\nget any mixed up experiences. Next to that we also make sure the user is not in\nany other test currently by checking for any cookies that start with: tnw.\nQuick start:\nInclude the whole script and at the top of the script change the testing variables:\n\/\/ Variables\nvar prefix = 'tnw';\nvar randomNumber = {{Random Number}};\nvar changes = {\n    1: {\n        variants: {\n            1: {\n                execute: function () {\n\n                }\n            }\n        }\n    },\n    2: {\n        variants: {\n            1: {\n                execute: function () {\n\n                }\n            },\n            2: {\n                execute: function () {\n\n                }\n            }\n        }\n    }\n};\n\nNote: You'll never have to add 0, which is the original, it will take care of\nthe original variant itself.\nYou can read more about how to add a test in Google Tag Manager here.\nGoogle Analytics\nTo measure the variants + experiments in Google Analytics we send the data to\nGoogle Analytics via the dataLayer. In both scripts we send the data via the\nabTest event.\nYou'll have to create a custom report in Google Analytics to show you the\nspecific data for your variants.\nData in Google Analytics for different variants will show up like:\n\nprefix-{testID}-{testVariant} for A\/B testing.\nprefix-{testID}-{changeID}-{testVariant} for multivariate testing (MVT).\n\nCookies\nWe set the cookies for the length of the test that is added in the variables at\nthe beginning of a test. The names and values of the cookies look like:\n\nName: prefix-{testID}, with the value: {testVariant} for A\/B testing.\nName: prefix-{testID}, with the value: {changeID}-{testVariant} for\nmultivariate testing (MVT).\n\nNote: We prepend the cookie name: tnw but obviously you can change this to\nwhatever you'd like by changing the prefix variable: prefix.\nHistory\nApril 27, 2016 (2016-04-27)\n\nAllow for running A\/B\/n tests.\nChange the use of the pre-fix.\nAdd support for the dataLayer.\nClean & tidy up a lot of code.\n\nNovember 21, 2015 (2015-11-21)\n\nAdd a preview URL.\n\nSeptember 27, 2015 (2015-09-27)\n\nAdded a check to see if Google Analytics is initialized.\n\nAugust 18, 2015 (2015-08-18)\n\nInitial commit to add more information to the README.\n\nWant to contribute?\nContributions are welcome! There are just a few requested guidelines:\n\nPlease create a feature branch for your changes and squash commits.\nDon't worry about updating the version, changelog, or minified version.\nPlease respect the original syntax\/formatting stuff.\nIf proposing a new feature, it may be a good idea to create an issue first to discuss.\n\nMaintainer history\n\nMartijn Scheijbeler (Current)\nSimon Vreeman (Current)\n\n","46":"node-randomstring\nInstallation\nTo install randomstring, use npm:\n$ npm install randomstring\n\nUsage\nvar randomstring = require(\"randomstring\");\n\nrandomstring.generate();\n\/\/ >> \"XwPp9xazJ0ku5CZnlmgAx2Dld8SHkAeT\"\n\nrandomstring.generate(7);\n\/\/ >> \"xqm5wXX\"\nLICENSE\nnode-randomstring is licensed under the MIT license.\n","47":"tx_salaries\nThis Django application was generated using the Texas Tribune Generic\nDjango app template.\n\nInstallation\nYou can install this using pip like this:\npip install tx_salaries\n\n\nUsage\ntx_salaries is meant to be used in conjunction with data received from\nvarious departments around the state.  You must request this data yourself if\nyou want to use tx_salaries.\n\nImporting Data\nData is imported using the import_salary_data management command. You can run it in the salaries.texastribune.org repo once tx_salaries is properly installed like so:\npython salaries\/manage.py import_salary_data \/path\/to\/some-salary-spreadsheet.xlsx\n\nData is imported using csvkit, so it can import from any spreadsheet format\nthat csvkit's in2csv understands.\n\nSetup\n\nPull master for both salaries.texastribune.org and tx_salaries\n\nSet up pipenv if it's not already setup, using instructions in the salaries.texastribune.org repo\n\nIf you're using the local postgres database (which I recommend!), then you need to set that up. First set the DATABASE_URL:\nexport DATABASE_URL=postgres:\/\/localhost\/salaries\n\n\nThen pull down the backup:\nmake local\/db-fetch\n\n\nAnd load it:\nmake local\/db-restore\n\n\n\nNow, you should be good to work on tx_salaries like normal! If you have any transformers already in progress, you'll need to merge master of tx_salaries into it. Don't forget!\n\nStart the salaries.texastribune.org server\nIn the terminal, go to the salaries.texastribune.org repo. While the transformers live in tx_salaries, all of the data management happens in the salaries.texastribune repo, and that's where you'll run these commands:\npipenv shell\nexport DATABASE_URL=postgres:\/\/localhost\/salaries\npython salaries\/manage.py runserver\n\nCheck localhost:8000, should be up and running.\n\nWriting a New Transformer\nThis section walks you through creating a new importer.  We're going to use\nthe fictional \"Rio Grande County\" (fictional in Texas at least).\nTransformers require two things:\n\nAn entry in the TRANSFORMERS map in tx_salaries\/utils\/transformers\/__init__.py\nAn actual transformer capable of processing that file\n\nEntries in the TRANSFORMERS dictionary are made up of a unique hash that\nserves as the key to a given spreadsheet and a callable function that can\ntransform it.\nTo generate a key, run the following command in the salaries.texastribune.org virtualenv:\npython salaries\/manage.py generate_transformer_hash path\/to\/rio_grande_county.xls --sheet=data_sheet --row=number_of_header_row\n\nThe output should be a 40-character string.  Copy that value and open the\ntx_salaries\/utils\/transformers\/__init__.py file which contains all of the\nknown transformers.  Find the spot where rio_grande_county would fit in the\nalphabetical dictionary in TRANSFORMERS and add this line:\n'{ generated hash }': [rio_grande_county.transform, ],\nIf the generated hash already exists with another transformer, provide a tuple with a text\nlabel for the transformer and the transformer module like this:\n'{ generated hash }': [('Rio Grande County', rio_grande_county.transform),\n                        ('Other Existing County', other_county.transform), ],\nNote that the second value isn't a string -- instead it's a module.  Now you need to\nimport that module.  Go up to the top of the __init__.py file and add an\nimport:\nfrom . import rio_grande_county\nSave that file.  Next up, you need to create the new module that you just\nreferenced.  Inside the tx_salaries\/utils\/transformers\/ directory, create a\nnew file call rio_grande_county.py  At the first pass, it should look like\nthis:\nfrom . import base\nfrom . import mixins\n\nimport string\n\nfrom datetime import date\n\n# add if necessary: --sheet=\"Request data\" --row=3\n\nclass TransformedRecord(\n    mixins.GenericCompensationMixin,\n    mixins.GenericIdentifierMixin,\n    mixins.GenericPersonMixin,\n    mixins.MembershipMixin, mixins.OrganizationMixin, mixins.PostMixin,\n    mixins.RaceMixin, mixins.LinkMixin, base.BaseTransformedRecord):\n\n    MAP = {\n        'last_name': 'LABEL FOR LAST NAME',\n        'first_name': 'LABEL FOR FIRST NAME',\n        'department': 'LABEL FOR DEPARTMENT',\n        'job_title': 'LABEL FOR JOB TITLE',\n        'hire_date': 'LABEL FOR HIRE DATE',\n        'compensation': 'LABEL FOR COMPENSATION',\n        'gender': 'LABEL FOR GENDER',\n        'race': 'LABEL FOR RACE',\n        'compensation_type': 'LABEL FOR FT\/PT STATUS'\n    }\n\n    # The order of the name fields to build a full name.\n    # If `full_name` is in MAP, you don't need this at all.\n    NAME_FIELDS = ('first_name', 'last_name', )\n\n    # The name of the organization this WILL SHOW UP ON THE SITE, so double check it!\n    ORGANIZATION_NAME = 'Rio Grande County'\n\n    # What type of organization is this? This MUST match what we use on the site, double check against salaries.texastribune.org\n    ORGANIZATION_CLASSIFICATION = 'County'\n\n    # Y\/M\/D agency provided the data\n    DATE_PROVIDED = date(2013, 10, 31)\n\n    # How do they track gender? We need to map what they use to `F` and `M`.\n    gender_map = {'Female': 'F', 'Male': 'M'}\n\n    # The URL to find the raw data in our S3 bucket.\n    URL = ( 'http:\/\/raw.texastribune.org.s3.amazonaws.com\/'\n        'path\/to\/'\n        'rio_grande_county.xls' )\n\n    @property\n    def is_valid(self):\n        # Adjust to return False on invalid fields.  For example:\n        return self.last_name.strip() != ''\n\n    @property\n    def person(self):\n        name = self.get_name()\n\n        print self.gender_map[self.gender.strip()]\n\n        r = {\n            'family_name': name.last,\n            'given_name': name.first,\n            'additional_name': name.middle,\n            'name': unicode(name),\n            'gender': self.gender_map[self.gender.strip()]\n        }\n\n        return r\n\n    @property\n    def compensation_type(self):\n        comptype = self.get_mapped_value('compensation_type')\n\n        if comptype.upper() == 'FULL TIME':\n            return 'FT'\n        else:\n            return 'PT'\n\n    @property\n    def description(self):\n        comptype = self.get_mapped_value('compensation_type')\n\n        if comptype == 'FT':\n            return 'Annual gross salary'\n        elif comptype == 'PT':\n            return 'Part-time, annual gross salary'\n\ntransform = base.transform_factory(TransformedRecord)\nEach of the LABEL FOR XXX fields should be adjusted to match the\nappropriate column in the given spreadsheet. If the file requires special\nsheet or row handling, note the --sheet and --row flags as a comment\nat the top of the file.\nTransformedRecord now represents a generic record.  You may need to\ncustomize the various properties added by the mixins or replace them with\ncustom properties in other cases.  See the mixins for further documentation on\nwhat they add.\nThe last line generates a transform function that uses the TransformedRecord\nthat you just created.  Now you're ready to run the importer.\nBack on the command line, run this in the salaries.texastribune.org repo:\npython salaries\/manage.py import_salary_data \/path\/to\/rio_grande_county.xls\n\nPay attention to any error messages you receive. Most transformer errors are due\nto missing data -- either the user didn't map to all the necessary fields,\ndidn't include a mixin to process a field or made an error in an overridden\nproperty that is supposed to return an attribute.\nNote the generate_transformer_hash and import_salary data\nmanagement commands can take --sheet and --row flags if the agency gave\nyou a spreadsheet with multiple sheets or a header row that isn't the first row.\nCongratulations!  You just completed your first salary transformer.\n\nUnderstanding Transformers\nTransformers are callable functions that take two arguments and return an array\nof data to be processed.  At its simplest, it would look like this:\ndef transform(labels, source):\n    data = []\n    for raw_record in source:\n        record = dict(zip(labels, raw_record))\n        # ... create the structure required ...\n        data.append(structured_record)\n    return data\nThe data contained in the fictitious structured_record variable is a\ndictionary that must look something like this:\nstructured_record = {\n    'original': ...,  # dictionary of key\/value pairs for the data\n    'tx_people.Identifier': ...,  # dictionary of attributes for the Identifier\n    'tx_people.Organization': ...,  # dictionary of attributes for the Organization\n    'tx_people.Post': ...,  # dictionary of attributes for the Post\n    'tx_people.Membership': ...,  # dictionary of attributes for the Membership\n    'compensations': [\n        # first dictionary of compensation and type\n        # should contain at least one, can contain as many as necessary\n    ]\n\n}}\nThat record is structured such that its keys and values match the models and kwargs\nfor storing tx_people and tx_salaries models. How do spreadsheets get structured?\nThe import_salary_data management command runs through several modules to store\nspreadsheet data. First it uses transformer.`transform`_, which uses the header\nrow to identify the transformer necessary to import the spreadsheet.\nThat transformer turns each row of the spreadsheet into a structured record with\nthe help of mixins.py and base.py. base.py defines the template of the\nrecord, and mixins.py provides functions to format the required data. Mixins\nare included in the definition of TransformedRecord. However, mixins cannot\nhandle all situations, and sometimes fields like CompensationType require\nspecial logic. You can override mixins by writing a custom @property in the\ntransformer. Errors often happen at this stage when a transformer and its mixins\nfail to provide all the fields required by base.\nAfter each of the rows of the spreadsheet are converted to structured records,\na list of records is sent to to_db.save(), which unpacks and stores the data.\nimport_salary_data also keeps track of the unique organizations and positions\nthat are imported so it can denormalize the stats when the import finishes.\nThat's a high-level view of transformers. Read the comments in mixins.py and\ncheck out the data template in base.py for more details on the specific attributes\ntransformers require.\n","48":"The Texas Higher Education Data Project\n\nA very rough guide to starting development\nExample .env file for environment variables:\nDJANGO_SETTINGS_MODULE=exampleproject.settings.dev\nDATABASE_URL=postgis:\/\/\/tx_highered\n\nComplete guide to getting started (remove steps to suit you):\n# install postgresql libpq-dev\n\ngit clone $REPOSITORY && cd $PATH\nmkvirtualenv tx_higher_ed\nsetvirtualenvproject\nadd2virtualenv .\npip install -r requirements.txt\n\n# if you need to create a database:\n# `postdoc` greatly simplifies connecting to Docker databases\npip install postdoc\nphd createdb --encoding=UTF8 -T template0\necho \"CREATE EXTENSION postgis;\" | phd psql\necho \"CREATE EXTENSION postgis_topology;\" | phd psql\n\n# or if you need to reset your database:\nmake resetdb\n\n# syncdb and load fixtures\nmake syncdb\n\n#######################################################################\n# You can stop at this point if you're just playing with the project. #\n#######################################################################\n\n# if using 2012 data, bump it up to 2014 standards\npython tx_highered\/scripts\/2014_update.py\n\n# get ipeds data, requires https:\/\/github.com\/texastribune\/ipeds_reporter\n..\/ipeds_reporter\/csv_downloader\/csv_downloader.py \\\n  --uid data\/ipeds\/ipeds_institutions.uid --mvl data\/ipeds\nmv ~\/Downloads\/Data_*.csv data\/ipeds\n# get thecb data\ncd data && make all\n# load data\n#   timing: 10m25.069s\nmake load\n# post-process the data\npython exampleproject\/manage.py tx_highered_process\n\n\n####################################\n# placeholder for post-2014 update #\n####################################\n# the 2012->2014 specific stuff can go out and the above importing\n# instructions can get updated\nDatabase\nThis project currently requires a PostGIS database (hopefully not for long):\n$ phd createdb\n$ phd psql\n\nCREATE EXTENSION postgis;\nCREATE EXTENSION postgis_topology;\nMoving data between databases\nYou can do a sql dump to move data from one postgres database to another\n(excluding geo info):\n$ phd SOURCE_DATABASE_URL pg_dump --no-owner --no-acl --table=tx_highered* --clean > tx_highered.sql\n$ phd DEST_DATABASE_URL psql -f tx_highered.sql\nAfter deploy\n\nFreeze the current data in a fixture\n\nEdit the tx_highered_YYYY.json.gz make task\nRun the task to save the data\n\n\nAdjust the loading scripts to reference the new fixture\nDeprecate (or delete) any one-time data migration scripts, e.g.\n2014_update.py won't be necessary after 2015\n\nGetting Data from the IPEDS Data Center\nWhen it asks you for an Institution, enter a list of UnitIDs generated by:\nlist(Institution.objects.filter(ipeds_id__isnull=False).values_list('ipeds_id', flat=True))\n\nGetting Data from the Texas Higher Education Coordinating Board\nIf you want to regrab data from THECB's web site, first find the data file that you want to re-grab.\nIt will be named something like \"top_10_percent.html\". There will also be a file called \"top_10_percent.POST\". From that file you can recreate the report with the command:\ncurl -X POST -d @top_10_percent.POST http:\/\/www.txhighereddata.org\/interactive\/accountability\/InteractiveGenerate.cfm -s -v > blahblahblah.html\n\nIf you need to modify the report, you can reverse engineer it from the POST data and the form markup.\n(c) 2012 The Texas Tribune\n","49":"Texas Legislative Districts\nA reusable Django app for working with Texas legislative districts.\nUsage\nAdd tx_lege_districts to your INSTALLED_APPS. Then configure a GIS-enabled\ndatabase and load the districts you want from a fixture:\npython manage.py loaddata districts_2006\n\nHookup the urls in the tx_lege_districts namespace:\nurlpatterns = patterns('',\n    url('^lege\\-districts\/', include('tx_lege_districts.urls',\n            namespace='tx_lege_districts')),\n)\n\nRepresentatives\nDistricts provide a representative property that is None by default.\nThe representative can be handled in your own project using a a configurable backend:\n# myapp\/backends.py\nclass MyBackend(object):\n    def get_representative(self, district):\n        return MyRepresentativeModel.objects.get_for_district(district)\n\n# settings.py\nTX_REPRESENTATIVE_BACKENDS = ['myapp.backends.MyBackend']\n\nTesting\nSet the DATABASE_URL environment variable. Example:\nexport DATABASE_URL=postgis:\/\/\/tx_lege_districts\n\nChange template1 to be postgis enabled:\npsql template1\n\nCREATE EXTENSION postgis;\nCREATE EXTENSION postgis_topology;\n\nJust use DROP EXTENSION if you want to go back.\nRun the test runner:\npython runtests.py\n\n","50":"\ntt_social_auth\nThis package is a custom python-social-auth backend for the Texas Tribune.\nDrop it into your app of choice to get TT OAuth2 working (relatively) quickly.\n\nInstallation: Django\nThe majority of this is the same as installing python-social-auth directly.\nIf you run into problems or confusions, consult their docs.\n\nRegister your app in the main Texas Tribune project. Talk to Tech if you need help with this. This will give you a client ID and secret key.\n\nAdd tt_social_auth to requirements.txt\n\nAdd social.apps.django_app.default to INSTALLED_APPS\n\nAdd tt_social_auth.backends.texastribune.TribOAuth2 to AUTHENTICATION_BACKENDS (NOTE: you want to keep Django's default auth backend too; see Django docs for details)\n\nSet your client and secret from Step 1 in settings.py:\nSOCIAL_AUTH_TEXASTRIBUNE_KEY = 'Your key here'\nSOCIAL_AUTH_TEXASTRIBUNE_SECRET = 'Your secret here'\n\n\nAdd the following to your TEMPLATE_CONTEXT_PROCESSORS (in Django 1.8 and higher, this goes in TEMPLATES['OPTIONS']['context_processors']):\n'social.apps.django_app.context_processors.backends',\n'social.apps.django_app.context_processors.login_redirect',\n\n\nAdd the following to your urls.py:\nurl('', include('social.apps.django_app.urls', namespace='social')),\n\n\nOptional: if you don't want admins in Texas Tribune to be admins in this app, set AUTHENTICATE_TEXASTRIBUNE_STAFF=False in your Django settings or env var.\n\nOptional: if you are testing with a non-production version of the Texas Tribune app, you can set TEXASTRIBUNE_BASE_URL in your settings.py. For instance, if you want to test against a local server, you could set it to http:\/\/local.texastribune.org:8000\/.\n\nOptional: you may want to set a LOGIN_REDIRECT_URL, which is where the login will redirect when complete.\n\n\nTest it out by navigating to \/login\/texastribune and confirm that it goes through the whole handshake and drops you off at the LOGIN_REDIRECT_URL.\n\nDevelopment & Tests\nTo install and run tests:\npip install -e .[test]\npy.test\n\n","51":"armstrong.core.tt_sections\nProvides the basic concept of sections within an Armstrong site.\nYou can use Section models to organize your content into a group.  Sections\ncan have a parent section to allow you to create a hierarchy.  For example, the\nTexas Tribune has an Immigration section which in turns has Sanctuary Cities\nand Dream Act as children sections.\nYou are not limited to a hierarchical structure---you can create a flat\nstructure as well.\n\nUsage\nYou need to add a section field to any model that you would like to show up\nin a given section.  For example:\n# your models.py\nfrom django.db import models\nfrom armstrong.core.tt_sections.models import Section\n\n\nclass MyArticle(models.Model):\n    title = models.CharField(max_length=100)\n    body = models.TextField()\n\n    section = models.ForeignKey(Section)\n\nYou can also relate to multiple sections as well through a ManyToManyField:\nclass MyArticle(models.Model):\n    # other fields\n    sections = models.ManyToManyField(Section)\n\n\nDisplaying Sections\nYou can display a section through the SimpleSectionView class-based-view\n(CBV).  The standard project template in Armstrong provides an example of how\nto configure this view.\nurl(r'^section\/(?P<full_slug>[-\\w\/]+)',\n        SimpleSectionView.as_view(template_name='section.html'),\n        name='section_view'),\n\nYou can use the {% section menu %} template tag to display list of all\nsections inside your template.  You must load the section_helpers template\ntags to use this.  You must provide it with a section_view kwarg that is\nassociated with the section view you configure inside your URL routes.  For\nexample, to display a list of sections that link to the section view created\nabove, you would put this in your template.\n{% load section_helpers %}\n{% section_menu section_view='section_view' %}\n\nWith the following sections in your database:\nPolitics\nSports\n    Football\n    Basketball\nFashion\n\nUsing all of the example we have so far, the output from your template would\nlook like this:\n<ul class=\"root\">\n    <li>\n        <a href='\/section\/politics\/'>Politics<\/a>\n    <\/li>\n    <li>\n        <a href='\/section\/sports\/'>Sports<\/a>\n        <ul class=\"children\">\n            <li>\n                <a href='\/section\/sports\/football\/'>Football<\/a>\n            <\/li>\n            <li>\n                <a href='\/section\/sports\/basketball\/'>Basketball<\/a>\n            <\/li>\n        <\/ul>\n    <\/li>\n    <li>\n        <a href='\/section\/fashion\/'>Fashion<\/a>\n    <\/li>\n<\/ul>\n\n\nInstallation & Configuration\nWe recommend installing this through the Cheese Shop.\npip install armstrong.core.tt_sections\n\nThis gets you the latest released version of armstrong.core.tt_sections.\n\nConfiguration\nThere are two setting that you can use to change the behavior of this\ncomponent.\n\nARMSTRONG_SECTION_ITEM_BACKEND\nThis is used to configure which backend is used to find the items\nassociated with a given Section.  (default:\narmstrong.core.tt_sections.backend.ItemFilter)\nARMSTRONG_SECTION_ITEM_MODEL\nThis is used by the default find_related_models backend to determine\nwhich model has a section associated with it. (default:\narmstrong.apps.content.models.Content)\n\n\nContributing\n\nCreate something awesome -- make the code better, add some functionality,\nwhatever (this is the hardest part).\nFork it\nCreate a topic branch to house your changes\nGet all of your commits in the new topic branch\nSubmit a pull request\n\n\nState of Project\nArmstrong is an open-source news platform that is freely available to any\norganization.  It is the result of a collaboration between the Texas Tribune\nand Bay Citizen, and a grant from the John S. and James L. Knight\nFoundation.\nTo follow development, be sure to join the Google Group.\narmstrong.core.arm_section is part of the Armstrong project.  You're\nprobably looking for that.\n\nLicense\nCopyright 2011 Bay Citizen and Texas Tribune\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\nhttp:\/\/www.apache.org\/licenses\/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n","52":"AEIS\nTools for analyzing data from the Academic Excellence Indicator System and the Texas Academic Performance Report.\nFor the 2012-2013 school year and later, see the Texas Academic Performance Reports.\n\nThe TAPRs were previously known as the Academic Excellence Indicator System (AEIS) Reports. Those reports were published from 1990-91 to 2011-12. They may be found at the AEIS Archive.\n\nHow to Use\nTo scrape and download all data from AEIS:\n$ python -m aeis.scrape data\n\nTo analyze the columns of the downloaded data:\n$ python analyze.py data --reload\n$ ls analysis.shelf\n$ ls metadata.shelf\n\nYou should now be able to decompose any analyzed field into its metadata:\n$ python analyze.py --decompose DH00A00T013R\n\nTo index all data in ElasticSearch:\n$ export ES_HOST=localhost:9200\n$ python index.py data --recreate\n\nNext Steps\nWe still need to scrape and analyze the 2013 academic indicators,\nbut those have moved to a new system: the Texas Academic Performance Report.\nThe 2012-2013 data can be downloaded from this page:\nhttp:\/\/ritter.tea.state.tx.us\/perfreport\/tapr\/2013\/download\/DownloadData.html\n","53":"ox-scale\nFor guessing the weight of oxen\nSetting up the project\nInstall requirements:\npip install -r requirements.txt\nnpm install\n\nSetup your Python path:\nadd2virtualenv .\n\nSetup your environment:\nDJANGO_SETTINGS_MODULE=ox_scale.settings\nDEBUG=1\n\n","54":"Django-GistPage\nDisclaimer: This is pre-alpha code. Not all the claims made in in this\ndocument are implemented.\nThis package provides two components:\n\nDjangoGistServer: A way to develop a simple static site that uses the\nfull Django templating engine, including extending some Django project's base\ntemplates.\nGistPage: A way to take a page developed using DjangoGistServer that's\nbeen published as a GitHub gist and serve it at a url in some Django project.\n\nYour static page should be in a directory that looks like this:\nindex.html\napp.css\napp.js\n\nIf you use a template base that supports DjangoGistServer, you don't have to\nworry about including css and js. If your base doesn't support DjangoGistServer,\nadd these two lines somewhere inside index.html:\n<link href=\"app.css\" rel=\"stylesheet\" type=\"text\/css\">\n<script type=\"text\/javascript\" src=\"app.js\"><\/script>\n\nMagic Warning: If you have multiple css and js files, they will be\nconcatenated into app.css and app.js. You may have guessed at this point that it\ndoesn't matter what your files are named. TODO add docs on how to deal with\nincluding third-party JS\/CSS.\nStatic pages are published as gists. Which are great because you get history and\ncan package multiple files together. A sample gist is available at:\n\nhttps:\/\/gist.github.com\/crccheck\/93a8a020f6789d373a6c\n\nDjangoGistServer\nSample Usage:\nUsing virtualenvwrapper and vi as the editor:\n# do development inside a virtualenv because this uses a lot of libraries\nmkvirtualenv gistme\npip install django-gistpage\nmkdir my_awesome_page\ncd my_awesome_age\nvi index.html\nvi app.js\nvi app.css\npython -m DjangoGistServer 8000 --template-dir=\/a\/b\/c\/templates\/\n# your directory is now being served on port 8000\n\n\nTurning your static page into a gist:\nThere are several command line tools you can install do do this:\n\nTODO\n\nYou can also create a gist from GitHub, clone it locally, and push it back up.\n\nGistPage\nTo serve that static site in your production Django site, you would create a new\nGistPage object in the admin, just like a flatpage. Point it to the gist\nversion of the static page, and that's it.\n","55":"Sidekiq::Throttler\n\n\n\nSidekiq::Throttler is a middleware for Sidekiq that adds the ability to rate\nlimit job execution on a per-worker basis.\nCompatibility\nSidekiq::Throttler supports Sidekiq versions 2 and 3 and is actively tested against Ruby versions 2.0.0, 2.1, and 2.2.\nInstallation\nAdd this line to your application's Gemfile:\ngem 'sidekiq-throttler'\nAnd then execute:\n$ bundle\n\nOr install it yourself as:\n$ gem install sidekiq-throttler\n\nConfiguration\nIn a Rails initializer or wherever you've configured Sidekiq, add\nSidekiq::Throttler to your server middleware:\nSidekiq.configure_server do |config|\n  config.server_middleware do |chain|\n    chain.add Sidekiq::Throttler\n  end\nend\nSidekiq::Throttler defaults to in-memory storage of job execution times. If\nyou have multiple worker processes, or frequently restart your processes, this\nwill be unreliable. Instead, specify the :redis storage option:\nSidekiq.configure_server do |config|\n  config.server_middleware do |chain|\n    chain.add Sidekiq::Throttler, storage: :redis\n  end\nend\nBasic Usage\nIn a worker, specify a threshold (maximum jobs) and period for throttling:\nclass MyWorker\n  include Sidekiq::Worker\n\n  sidekiq_options throttle: { threshold: 50, period: 1.hour }\n\n  def perform(user_id)\n    # Do some heavy API interactions.\n  end\nend\nIn the above example, when the number of executed jobs for the worker exceeds\n50 in an hour, remaining jobs will be delayed.\nAdvanced Usage\nCustom Keys\nBy default, each worker has its own key for throttling. For example:\nclass FooWorker\n  include Sidekiq::Worker\n\n  sidekiq_options throttle: { threshold: 50, period: 1.hour }\n\n  # ...\nend\n\nclass BarWorker\n  include Sidekiq::Worker\n\n  sidekiq_options throttle: { threshold: 50, period: 1.hour }\n\n  # ...\nend\nEven though FooWorker and BarWorker use the same throttle options, they are\ntreated as different groups. To have multiple workers with shared throttling,\nthe :key options can be used:\nsidekiq_options throttle: { threshold: 50, period: 1.hour, key: 'foobar' }\nAny jobs using the same key, regardless of the worker will be tracked under the\nsame conditions.\nDynamic Throttling\nEach option (:threshold, :period, and :key) accepts a static value but can\nalso accept a Proc that's called each time a job is processed.\nDynamic Keys\nIf throttling is per-user, for example, you can specify a Proc for key which\naccepts the arguments passed to your worker's perform method:\nsidekiq_options throttle: { threshold: 20, period: 1.day, key: ->(user_id){ user_id } }\nIn the above example, jobs are throttled for each user when they exceed 20 in a\nday.\nDynamic Thresholds\nThresholds can be configured based on the arguments passed to your worker's perform method,\nsimilar to how the key option works:\nsidekiq_options throttle: { threshold: ->(user_id, rate_limit) { rate_limit }, period: 1.hour, key: ->(user_id, rate_limit){ user_id } }\nIn the above example, jobs are throttled for each user when they exceed the rate limit provided in the message. This is useful in cases where each user may have a different rate limit (ex: interacting with external APIs)\nDynamic Periods\nIn this contrived example, our worker is limited to 9 thousand jobs every 10\nminutes. However, on Tuesdays limit jobs to 9 thousand every 15 minutes:\nsidekiq_options throttle: { threshold: 9000, period: ->{ Date.today.tuesday? ? 15.minutes : 10.minutes } }\nContributing\n\nFork it\nCreate your feature branch (git checkout -b my-new-feature)\nCommit your changes (git commit -am 'Add some feature')\nPush to the branch (git push origin my-new-feature)\nCreate new Pull Request\n\nLicense\nMIT Licensed. See LICENSE.txt for details.\n","56":"django-nested-admin\n \ndjango-nested-admin is a project that makes it possible to nest\nadmin inlines (that is, to define inlines on InlineModelAdmin classes).\nIt is compatible with Django 1.11+ and Python versions 2.7 and 3.4+ and works\nwith or without Grappelli. When Grappelli is not installed it allows\nGrappelli-like drag-and-drop functionality.\n\nInstallation\nThe recommended way to install django-nested-admin is from\nPyPI:\npip install django-nested-admin\n\nAlternatively, one can install a development copy of django-nested-admin\nfrom source:\npip install -e git+git:\/\/github.com\/theatlantic\/django-nested-admin.git#egg=django-nested-admin\n\nIf the source is already checked out, use setuptools to install:\npython setup.py develop\n\n\nConfiguration\nTo use django-nested-admin in your project, \"nested_admin\" must be added\nto the INSTALLED_APPS in your settings:\nINSTALLED_APPS = (\n    # ...\n    'nested_admin',\n)\nIf you\u2019re using django-grappelli,\nyou will also need to add to include nested_admin.urls in your urlpatterns:\n# Django 2+\nurlpatterns = [\n    # ...\n    path('_nested_admin\/', include('nested_admin.urls')),\n]\n\n# Django < 2\nurlpatterns = [\n    # ...\n    url(r'^_nested_admin\/', include('nested_admin.urls')),\n]\n\nExample Usage\nIn order to use django-nested-admin, use the following classes in\nplace of their django admin equivalents:\n\n\ndjango.contrib.admin\nnested_admin\n\nModelAdmin\nNestedModelAdmin\n\nInlineModelAdmin\nNestedInlineModelAdmin\n\nStackedInline\nNestedStackedInline\n\nTabularInline\nNestedTabularInline\n\n\n\nThere is also nested_admin.NestedGenericStackedInline and\nnested_admin.NestedGenericTabularInline which are the nesting-capable\nversions of GenericStackedInline and GenericTabularInline in\ndjango.contrib.contenttypes.admin.\n# An example admin.py for a Table of Contents app\n\nfrom django.contrib import admin\nimport nested_admin\n\nfrom .models import TableOfContents, TocArticle, TocSection\n\nclass TocArticleInline(nested_admin.NestedStackedInline):\n    model = TocArticle\n    sortable_field_name = \"position\"\n\nclass TocSectionInline(nested_admin.NestedStackedInline):\n    model = TocSection\n    sortable_field_name = \"position\"\n    inlines = [TocArticleInline]\n\nclass TableOfContentsAdmin(nested_admin.NestedModelAdmin):\n    inlines = [TocSectionInline]\n\nadmin.site.register(TableOfContents, TableOfContentsAdmin)\n\nTesting\ndjango-nested-admin has fairly extensive test coverage.\nThe best way to run the tests is with tox,\nwhich runs the tests against all supported Django installs. To run the tests\nwithin a virtualenv run pytest from the repository directory. The tests\nrequire a selenium webdriver to be installed. By default the tests run with\nphantomjs, but it is also possible to run the tests with the chrome webdriver\nby passing --selenosis-driver=chrome to pytest or, if running with\ntox, running tox -- --selenosis-driver=chrome. See pytest --help for\na complete list of the options available.\n\nContributing\nThis project uses webpack for building its\njavascript and css. To install the dependencies for the build process, run\nnpm install from the root of the repository. You can then run\nnpm run build to rebuild the static files.\n\nLicense\nThe django code is licensed under the Simplified BSD\nLicense. View the\nLICENSE file under the root directory for complete license and\ncopyright information.\n","57":"django-xml\n\ndjango-xml is a python module which provides an abstraction to\nlxml's XPath and XSLT functionality in a manner resembling\ndjango database models.\nContents\n\nInstallation\nExample\nAdvanced Example\nXmlModel Meta options\n\nnamespaces\nparser_opts\nextension_ns_uri\n\n\n@lxml_extension reference\n\nns_uri\nname\n\n\nXPathField options\n\nxpath_query\nrequired\nextra_namespaces\nextensions\n\n\nXPathSingleNodeField options\n\nignore_extra_nodes\n\n\nXsltField options\n\nxslt_file, xslt_string\nparser\nextensions\n\n\nXmlModel field reference\n\nInstallation\nTo install the latest stable release of django-xml, use pip or easy_install\npip install django-xml\neasy_install django-xml\nFor the latest development version, install from source with pip:\npip install -e git+git:\/\/github.com\/theatlantic\/django-xml#egg=django-xml\nIf the source is already checked out, install via setuptools:\npython setup.py develop\nExample\nimport math\nfrom djxml import xmlmodels\n\nclass NumbersExample(xmlmodels.XmlModel):\n\n    class Meta:\n        extension_ns_uri = \"urn:local:number-functions\"\n        namespaces = {\"fn\": extension_ns_uri,}\n\n    all_numbers  = xmlmodels.XPathIntegerListField(\"\/\/num\")\n    even_numbers = xmlmodels.XPathIntegerListField(\"\/\/num[fn:is_even(.)]\")\n    sqrt_numbers = xmlmodels.XPathFloatListField(\"fn:sqrt(\/\/num)\")\n\n    @xmlmodels.lxml_extension\n    def is_even(self, context, number_nodes):\n        numbers = [getattr(n, 'text', n) for n in number_nodes]\n        return all([bool(int(num) % 2 == 0) for num in numbers])\n\n    @xmlmodels.lxml_extension\n    def sqrt(self, context, number_nodes):\n        sqrts = []\n        for number_node in number_nodes:\n            number = getattr(number_node, 'text', number_node)\n            sqrts.append(repr(math.sqrt(int(number))))\n        return sqrts\n\n\ndef main():\n    numbers_xml = u\"\"\"\n    <numbers>\n        <num>1<\/num>\n        <num>2<\/num>\n        <num>3<\/num>\n        <num>4<\/num>\n        <num>5<\/num>\n        <num>6<\/num>\n        <num>7<\/num>\n    <\/numbers>\"\"\"\n\n    example = NumbersExample.create_from_string(numbers_xml)\n\n    print \"all_numbers  = %r\" % example.all_numbers\n    print \"even_numbers = %r\" % example.even_numbers\n    print \"sqrt_numbers = [%s]\" % ', '.join(['%.3f' % n for n in example.sqrt_numbers])\n    # all_numbers  = [1, 2, 3, 4, 5, 6, 7]\n    # even_numbers = [2, 4, 6]\n    # sqrt_numbers = [1.000, 1.414, 1.732, 2.000, 2.236, 2.449, 2.646]\n\nif __name__ == '__main__':\n    main()\nAdvanced Example\nAn example of django-xml usage which includes XsltField and @lxml_extension methods\ncan be found here.\nXmlModel Meta options\nMetadata for an XmlModel is passed as attributes of an\ninternal class named Meta. Listed below are the options\nthat can be set on the Meta class.\nnamespacesOptions.namespaces = {}\nA dict of prefix \/ namespace URIs key-value pairs that is passed to\nlxml.etree.XPathEvaluator()\nfor all XPath fields on the model.\nparser_optsOptions.parser_opts = {}\nA dict of keyword arguments to pass to\nlxml.etree.XMLParser()\nextension_ns_uriOptions.extension_ns_uri\nThe default namespace URI to use for extension functions created using the\n@lxml_extension decorator.\n@lxml_extension reference\ndef lxml_extension(method=None, ns_uri=None, name=None)\nThe @lxml_extension decorator is for registering model methods as\nlxml extensions which can be used in XPathFields and XsltFields. All keyword\narguments to it are optional.\nns_uri\nThe namespace uri for the function. If used in an XPathField, this uri will need to\nbe one of the values in the namespaces attribute of the XmlModel's internal\nMeta class. If used in an XSLT, the namespace will need to be defined in\nthe xslt file or string.\nDefaults to the value of the extension_ns_uri attribute of the\nXmlModel's internal Meta class, if defined. If neither the\nextension_ns_uri attribute of XmlModel.Meta is set, nor is the\nns_uri keyword argument passed, an ExtensionNamespaceException\nwill be thrown.\nname\nThe name of the function to register. Defaults to the method's name.\nXPathField options\nThe following arguments are available to all XPath field types. All but the\nfirst are optional.\nxpath_queryXPathField.xpath_query\nThe XPath query string to perform on the document. Required.\nrequiredXPathField.required = True\nIf True, a DoesNotExist exception will be thrown if no nodes match the\nXPath query for the field. Defaults to True.\nextra_namespacesXPathField.extra_namespaces\nA dict of extra prefix\/uri namespace pairs to pass to\nlxml.etree.XPathEvaluator().\nextensionsXPathField.extensions\nExtra extensions to pass on to\nlxml.etree.XSLT.\nSee the lxml documentation\nfor details on how to form the extensions keyword argument.\nXPathSingleNodeField options\nignore_extra_nodesXPathSingleNodeField.ignore_extra_nodes = False\nIf True return only the first node of the XPath evaluation result, even if it\nevaluates to more than one node. If False, accessing an xpath field which\nevaluates to more than one node will throw a MultipleObjectsExist exception\nDefaults to False.\nTo return the full list of nodes, Use an XPathListField\nXsltField options\nxslt_file, xslt_stringXsltField.xslt_fileXsltField.xslt_string\nThe first positional argument to XsltField is the path to an xslt file.\nAlternatively, the xslt can be passed as a string using the\nxslt_string keyword argument. It is required to specify one of these\nfields.\nparserXsltField.parser\nAn instance of lxml.etree.XMLParser\nto override the one created by the XmlModel class. To override parsing options\nfor the entire class, use the parser_opts\nattribute of the XmlModel internal Meta class.\nextensionsXsltField.extensions = {}\nExtra extensions to pass on to the constructor of\nlxml.etree.XSLT.\nSee the lxml documentation\nfor details on how to form the extensions keyword argument.\nXmlModel field reference\nclass XsltField(xslt_file=None, xslt_string=None, parser=None, extensions=None)\nField which abstracts the creation of\nlxml.etree.XSLT objects.\nThis field's return type is a callable which accepts keyword arguments that\nare passed as parameters to the stylesheet.\nclass XPathField(xpath_query, required=False, extra_namespaces=None, extensions=None)\nBase field for abstracting the retrieval of node results from the xpath\nevaluation of an xml etree.\nclass XPathField(xpath_query, required=False, extra_namespaces=None, extensions=None)\nBase field for abstracting the retrieval of node results from the xpath\nevaluation of an xml etree.\nclass XPathListField(xpath_query, required=False, extra_namespaces=None, extensions=None)\nField which abstracts retrieving a list of nodes from the xpath evaluation\nof an xml etree.\nclass XPathSingleItemField(xpath_query, required=False, extra_namespaces=None,\n                           extensions=None, ignore_extra_nodes=False)\nField which abstracts retrieving the first node result from the xpath\nevaluation of an xml etree.\nclass XPathTextField(XPathSingleNodeField)\nReturns a unicode value when accessed.\nclass XPathIntegerField(XPathSingleNodeField)\nReturns an int value when accessed.\nclass XPathFloatField(XPathSingleNodeField)\nReturns a float value when accessed.\nclass XPathDateTimeField(XPathSingleNodeField)\nReturns a datetime.datetime value when accessed.\nclass XPathTextListField(XPathListField)\nReturns a list of unicode values when accessed.\nclass XPathIntegerListField(XPathListField)\nReturns a list of int values when accessed.\nclass XPathFloatListField(XPathListField)\nReturns a list of float values when accessed.\nclass XPathDateTimeListField(XPathListField)\nReturns a list of datetime.datetime values when accessed.\n","58":"django-select2-forms\n\ndjango-select2-forms is a project that makes available Django form\nfields that use the Select2 javascript\nplugin. It was created by\ndevelopers at The Atlantic.\n\nSupport\nBeing that Django added select2 support in 2.0, we will support up to that version\nfor compatibility purposes.\n\n~=v2.0.2: Python ~=2.7,~=3.6 | Django >=1.8,<2.1\n~=v2.1: Python ~=2.7,>=3.6,<3.8 | Django >=1.11,<2.1\n~=v3.0: __Python >=3.6,<3.9 | Django >=2.0,<2.1 (future release)__\n\n\nInstallation\nThe recommended way to install is with pip:\npip install django-select2-forms\n\nor, to install with pip from source:\npip install -e git+git:\/\/github.com\/theatlantic\/django-select2-forms.git#egg=django-select2-forms\n\nIf the source is already checked out, use setuptools:\npython setup.py develop\n\n\nConfiguration\ndjango-select2-forms serves static assets using\ndjango.contrib.staticfiles,\nand so requires that \"select2\" be added to your settings'\nINSTALLED_APPS:\nINSTALLED_APPS = (\n    # ...\n    'select2',\n)\nTo use django-select2-forms' ajax support, 'select2.urls' must be\nincluded in your urls.py urlpatterns:\nurlpatterns = patterns('',\n    # ...\n    url(r'^select2\/', include('select2.urls')),\n)\n\nUsage\nThe simplest way to use django-select2-forms is to use\nselect2.fields.ForeignKey and select2.fields.ManyToManyField in\nplace of django.db.models.ForeignKey and\ndjango.db.models.ManyToManyField, respectively. These fields extend\ntheir django equivalents and take the same arguments, along with extra\noptional keyword arguments.\n\nselect2.fields.ForeignKey examples\nIn the following two examples, an \"entry\" is associated with only one\nauthor. The example below does not use ajax, but instead performs\nautocomplete filtering on the client-side using the <option>\nelements (the labels of which are drawn from Author.__str__())\nin an html <select>.\n@python_2_unicode_compatible\nclass Author(models.Model):\n    name = models.CharField(max_length=100)\n\n    def __str__(self):\n        return self.name\n\nclass Entry(models.Model):\n    author = select2.fields.ForeignKey(Author,\n        overlay=\"Choose an author...\",\n        on_delete=models.CASCADE)\nThis more advanced example autocompletes via ajax using the\nAuthor.name field and limits the autocomplete search to\nAuthor.objects.filter(active=True)\nclass Author(models.Model):\n    name = models.CharField(max_length=100)\n    active = models.BooleanField()\n\nclass Entry(models.Model):\n    author = select2.fields.ForeignKey(Author,\n        limit_choices_to=models.Q(active=True),\n        ajax=True,\n        search_field='name',\n        overlay=\"Choose an author...\",\n        js_options={\n            'quiet_millis': 200,\n        },\n        on_delete=models.CASCADE)\n\nselect2.fields.ManyToManyField examples\nIn the following basic example, entries can have more than one author.\nThis example does not do author name lookup via ajax, but populates\n<option> elements in a <select> with Author.__unicode__()\nfor labels.\n@python_2_unicode_compatible\nclass Author(models.Model):\n    name = models.CharField(max_length=100)\n\n    def __str__(self):\n        return self.name\n\nclass Entry(models.Model):\n    authors = select2.fields.ManyToManyField(Author)\nThe following \"kitchen sink\" example allows authors to be ordered, and\nuses ajax to autocomplete on two variants of an author's name.\nfrom django.db import models\nfrom django.db.models import Q\nimport select2.fields\nimport select2.models\n\nclass Author(models.Model):\n    name = models.CharField(max_length=100)\n    alt_name = models.CharField(max_length=100, blank=True, null=True)\n\nclass Entry(models.Model):\n    categories = select2.fields.ManyToManyField(Author,\n        through='EntryAuthors',\n        ajax=True,\n        search_field=lambda q: Q(name__icontains=q) | Q(alt_name__icontains=q),\n        sort_field='position',\n        js_options={'quiet_millis': 200})\n\nform field example\nIf you don't need to use the ajax features of django-select2-forms\nit is possible to use select2 on django forms without modifying your\nmodels. The select2 formfields exist in the select2.fields module\nand have the same class names as their standard django counterparts\n(ChoiceField, MultipleChoiceField, ModelChoiceField,\nModelMultipleChoiceField). Here is the first ForeignKey example\nabove, done with django formfields.\nclass AuthorManager(models.Manager):\n    def as_choices(self):\n        for author in self.all():\n            yield (author.pk, force_text(author))\n\n@python_2_unicode_compatible\nclass Author(models.Model):\n    name = models.CharField(max_length=100)\n    objects = AuthorManager()\n\n    def __str__(self):\n        return self.name\n\nclass Entry(models.Model):\n    author = models.ForeignKey(Author, on_delete=models.CASCADE)\n\nclass EntryForm(forms.ModelForm):\n    author = select2.fields.ChoiceField(\n        choices=Author.objects.as_choices(),\n        overlay=\"Choose an author...\")\n\n    class Meta:\n        model = Entry\n\nLicense\nThe django code is licensed under the Simplified BSD\nLicense and is\ncopyright The Atlantic Media Company. View the LICENSE file under\nthe root directory for complete license and copyright information.\nThe Select2 javascript library included is licensed under the Apache\nSoftware Foundation License Version\n2.0. View the file\nselect2\/static\/select2\/select2\/LICENSE for complete license and\ncopyright information about the Select2 javascript library.\n","59":"Django Admin Locking\n \nPrevents users from overwriting each others changes in Django.\nRequirement\nDjango Admin Locking is tested in the following environments\n\nPython (2.7, 3.7)\nDjango (1.11, 2.0, 2.2)\n\nInstallation\nAdd 'locking' to your INSTALLED_APPS setting.\nINSTALLED_APPS = (\n    ...\n    'locking',\n)\nAdd the required URL pattern:\nurl(r'^locking\/', include('locking.urls')),\nUsage\nTo enable locking for a ModelAdmin:\nfrom django.contrib import admin\nfrom locking.admin import LockingAdminMixin\nfrom my_project.mt_models import MyModel\n\nclass MyModelAdmin(LockingAdminMixin, admin.ModelAdmin):\n     pass\n\nadmin.site.register(MyModel, MyModelAdmin)\nThe LockingAdminMixin will automatically add a new column that displays which rows are currently locked. To manually place this column add is_locked to the admin's list_display property.\nLocking Admin offers the following variables for customization in your settings.py:\n\nLOCKING_EXPIRATION_SECONDS - Time in seconds that an object will stay locked for without a 'ping' from the server. Defaults to 180.\nLOCKING_PING_SECONDS - Time in seconds between 'pings' to the server with a request to maintain or gain a lock on the current form. Defaults to 15.\nLOCKING_SHARE_ADMIN_JQUERY - Should locking use instance of jQuery used by the admin or should it use it's own bundled version of jQuery? Useful because older versions of Django do not come with a new enough version of jQuery for admin locking. Defaults to True.\nLOCKING_DB_TABLE - Used to override the default locking table name (locking_lock)\nLOCKING_DELETE_TIMEOUT_SECONDS - If not zero, locks will not be deleted immediately when a user leaves an admin form, but will instead be set to expire in the specified number of seconds. Specifying this setting can help avoid the following situation: a user hits 'save and continue' on a form, causing the page to reload. If locks are deleted instantly, someone else might grab the lock before the form loads again. If this value is specified, it should be set to the approximate time it takes a form to save (generally a few seconds). Defaults to 0.\n\nCleaning up expired locks\nOvertime, you may find it necessary to remove expired locks from the database. This can be done with the following management command\n$ python manage.py delete_expired_locks\n\nIf you have a non-zero specified for LOCKING_DELETE_TIMEOUT_SECONDS in your settings, you should setup a reoccurring Cron or Celery task to automatically run this management command on a regular interval.\nTesting\nYou will need to install the ChromeDriver\nnecessary to interface with your version of Chrome. You must ensure that the driver\napp resides somewhere on your $PATH.\nRunning the included test suite requires the following additional requirements:\n\ntox\ntox-venv\n\n$ pip install tox tox-venv\n\nAdditionally, for all tests to succeed, you will need Python 2.7 and 3.4-3.7 installed.\nJavaScript plugins for advanced widgets\nBy default, form field widgets are disabled by adding the attribute disabled = disabled to all inputs. If you are using a custom widget, such as a WYSIWYG editor, you may need to register a locking plugin to ensure it is correctly locked and unlocked.\nPlugin registration takes the following form:\nwindow.locking.LockingFormPlugins.register({\n    'enable': function(form) {  \/* Enabled my custom widget *\/ },\n    'disable': function(form) {  \/* Disable my custom widget *\/ }\n})\nFor an example, look at the included plugin for the CKEditor WYSIYG editor.\nCompatibility Notes\nThis app is compatible the popular admin theme django-grappelli\nLicense\nThis code is licensed under the Simplified BSD License. View the LICENSE file under the root directory for complete license and copyright information.\n","60":"Lethal Injection Drugs in Texas\nThe Texas Tribune's project tracking the state's supply of lethal injection drugs.\nThis project was produced and is maintained by Jolie McCullough using the Texas Tribune's Data Visuals kit.\nQuickstart\nPlease note - some static assets required to make this project work are only accessible to Texas Tribune developers.\nClone the project, then run npm install. Then pull down the assets with npm run assets:pull, and the data with npm run data:fetch. Use npm run serve to run the local development server.\nNow, get to work!\n","61":"TXLege Camera Status\nA quick script that attempts to confirm whether a Texas House or Senate Granicus stream is live or not.\nInstallation\nnpm install [--save-dev] txlege-camera-status\nUsage\nconst { isCameraLive } = require('txlege-camera-status');\n\n\/\/ first parameter is chamber, second parameter is camera ID\nisCameraLive('house', 3).then((cameraIsLive) => {\n  \/\/ if camera is live, `cameraIsLive` is true\n});\nThere's also a tiny command line tool built in, too. (Mostly exists from when I was testing the script.)\n> is-txlege-camera-live house 3\ntrue\nLicense\nMIT\n","62":"PostCSS Amp  \n\n\nPostCSS plugin to convert CSS according Accelerated Mobile Pages requirements [].\n\nAMP CSS requirements: https:\/\/github.com\/tinovyatkin\/postcss-amp\ni-amp-el {\n    \/* elements starting with i-amp- are banned\n}\n.boo {\n    filter: gray(.5); \/* filter is banned *\/\n    color: red !important; \/* important is banned *\/\n}\n.boo {\n  \/* all banned elements stripped *\/\n  color: red;\n}\nUsage\npostcss([ require('postcss-amp') ])\nSee PostCSS docs for examples for your environment.\n","63":"Talk \u00b7  \u00b7 \nOnline comments are broken. Our open-source commenting platform, Talk, rethinks how moderation, comment display, and conversation function, creating the opportunity for safer, smarter discussions around your work. Read more about Talk here.\nBuilt with <3 by The Coral Project, a part of Vox Media.\nGetting Started\nCheck out our Quickstart and Install guides to get started with Talk in our Technical Docs.\nProduct Guide\nLearn more about Talk, including a deep dive into features for commenters and moderators, and FAQs in our Talk Product Guide.\nPre-Launch Guide\nYou\u2019ve installed Talk on your server, and you\u2019re preparing to launch it on your site. The real community work starts now, before you go live. You have a unique opportunity pre-launch to set your community up for success. Read our Talk Community Guide.\nAdvanced Usage\nFor advanced configuration and usage of Talk, check out our Configuration and Integration how-tos. This covers topics in which you will need dev support to fully customize and integrate Talk, such as SSO\/authentication, creating and managing assets and articles, styling Talk with custom CSS, and setting up Notifications and SMTP support.\nVersions & Upgrading\nCheck our Releases page for the latest recommended release version. Releases All future even-numbered versions are considered stable LTS versions. We recommend the latest verified release for use in production environments.\nMore Resources\n\nOur Blog\nCommunity Guides for Journalism\nMore About Us\n\nEnd-to-End Testing\nTalk uses Nightwatch as our e2e testing framework. The testing infrastructure that allows us to run our tests in real browsers is provided with love by our friends at Browserstack.\n\nLicense\nTalk is released under the Apache License, v2.0.\n","64":"Tribune Code Grabber\nThis repo uses the latest version of the Data Visuals app kit.\nTo start run, npm install. This project requires Node v4.0.0. If you hit a syntax error on your initial attempts to serve the project locally, it's likely that you're trying to use an older version of node. Update node, completely remove your node_modules folder and re-run npm install.\nInstructions for adding or editing a component\nThe HTML for each component in Code Grabber is built using a Nunjucks {% macro %} called {{ copyBlock(id, formOptions, preview) }} located in app\/templates\/layouts\/macros.html. It takes three arguments:\n\nid: A string to identify the component and the targeted clipboard. For customizable components, it's also used to locate the form in main.js and set its behaviors.\nformOptions: The template will add a <form> tag around the options.\npreview: Not required. If set to 'load', the template will include a dashed frame to preview the component.\n\nEach group of components has a file in app\/templates\/includes\/ in which the parameters for these templates are set and then called. For example, here's how the {{ copyBlock() } is used in the read_more.html file:\n<!-- formOptions  -->\n{% set readmoreform %}\n  <label>Headline<\/label>\n  <input type=\"text\" id=\"readmore_headline\" value=\"\" required>\n  <label>Link to the story<\/label>\n  <input type=\"text\" id=\"readmore_link\" value=\"\" required>\n{% endset %}\n\n<!-- id, formOptions, preview -->\n{{ macro.copyBlock('readmorecode', readmoreform, preview='load')}}\n\n\nEach component has a function to build its codeBlock. For some components (readmore and twitterinline) that function is also used to initialized a preview on load, but for most, it's just called when the component's form is submitted. When the user submits the form, it picks up the variables, calls that component function with the variables to build a new codeBlock, then runs returnCode(codeBlock, id). (The id must match the id assigned to the component in {% copyBlock() %}). The returnCode() function uses the id to push the updated codeBlock to the DOM in both the preview frame and the  block for that component, and then triggers a hidden clipboard copy button for that component. Afterwards, it triggers copied(this.id) to show the Copied! tooltip.\n\/\/ build readmore codeBlock\nfunction readmore(headlineSlug, link, headline) {\n  var codeBlock = '<p class=\"readmore\" style=\"font-style: italic; padding-top: .5em; padding-bottom: .5em; vertically-align: middle;\"><span class=\"readmore--label\" style=\"color: #111111; font-family: Helvetica,Arial,sans-serif; font-size: .9em; font-style: italic; font-weight: 800; margin: 0 1em 1em 0; text-decoration: none; text-transform: uppercase;\">Read More<\/span><a onclick=\"ga(\\'send\\', \\'event\\', \\'codegrabber\\', \\'click\\', \\'readmore\\', \\'' + headlineSlug + '\\', {\\'nonInteraction\\': 1})\" class=\"readmore_link\" href=\"'+ link +'\">'+ headline +'<\/a><\/p>';\n\n  return codeBlock;\n}\n\n\/\/ submit readmore form\n$('#readmorecode_form').submit(function(e) {\n  var headline = $('#readmore_headline').val(),\n      link = $('#readmore_link').val(),\n      headlineSlug = slugify(headline),\n      \/\/ assign output from readmore() to codeBlock\n      codeBlock = readmore(headlineSlug, link, headline);\n\n  \/\/ update DOM & copy element\n  returnCode(codeBlock, 'readmorecode');\n\n  \/\/ provide user feedback\n  copied(this.id);\n\n  e.preventDefault();\n});\n\nDevelopment\nRun the following command to start the development server:\nnpm run serve\nWebpack\nThis kit uses the webpack module bundler.\nConnect to S3\nTo use the commands to deploy your project to Amazon S3, you'll need to add a profile to your ~\/.aws\/config. It should look something like this:\n[profile newsapps]\naws_access_key_id=YOUR_UNIQUE_ID\naws_secret_access_key=YOUR_SECRET_ACCESS_KEY\n\nDeployment\nRun these commands to build and deploy:\nnpm run build\nnpm run deploy\n\nThe project will deploy using the S3 bucket and slug found in your config.js.\nGoogle Sheets\nCurrently, the 2016 Festival Talent lineup is brought into the project through a Google Doc. If this is your first time using Google sheets with a Data Visuals Kit, you'll need to authorize your computer by saving a file named .tt_kit_google_client_secrets.json in your root directory. You can get the specific contents of that file from a fellow teammate.\nThe config.js is already set up to pull in the information. If you need to make changes or add another Google sheet or document, this is where you do it. To update the data pulled into the project, run npm run data\/fetch. You'll see an updated .json file in the data folder, which you can then reference in your app\/templates.\n","65":"React External Boilerplate\nTrying to develop a React app inside a code base that's predominantly something else -- Django or Rails, for example - can be annoying. If you wan't hot reloading and all that jazz, you're likely going to have to find a way to get Node.js running on top of your existing server. It's not impossible, but it's also not fun.\nThis boilerplate doesn't fix that issue but rather gives you some tools for building your React app as a separate repository, then bringing it into your main code base during its build process.\nIt's all super basic stuff -- but it'll save you a few minutes.\nCommands\n\nnpm start: Start the development server, hot reloading included.\nnpm run build: Build the bundle and stylesheet for production.\n\nHow to use it\nDo your React-y development as you normally would. Go crazy. Build 100 components. We like to put them in the src\/ directory, but that's up to you.\nThe important stuff lives in src\/index.js (the Webpack entry point). During development, your app will simply attach to the DOM node specified in the isDev conditional block. But after doing npm run build, the bundle has three special qualities:\n\nIt exports App as a CommonJS module. That means you can put your React app on NPM and include it in your main code base's package.json. This will allow for statements like import App from YourSeparateReactApp throughout your main code base.\nIt exports the object baseProps, which contains props defined in src\/index.js that are sent to <App\/>. This is useful if your workflow is something like that described in No. 1. You can define additional props inside your main code base and send both those and baseProps to <App\/>.\nIt exports the method renderApp(). This method accepts a DOM element as its first argument -- to which your React app widget will eventually attach. You can also send an object of additional props as the second argument. renderApp() could be useful if, instead of putting your package on NPM, you're putting the production bundle on a CDN.\n\nExamples\nImporting your React app into your main code base\n  import React from 'react';\n  import ReactDOM from 'react-dom';\n\n  import { App, baseProps } from 'YourSeparateReactApp';\n\n  const additionalProps = { bar: 'foo' };\n\n  ReactDOM.render(\n    <App {...additionalProps} {...baseProps} \/>,\n    document.getElementById('root')\n  );\nUsing a CDN\n  <script src=\"https:\/\/mycdn.org\/reactExternal.min.js\"><\/script>\n  <script>\n    var el = document.querySelector('#foo');\n    var additionalProps = { bar: 'foo' };\n\n    reactExternal.renderApp(el, additionalProps);\n    \/**\n      Your React widget will attach to <div id=\"foo\">.\n      Nothing fancy at all -- just allows you to define\n      your React app's attachment point inside the\n      document instead of that logic floating\n      somewhere in the CDN code.\n    *\/\n  <\/script>\n","66":"IKEA Minisite for Vox Creative\nIkea Minisite 2020\nLives at https:\/\/ikeatinyhome.curbed.com\/ and https:\/\/ikea-vc-2020.herokuapp.com\/index.html\n\nThe site is hosted on heroku, under the revenue team\nin the ikea-vc-2020 Project\nIt will autodeploy when commits are made to the master branch.\n\n","67":"Vizier\nA GUI for ai2html projects. Vizier makes it easy to use the New York Times' ai2html plugin for Adobe Illustrator.\n\nHow to use it\n\nDownload the most recent release.\nMake sure you adjust your Mac OS X Gatekeeper settings to allow applications from anywhere.\nThe first time you run the app, you will be prompted to install ai2html. This will replace any\npreviously installed ai2html.js script. If you installed Illustrator to a non-standard location\nyou may be asked to find the install folder.\nOpen the preference panel to specify the default project folder (where Vizier\nwill try to create new projects). You will also need to provide AWS settings if\nyou plan to publish anything.\nClick new project. You will be prompted to specify a name and save the new\nproject. This will create a new folder containing an ai file and a src folder.\nOpen the ai file by double clicking the project in Vizier.\nMake an awesome graphic.\nRun ai2html to export your graphic. File > Scripts > ai2html (If you don't see an ai2html option, you may need to restart illustrator).\nBy default ai2html will export every artboard. Ai2html looks at the pixel size\nof every artboard and the artboard will be displayed if the window is large enough.\nIf you want to exempt an artboard from export, adjust the artboard name to begin\nwith a minus -artboardname.\nOnce the ai2html export completes, return to Vizier, highlight the project\nand click deploy.\nOnce deploy is complete (green check will appear), right click on the project\nand click copy embed code.\nPaste the embed code into your story and publish!\nIf your CMS supports oembed urls, you can use the preview link to automatically\ndiscover the embed code!\n\nCaveats\nOut of the box, Vizier and the ai2html script it provides only supports Arial and Georgia fonts. If you want to use non-standard web fonts, you will need to create a .vizappconfig file and load it in the program.\nIf you notice a standard web font is missing or not working, please open a github issue about it. We won't add non-standard web fonts to the included fonts, even if it's free.\nCustomizing\nYou can write site config files for Vizier which include font data and css to customize the graphic preview, embed and ai2html script.\nThe config file is a valid YAML document with the extension .vizappconfig. Take a look at the example.\nDeveloping\nThis app uses Electron, Vue.js.\nClone this repo, then:\n# install dependencies\nnpm install\n\n# serve with hot reload at localhost:9080\nnpm run dev\n\n# build electron application for production\nnpm run build\n\n# run unit & end-to-end tests\nnpm test\n\n# lint all JS\/Vue component files in `src\/`\nnpm run lint\nContributing\nFork this repo, create a new branch on your fork, and make your changes there.\nOpen a pull request on this repo for consideration.\nIf its a small bugfix, feel free making the changes and opening a PR. If it's a\nfeature addition or a more substantial change, please open a github issue\noutlining the feature or change. This is just to save you time and make sure\nyour efforts can get aligned with other folks' plans.\n\nThis project was generated with electron-vue@7c4e3e9 using vue-cli. Documentation about the original structure can be found here.\n","68":"This repository contains the source files for the Prebid.js documentation site at Prebid.org.\nPlease see the sections below for more information.\n\nContributing\nLicense\nPrerequisites\nRunning Jekyll Locally\nThe Downloads Page\nThanks\n\n\nContributing\nThanks in advance for your contribution!  Contributors are listed in the Thanks section below.\nFor smaller changes, such as fixing a typo or adding a new section to an existing page, submit a pull request.\nFor larger changes such as reorganizing the site and moving\/removing content, you may want to open an issue so we can discuss the work beforehand.  This is a good idea because:\n\nWe want to value your time, so you don't do unnecessary work\nWe want to value our users' time; we don't want to break links and bookmarks for users\n\n\nLicense\nAll docs are under the license shown in the LICENSE file in this directory.\n\nPrerequisites\nThe site uses Jekyll, which is written in the Ruby language.\n\nfollow the instructions at https:\/\/jekyllrb.com\/docs\/installation\/ for your OS\ngem install github-pages\nstart Jekyll as described below\n\n\nRunning Jekyll Locally\nBefore submitting a pull request, you should run the site locally to make sure your edits actually work.\nTo get started editing the site and seeing your changes, clone this repo and enter the following commands in your terminal:\n\ncd path\/to\/prebid.github.io\nbundle exec jekyll serve\n\nYou should see output that looks something like this:\nConfiguration file: \/Users\/rloveland\/Dropbox\/Code\/prebid.github.io\/_config.yml  \n            Source: \/Users\/rloveland\/Dropbox\/Code\/prebid.github.io  \n       Destination: \/Users\/rloveland\/Dropbox\/Code\/prebid.github.io\/_site  \n Incremental build: disabled. Enable with --incremental  \n      Generating...   \n                    done in 13.596 seconds.  \n Auto-regeneration: enabled for '\/Users\/rloveland\/Dropbox\/Code\/prebid.github.io'  \nConfiguration file: \/Users\/rloveland\/Dropbox\/Code\/prebid.github.io\/_config.yml  \n    Server address: http:\/\/127.0.0.1:8080\/  \n  Server running... press ctrl-c to stop.  \n...  \n...  \n\nOpen the Server address URL in your browser, and you should see a locally running copy of the site.\nThe Downloads Page\nPlease don't submit PRs to the Prebid.org downloads page. That page gets updated in tandem with the Prebid.js release process.\nThe Downloads page is generated from the Markdown bidder adapter docs, so the process for updating is:\n\nYour adapter code is merged into Prebid.js\nYour bidder docs PR is submitted over here to the docs site\nYour adapter code is included with a release\nOnce your adapter code is actually released, we merge the adapter docs PR, and the Downloads page is automagically updated with a checkbox to include your adapter.\n\nThis means an adaptor is not available to download from Prebid.org as soon as the code gets merged into Prebid.js - it will be available after the next release (usually in a couple of weeks).\n\nThanks\nMany thanks to the following people who have submitted content to Prebid.org.  We really appreciate the help!\n","69":"Lightweight Speech to text desktop app for OSX Using IBM Watson API\nThis app was an initial prototype to test the quality of IBM STT, and is no longer activly supported I am now working on a more full fledge version at https:\/\/github.com\/OpenNewsLabs\/autoEdit_2 ( http:\/\/www.autoedit.io ).\nIBM Speech to text API\nTo use this app you need to get IBM Watson API keys for their speech to text service, by making an account with Bluemix\nUsage - Development\nIf you clone the repo you can start the app with npm start.\nUsage - User\nOr you can get the latest release packaged and ready for use here\nThis is a Tray Menu app.\n\nFirst you Select Media, audio or video you'd like to transcribe.\nNotifications show when a transcription as started and when it's finished.\nOn completion a editable text area shows you the transcription.\n\nBy default the transcription is also saved to clipboard.\nYou can disable Autosave to clipboard if working on text editing or making use of the system clipboard for some other program to avoid it overwriting something else you might be doing with it.\nSetting IBM Watson API keys\nFirst time you start the application you'd be prompt to set the API keys.\nShould you need to change those you can use shortcut cmd + shift + a.\nThese are saved inside the app as a json file wttskeys.json at the root of the application.\nWhich is in the .gitignore so that it doesn't accidentally gets added to git by mistake, when in development mode.\nOverview of project\n\nOnce you select a video, the app converts it into audio and sends it to the IBM Speech to text API.\nWhen the transcription comes back it's copied to clipboard, unless you un-tick the option in the menu.\nPaste the transcription wherever you want and take it from there.\n\nTechnical overview\nConvert video to audio\nThe video_to_audio module converts video or audio into IBM audio specs.  Initially modified from Sam Lavine's gist.\nAudio files are saved in .\/tmp\/audio folder.\nIBM Speech to text API\nThe stt folder contains the module to interact with the IBM Speech to text API.\nIf you want to dive more into this their documentation on how to interact with the API is pretty good.\nTranscribing video\ntranscribe.js requires both modules described above and brings it all together.\nConverts audio into video, and then sends it to Watson for transcriptions. Transcriptions are saved onto a text file in .\/tmp\/text folder.\nmodule returns the path to the text file.\nindex.js abstracts transcribe.js in case the interface needs to change at a later stage.\nNWJS\nindext.html contains the Implementation of the NWJS app.\nAdding Menu Tray to the application.\nSee comments in the code ``.\/index.html` and nwjs wiki as well as nwjs documentation for more on this.\nUser flow\nWhen a user selects a video it's transcribed, appropriate system notifications for start and end are triggered.\nWhen done unless option is un-ticked transcription is saved to clipboard.\nin which case user can click on Copy transcriptions to cliboard to get the transcriptions.\nBuild NWJS app\nOption 1\nUse deploy script\nnode deploy.js\n\nThis creates a build folder inside the repo. The build folder is also in .gitignore to avoid accidentally pushing it to remote.\nOption 2\nTo rebuild the app in NWJS refer to the documentation\nInstall nw-builder\nnpm install -g nw-builder\n\nFrom one level above the application folder (cd .. from root of repo)\nnwbuild -p osx64 .\/transcriber\n\ncreates a build folder that contains the app\nTodo\n\n Write proper test using testing framework.\n Ad some proper form of error handling\n IBM has a size limit of 100mb per audio post request. Double check if there's a use case when converting video to audio it exceeds that size. Rough test with 54gb video to audio with that module ended up 50 to 70 mb. So it would seem ok for now?\n\n","70":"Userstamp Plugin (v 2.0)\nOverview\nThe Userstamp Plugin extends ActiveRecord::Base to add automatic updating of 'creator',\n'updater', and 'deleter' attributes. It is based loosely on the ActiveRecord::Timestamp module.\nTwo class methods (model_stamper and stampable) are implemented in this plugin.\nThe model_stamper method is used in models that are responsible for creating, updating, or\ndeleting other objects. The stampable method is used in models that are subject to being\ncreated, updated, or deleted by 'stampers'.\nInstallation\nInstallation of the plugin can be done using the built in Rails plugin script. Issue the following\ncommand from the root of your Rails application:\n$ .\/script\/rails plugin install git:\/\/github.com\/delynn\/userstamp.git\n\nor add it to your Gemfile:\ngem 'userstamp'\n\nand run bundle install to install the new dependency.\nOnce installed you will need to restart your application for the plugin to be loaded into the Rails\nenvironment.\nUsage\nIn this new version of the Userstamp plug-in, the assumption is that you have two different\ncategories of objects; those that manipulate, and those that are manipulated. For those objects\nthat are being manipulated there's the Stampable module and for the manipulators there's the\nStamper module. There's also the actual Userstamp module for your controllers that assists in\nsetting up your environment on a per request basis.\nTo better understand how all this works, I think an example is in order. For this example we will\nassume that a weblog application is comprised of User and Post objects. The first thing we need to\ndo is create the migrations for these objects, and the plug-in gives you a userstamps\nmethod for very easily doing this:\nclass CreateUsers < ActiveRecord::Migration\n  def self.up\n    create_table :users, :force => true do |t|\n      t.timestamps\n      t.userstamps\n      t.name\n    end\n  end\n  \n  def self.down\n    drop_table :users\n  end\nend\n\nclass CreatePosts < ActiveRecord::Migration\n  def self.up\n    create_table :posts, :force => true do |t|\n      t.timestamps\n      t.userstamps\n      t.title\n    end\n  end\n  \n  def self.down\n    drop_table :posts\n  end\nend\nSecond, since Users are going to manipulate other objects in our project, we'll use the\nmodel_stamper method in our User class:\nclass User < ActiveRecord::Base\n  model_stamper\nend\nFinally, we need to setup a controller to set the current user of the application. It's\nrecommended that you do this in your ApplicationController:\nclass ApplicationController < ActionController::Base\n  include Userstamp\nend\nIf all you are interested in is making sure all tables that have the proper columns are stamped\nby the currently logged in user you can stop right here. More than likely you want all your\nassociations setup on your stamped objects, and that's where the stampable class method\ncomes in. So in our example we'll want to use this method in both our User and Post classes:\nclass User < ActiveRecord::Base\n  model_stamper\n  stampable\nend\n\nclass Post < ActiveRecord::Base\n  stampable\nend\nOkay, so what all have we done? The model_stamper class method injects two methods into the\nUser class. They are #stamper= and #stamper and look like this:\ndef stamper=(object)\n  object_stamper = if object.is_a?(ActiveRecord::Base)\n    object.send(\"#{object.class.primary_key}\".to_sym)\n  else\n    object\n  end\n  \n  Thread.current[\"#{self.to_s.downcase}_#{self.object_id}_stamper\"] = object_stamper\nend\n\ndef stamper\n  Thread.current[\"#{self.to_s.downcase}_#{self.object_id}_stamper\"]\nend\nThe big change with this new version is that we are now using Thread.current to save the current\nstamper so as to avoid conflict with concurrent requests.\nThe stampable method allows you to customize what columns will get stamped, and also\ncreates the creator, updater, and deleter associations.\nThe Userstamp module that we included into our ApplicationController uses the setter method to\nset which user is currently making the request. By default the 'set_stampers' method works perfectly\nwith the RestfulAuthentication plug-in:\ndef set_stampers\n  User.stamper = self.current_user\nend\nIf you aren't using ActsAsAuthenticated, then you need to create your own version of the\nset_stampers method in the controller where you've included the Userstamp module.\nNow, let's get back to the Stampable module (since it really is the interesting one). The Stampable\nmodule sets up before_* filters that are responsible for setting those attributes at the appropriate\ntimes. It also creates the belongs_to relationships for you.\nIf you need to customize the columns that are stamped, the stampable method can be\ncompletely customized. Here's an quick example:\nclass Post < ActiveRecord::Base\n  acts_as_stampable :stamper_class_name => :person,\n                    :creator_attribute  => :create_user,\n                    :updater_attribute  => :update_user,\n                    :deleter_attribute  => :delete_user\nend\nIf you are upgrading your application from the old version of Userstamp, there is a compatibility\nmode to have the plug-in use the old \"_by\" columns by default. To enable this mode, add the\nfollowing line to the Rails.root\/config\/initializers\/userstamp.rb file:\nDdb::Userstamp.compatibility_mode = true\nIf you are having a difficult time getting the Userstamp plug-in to work, I recommend you checkout\nthe sample application that I created. You can find this application on GitHub\nUninstall\nUninstalling the plugin can be done using the built in Rails plugin script. Issue the following\ncommand from the root of your application:\nscript\/plugin remove userstamp\n\nDocumentation\nRDoc has been run on the plugin directory and is available in the doc directory.\nRunning Unit Tests\nThere are extensive unit tests in the \"test\" directory of the plugin. These test can be run\nindividually by executing the following command from the userstamp directory:\nruby test\/compatibility_stamping_test.rb\nruby test\/stamping_test.rb\nruby test\/userstamp_controller_test.rb\n\nBugs & Feedback\nBug reports and feedback are welcome via GitHub Issues. I also encouraged everyone to clone the git repository and make modifications--I'll be more than happy to merge any changes from other people's branches that would be beneficial to the whole project.\nCredits and Special Thanks\nThe original idea for this plugin came from the Rails Wiki article entitled\n\"Extending ActiveRecord\" on the Rails Wiki (no longer published).\n","71":"Gliss\nGliss is a constraint based layout system for articles. It uses the CSS properties width and max-width instead of media queries to change the layout at different viewport sizes. You can see a demo here, and the related source code.\nGetting started\nInstall with Compass:\nIn your terminal execute:\ngem install gliss-layout\n\nIn your config.rb file write\nrequire 'gliss'\n\nAnd finally, import Gliss into your stylesheet\n@import 'gliss';\n\nInstall without Compass:\nDownload the latest release.\nUnzip into your project and import Gliss into your stylesheet\n@import 'gliss';\n\nDocumentation\nFirst, you are going to want to configure Gliss. Here are the main variables to define your grid:\n\n\nVariableDefaultDescription\n\n$gliss-max-width80remMax width of content container. This must be a fixed width (em, rem or px)\n\n$gliss-gutter1remGutter width, must be same units as $max-width.\n\n$gliss-margin4%Margins on either side of the content container.\n\n$gliss-cols12Number of columns the grid is divided into.\n\nSo far this just provides you with a basic toolset, but if you want to have something more out of the box to work with, set $gliss-modules to true and define these variables as needed:\n\n\nVariableDefaultDescription\n\n$gliss-modulesfalseGenerates default styles so you can get going with writing markup.\n\n$gliss-wrapper'article'The wrapper for the entire article.\n\n$gliss-text'.text'A block of text.\n\n$gliss-figure'figure'A figure to contain an image, media or graphic that you wish to be floated alongside the main text column.\n\nGliss is primarily designed to help out with math by providing a few mixins that set the width constraints and alignment on elements.\nThe primary mixin you will use is gliss() and you can pass the small and large constraints to it. If we wanted foo to be 6 columns wide at small sizes and 3 columns wide at the largest sizes then we would write it like this:\nfoo {\n\t@include gliss(6,3);\n}\nThis will generate the following CSS:\nfigure._quarter {\n\twidth: 19.25rem;\n\tmax-width: calc(50% + -0.5rem);\n}\nFor alignment, you can choose left, right, or center by using the following mixins:\nfoo {\n\t@include gliss-center();\n\t@include gliss-left();\n\t@include gliss-right();\n}\nIf you are generating Gliss modules, then use the module definitions you have created with your variables.\nFirst off, the markup should be structure something like this:\n<article>\n  <figure><\/figure>\n  <div class=\"text\"><\/div>\n  <figure><\/figure>\n  <div class=\"text\"><\/div>\n<\/article>\nGliss figures have a number of modifiers available. ._half, ._third, and ._quarter set elements to be half width, a quarter width, or one third the width of the page at the max width of the layout. All three degrade to half width. ._left and ._right will float these figures left or right.\n._hang is an additional figure modifier that will align the figure with the main text column and also push it flush with the side of the article. You can use ._left and ._right with hanging figures.\nThis framework is intended as a barebones toolset, not something that will solve all your layout problems for you. It is intended as a beginning to solving some tough problems, not an end and it\u2019s likely that you will find value in reworking components to fit your needs.\nAuthors\nScott Kellum\nContribute\nThis is an active project and we encourage contributions. Please review our guidelines and code of conduct before contributing.\nLicense\nCopyright (c) 2015, Vox Media, Inc.\nAll rights reserved.\nBSD license\nRedistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n\n\nRedistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\n\n\nRedistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and\/or other materials provided with the distribution.\n\n\nNeither the name of the copyright holder nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\n\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n","72":"A-Frame Boilerplate\nDownload a video\nsh smapi\/downloadmp4.sh \/news 1034_20171013170000\nBoilerplate for creating WebVR scenes with A-Frame.\nAlternatively, check out the A-Frame Starter on\nglitch.com for a more interactive way on getting\nstarted.\nGetting Started\nThere are two easy options for obtaining this A-Frame scene. It's then up to you to make it your own!\nOption 1: Download the ZIP kit \ud83d\udce6\n\nAfter you have downloaded and extracted this .zip file containing the contents of this repo, open the resulting directory, and you'll be have your scene ready in these few steps:\nnpm install && npm start\nopen http:\/\/localhost:3000\/\n\n\nOption 2: Fork this Git repo \ud83c\udf74\ud83d\udc19\nAlternatively, you can fork this repo to get started, if you'd like to maintain a Git workflow.\nAfter you have forked this repo, clone a copy of your fork locally and you'll be have your scene ready in these few steps:\ngit clone https:\/\/github.com\/aframevr\/aframe-boilerplate.git\ncd aframe-boilerplate && rm -rf .git && npm install && npm start\nopen http:\/\/localhost:3000\/\n\n\n\ud83d\udcf1 Mobile pro tip: Upon starting the development server, the URL will be logged to the console. Load that URL from a browser on your mobile device. (If your mobile phone and computer are not on the same LAN, consider using ngrok for local development and testing. Browsersync is also worth a gander.)\n\n\nOption 3: Fork this CodePen example \ud83c\udf74\ud83d\udcbe\u2712\ufe0f\nOr, you can simply fork this CodePen example to dive right in. Enjoy!\nPublishing your scene\nIf you don't already know, GitHub offers free and awesome publishing of static sites through GitHub Pages.\nTo publish your scene to your personal GitHub Pages:\nnpm run deploy\n\nAnd, it'll now be live at http:\/\/your_username.github.io\/ :)\n\nTo know which GitHub repo to deploy to, the deploy script first looks at the optional repository key in the package.json file (see npm docs for sample usage). If the repository key is missing, the script falls back to using the local git repo's remote origin URL (you can run the local command git remote -v to see all your remotes; also, you may refer to the GitHub docs for more information).\n\nStill need Help?\nInstallation\nFirst make sure you have Node installed.\nOn Mac OS X, it's recommended to use Homebrew to install Node + npm:\nbrew install node\n\nTo install the Node dependencies:\nnpm install\n\nLocal Development\nTo serve the site from a simple Node development server:\nnpm start\n\nThen launch the site from your favourite browser:\nhttp:\/\/localhost:3000\/\nIf you wish to serve the site from a different port:\nPORT=8000 npm start\n\nLicense\nThis program is free software and is distributed under an MIT License.\n","73":"THIS APP IS NO LONGER MAINTAINED.\nDependencies\n\nNode 6.11.3\nYarn 0.27.5\n\nThese versions are pinned in Dockerfile, so you shouldn't have to worry about it.\nSet-up\n\nCreate an env-docker file in the root. Fill it out according to what's in env.sample.\nRun make. This will take a few minutes the first time you run it as Docker has to build the image from scratch.\n\nFiles and directories of note\nsource\/\nContains files that Middleman will eventually process and move to the build\/ directory.\n\nEverything in source\/scss gets compiled into source\/css, which is .gitignored.\nsource\/javascripts contains ES6 modules that are eventually bundled into source\/javascripts\/bundle.js, which is ignored from version control.\n\nutils\/\n\ndeploy.sh: A shell script for deployment to S3.\ncritical.js: A Node script for inlining critical CSS.\n\ndata\/\nContains YAML files, the data of which is fed into some ERB templates.\nbrowserslist\nTells Autoprefixer what browsers we care about.\npostcss.config.js\nTells Webpack what PostCSS stuff we want it to do.\nCommands\n\nyarn run dev: Fire up the development server. This will enable live reloading of templates, JavaScript and CSS.\nyarn run build: Build for production.\nyarn run clean: Clean out the build\/ and .tmp\/ directories.\nyarn run middleman: Do the official Middleman build process.\nyarn run critical: Inline critical CSS.\nyarn run js:dev: Put Webpack in watch mode.\nyarn run js:prod: Build the Webpack bundle for production.\nyarn run deploy: Deploy to S3.\nyarn run build:deploy: Build for production, then deploy.\n\nYou'll typically only need these three: yarn run dev, yarn run build and yarn run deploy.\nDeployment\n\nBuild for production: yarn run build\nPush to S3: yarn run deploy (this also automatically busts the FB OG cache)\n\nYou can combine the build and deploy into one step with yarn run build:deploy.\nCampaign IDs\nMembership uses unique IDs to track specific campaigns (FMD, Giving Tuesday, etc.). If a campaign is in progress, we hard-code that ID into the donation-widget form (see markup below). This is to ensure every click of \"Go To Checkout\" passes the ID onto our checkout app.\n<input id=\"campaign-id\" type=\"hidden\" value=\"<campaign-id>\" name=\"campaignId\">\nIf a campaign is not in progress, that hidden field should not be present. However, we still want to pass campaign IDs to checkout if campaignId is included as a query parameter. (For campaigns, our membership folks create special URLs to support.texastribune.org that include that query parameter.) That's why this code exists. The thinking is we want to ensure remnant donations spurred by campaign marketing materials still get recorded as part of that campaign, even if the date of giving is after we've removed the hidden field.\nBrowser support\nWe autoprefix for everything in browserslist but officially only support modern browsers plus Internet Explorer 9.\n","74":"djangocms-text-ckeditor\nText Plugin for django-cms with CK-Editor.\nThe latest version of this package supports:\n\nDjango >= 1.8\ndjango CMS >= 3.3\n\n\nWarning\n\nFor django CMS 3.4.x use djangocms-text-ckeditor >= 3.2.x (e.g.: version 3.2.1).\nFor django CMS 3.3.x use djangocms-text-ckeditor >= 3.1.x (e.g.: version 3.1.0).\nFor django CMS 3.2.x use djangocms-text-ckeditor <= 2.9.x (e.g.: version 2.9.3).\nFor django CMS 3.0 and 3.1 use djangocms-text-ckeditor <= 2.7 (e.g.: version 2.7.0).\nFor django CMS 2.3 and 2.4 use the djangocms-text-ckeditor 1.x releases (e.g.: version 1.0.10).\nFor Django 1.4 and 1.5 use djangocms-text-ckeditor < 2.7.\ncms.plugins.text and djangocms-text-ckeditor can't be used at the same time.\n\n\n\nInstallation\nThis plugin requires django CMS 3.3 or higher to be properly installed.\n\nIn your projects virtualenv, run pip install djangocms-text-ckeditor.\nAdd djangocms_text_ckeditor to your INSTALLED_APPS (the order does not matter).\nRun manage.py migrate djangocms_text_ckeditor.\n\n\nSome notes:\n\nIf upgrading from previous djangocms_text_ckeditor, be aware that the\nnames of the migration modules have changed:\nDjango 1.6: djangocms_text_ckeditor.migrations to\ndjangocms_text_ckeditor.south_migrations\nDjango 1.7: djangocms_text_ckeditor.migrations_django to\ndjangocms_text_ckeditor.migrations\n\n\nIf using Django 1.6 add 'djangocms_text_ckeditor': 'djangocms_text_ckeditor.south_migrations',\nto SOUTH_MIGRATION_MODULES  (or define SOUTH_MIGRATION_MODULES if it does not exists);\nIf using Django 1.7 and you were using version prior to 2.5, remove\ndjangocms_text_ckeditor from MIGRATION_MODULES;\n\n\nUpgrading from cms.plugins.text\n\nRemove cms.plugins.text from INSTALLED_APPS\nAdd djangocms_text_ckeditor to INSTALLED_APPS\nRun python manage.py migrate djangocms_text_ckeditor 0001 --fake\n\n\nUsage\n\nDefault content in Placeholder\nIf you use Django-CMS >= 3.0, you can use TextPlugin in \"default_plugins\"\n(see docs about the CMS_PLACEHOLDER_CONF setting in Django CMS 3.0).\nTextPlugin requires just one value: body where you write your default\nHTML content. If you want to add some \"default children\" to your\nautomagically added plugin (i.e. a LinkPlugin), you have to put children\nreferences in the body. References are \"%(_tag_child_<order>)s\" with the\ninserted order of chidren. For example:\nCMS_PLACEHOLDER_CONF = {\n    'content': {\n        'name' : _('Content'),\n        'plugins': ['TextPlugin', 'LinkPlugin'],\n        'default_plugins':[\n            {\n                'plugin_type':'TextPlugin',\n                'values':{\n                    'body':'<p>Great websites : %(_tag_child_1)s and %(_tag_child_2)s<\/p>'\n                },\n                'children':[\n                    {\n                        'plugin_type':'LinkPlugin',\n                        'values':{\n                            'name':'django',\n                            'url':'https:\/\/www.djangoproject.com\/'\n                        },\n                    },\n                    {\n                        'plugin_type':'LinkPlugin',\n                        'values':{\n                            'name':'django-cms',\n                            'url':'https:\/\/www.django-cms.org'\n                        },\n                    },\n                ]\n            },\n        ]\n    }\n}\n\n\nCKEDITOR_SETTINGS\nYou can override the setting CKEDITOR_SETTINGS in your settings.py:\nCKEDITOR_SETTINGS = {\n    'language': '{{ language }}',\n    'toolbar': 'CMS',\n    'skin': 'moono-lisa',\n}\n\nThis is the default dict that holds all CKEditor settings.\n\nCustomizing plugin editor\nTo customize the plugin editor, use toolbar_CMS attribute, as in:\nCKEDITOR_SETTINGS = {\n    'language': '{{ language }}',\n    'toolbar_CMS': [\n        ['Undo', 'Redo'],\n        ['cmsplugins', '-', 'ShowBlocks'],\n        ['Format', 'Styles'],\n    ],\n    'skin': 'moono-lisa',\n}\n\n\nCustomizing HTMLField editor\nIf you use HTMLField from djangocms_text_ckeditor.fields in your own\nmodels, use toolbar_HTMLField attribute:\nCKEDITOR_SETTINGS = {\n    'language': '{{ language }}',\n    'toolbar_HTMLField': [\n        ['Undo', 'Redo'],\n        ['ShowBlocks'],\n        ['Format', 'Styles'],\n    ],\n    'skin': 'moono-lisa',\n}\n\nYou can further customize each HTMLField field by using different\nconfiguration parameter in your settings:\nmodels.py\n\nclass Model1(models.Model):\n    text = HTMLField(configuration='CKEDITOR_SETTINGS_MODEL1')\n\nclass Model2(models.Model):\n    text = HTMLField(configuration='CKEDITOR_SETTINGS_MODEL2')\n\nsettings.py\n\nCKEDITOR_SETTINGS_MODEL1 = {\n    'toolbar_HTMLField': [\n        ['Undo', 'Redo'],\n        ['ShowBlocks'],\n        ['Format', 'Styles'],\n        ['Bold', 'Italic', 'Underline', '-', 'Subscript', 'Superscript', '-', 'RemoveFormat'],\n    ]\n}\n\nCKEDITOR_SETTINGS_MODEL2 = {\n    'toolbar_HTMLField': [\n        ['Undo', 'Redo'],\n        ['Bold', 'Italic', 'Underline', '-', 'Subscript', 'Superscript', '-', 'RemoveFormat'],\n    ]\n}\n\n\nAdd configuration='MYSETTING' to the HTMLField usage(s) you want to\ncustomize;\nDefine a setting parameter named as the string used in the configuration\nargument of the HTMLField instance with the desidered configuration;\n\nValues not specified in your custom configuration will be taken from the global\nCKEDITOR_SETTINGS.\nFor an  overview of all the available settings have a look here:\nhttp:\/\/docs.ckeditor.com\/#!\/api\/CKEDITOR.config\n\nInline preview\nThe child plugins of TextPlugin can be rendered directly inside CKEditor if\ntext_editor_preview isn't False. However there are few important points\nto note:\n\nby default CKEditor doesn't load CSS of your project inside the editing area\nand has specific settings regarding empty tags, which could mean that things\nwill not look as they should until CKEditor is configured correctly.\nSee examples:\n\n\nadd styles and js configuration\nstop CKEditor from removing empty spans (useful for iconfonts)\n\n\n\nif you override widget default behaviour - be aware that it requires the\nproperty \"allowedContent\" to contain cms-plugin[*] as this custom tag is\nwhat allows the inline previews to be rendered\n\n\n\nDrag & Drop Images\nIn IE and Firefox based browsers it is possible to drag and drop a picture into the text editor.\nThis image is base64 encoded and lives in the 'src' attribute as a 'data' tag.\nWe detect this images, encode them and convert them to picture plugins.\nIf you want to overwirite this behavior for your own picture plugin:\nThere is a setting called:\nTEXT_SAVE_IMAGE_FUNCTION = 'djangocms_text_ckeditor.picture_save.create_picture_plugin'\n\nyou can overwrite this setting in your settings.py and point it to a function that handles image saves.\nHave a look at the function create_picture_plugin for details.\nTo completely disable the feature, set TEXT_SAVE_IMAGE_FUNCTION = None.\n\nTranslations\nIf you want to help translate the plugin please do it on transifex:\nhttps:\/\/www.transifex.com\/projects\/p\/django-cms\/resource\/djangocms-text-ckeditor\/\n\nUsage as a model field\nIf you want to use the widget on your own model fields, you can! Just import the provided HTMLField like so:\nfrom djangocms_text_ckeditor.fields import HTMLField\n\nAnd use it in your models, just like a TextField:\nclass MyModel(models.Model):\n    myfield = HTMLField(blank=True)\n\nThis field does not allow you to embed any other CMS plugins within the text editor. Plugins can only be embedded\nwithin Placeholder fields.\nIf you need to allow additional plugins to be embedded in a HTML field, convert the HTMLField to a Placeholderfield\nand configure the placeholder to only accept TextPlugin. For more information on using placeholders outside of the CMS see:\nhttp:\/\/docs.django-cms.org\/en\/latest\/introduction\/templates_placeholders.html\n\nAuto Hyphenate Text\nYou can hyphenate the text entered into the editor, so that the HTML entity &shy; (soft-hyphen)\nautomatically is added in between words, at the correct syllable boundary.\nTo activate this feature, pip install django-softhyphen. In settings.py add 'softhyphen'\nto the list of INSTALLED_APPS. django-softhyphen also installs hyphening dictionaries for 25\nnatural languages.\nIn case you already installed django-softhyphen but do not want to soft hyphenate, set\nTEXT_AUTO_HYPHENATE to False.\n\nExtending the plugin\n\nNote\nAdded in version 2.0.1\n\nYou can use this plugin as base to create your own CKEditor-based plugins.\nYou need to create your own plugin model extending AbstractText:\nfrom djangocms_text_ckeditor.models import AbstractText\n\nclass MyTextModel(AbstractText):\n    title = models.CharField(max_length=100)\n\nand a plugin class extending TextPlugin class:\nfrom djangocms_text_ckeditor.cms_plugins import TextPlugin\nfrom .models import MyTextModel\n\n\nclass MyTextPlugin(TextPlugin):\n    name = _(u\"My text plugin\")\n    model = MyTextModel\n\nplugin_pool.register_plugin(MyTextPlugin)\n\nNote that if you override the render method that is inherited from the base TextPlugin class, any child text\nplugins will not render correctly. You must call the super render method in order for plugin_tags_to_user_html()\nto render out all child plugins located in the body field. For example:\nfrom djangocms_text_ckeditor.cms_plugins import TextPlugin\nfrom .models import MyTextModel\n\n\nclass MyTextPlugin(TextPlugin):\n    name = _(u\"My text plugin\")\n    model = MyTextModel\n\n    def render(self, context, instance, placeholder):\n        context.update({\n            'name': instance.name,\n        })\n        # Other custom render code you may have\n    return super(MyTextPlugin, self).render(context, instance, placeholder)\n\nplugin_pool.register_plugin(MyTextPlugin)\n\nYou can further customize your plugin as other plugins.\n\nAdding plugins to the \"CMS Plugins\" dropdown\nIf you have another plugin that you want to use inside texts you can make them appear in the dropdown by making them text_enabled.\nCheck in django-cms doc how to do this.\n\nConfigurable sanitizer\ndjangocms-text-ckeditor uses html5lib to sanitize HTML to avoid\nsecurity issues and to check for correct HTML code.\nSanitisation may strip tags usesful for some use cases such as iframe;\nyou may customize the tags and attributes allowed by overriding the\nTEXT_ADDITIONAL_TAGS and TEXT_ADDITIONAL_ATTRIBUTES settings:\nTEXT_ADDITIONAL_TAGS = ('iframe',)\nTEXT_ADDITIONAL_ATTRIBUTES = ('scrolling', 'allowfullscreen', 'frameborder')\n\nIn case you need more control on sanitisation you can extend AllowTokenParser class and define\nyour logic into parse() method. For example, if you want to skip your donut attribute during\nsanitisation, you can create a class like this:\nfrom djangocms_text_ckeditor.sanitizer import AllowTokenParser\n\n\nclass DonutAttributeParser(AllowTokenParser):\n\n    def parse(self, attribute, val):\n        return attribute.startswith('donut-')\n\nAnd add your class to ALLOW_TOKEN_PARSERS settings:\nALLOW_TOKEN_PARSERS = (\n    'mymodule.DonutAttributeParser',\n)\n\nNOTE: Some versions of CKEditor will pre-sanitize your text before passing it to the web server,\nrendering the above settings useless. To ensure this does not happen, you may need to add the\nfollowing parameters to CKEDITOR_SETTINGS:\n...\n'basicEntities': False,\n'entities': False,\n...\n\nTo completely disable the feature, set TEXT_HTML_SANITIZE = False.\nSee the html5lib documentation for further information.\n\nSearch\ndjangocms-text-ckeditor works well with aldryn-search to make text content using Haystack.\n\nAbout CKEditor\nThe current integrated Version of CKeditor is 4.6.2. For a full documentation visit: http:\/\/ckeditor.com\/\n\nBuilding the JavaScript\ndjangocms-text-ckeditor distributes a javascript bundle required for the\nplugin to work, which contains CKEditor itself and all the necessary plugins for\nfunctioning within CMS. To build the bundle you need to have to install\ndependencies with npm install and then to run gulp bundle.\nThis command also updates the file name loaded based on the file contents.\n\nUpdating the CKEditor\nMake sure to use the url in build config\n<https:\/\/github.com\/divio\/djangocms-text-ckeditor\/blob\/master\/djangocms_text_ckeditor\/static\/djangocms_text_ckeditor\/ckeditor\/build-config.js#L16>_.\n","75":"News Apps Style Guide\nThe Texas Tribune's framework for basic styles used on news apps projects.\nCurrent Features\n\nColors\nUtils\nTypography\nProse \/ Graphics Themes\nButtons\nTables\nLogos\n\nQuickstart\nWe've built this style guide using the News Apps' app kit. The project pulls styles from the News Apps' Styles repo.\n","76":"500 Page\nCloudflare lets us customize our 500 page by giving it the address of a static page. In order to show any changes to\nthe branch, you'll have to republish the page in the Cloudflare.\nIf readers are seeing this page, the site's in pretty bad shape, so we shouldn't include a nav bar or direct\nthem anywhere that's just going to error out again. Hence the Twitter feed.\nFrom the Cloudflare docs: The maximum customized page size is 1.5 MB.\ndeploying\nFrom the Cloudflare admin, find the 500 page section. Choose 'customize' and put the url of the raw\nindex file in the admin.\nstyling\nAn example of the HTML generated by the 500 token is as follows (presumably the content changes according to which 500 error):\n<div class=\"cf-error-details cf-error-522\">\n  <h1>Connection timed out<\/h1>\n  <p data-translate=\"connection_timed_out\">The initial connection between CloudFlare's network and the origin web server timed out. As a result, the web page can not be displayed.<\/p>\n  <ul class=\"cferror_details\">\n    <li>Ray ID: 000000000000000<\/li>\n    <li>Your IP address: 71.41.155.50<\/li>\n    <li>Error reference number: 522<\/li>\n    <li>CloudFlare Location: San Jose<\/li>\n  <\/ul>\n<\/div>\n","77":"Pinpoint Editor\nPinpoint Editor is a web app for quickly creating and editing Pinpoint maps.\nFeatures:\n\nSimple user interface allows maps to be created in seconds.\nFlexible Angular app with Node backend.\nBuilt-in support for uploading JSON data files to Amazon S3.\n\nHow to set up Pinpoint Editor\nPinpoint Editor requires:\n\nA node.js server\nA PostgresSQL database\nNPM and Bower for installing dependencies\n(optional) Amazon S3 to host data\n\nHere's how to install it locally:\nNote: If you have trouble setting up Pinpoint Editor, please open a ticket on GitHub.\n\n\nInstall required software\nIf on OS X, you can install all software using these commands:\n # Install Brew\n ruby -e \"$(curl -fsSL https:\/\/raw.githubusercontent.com\/Homebrew\/install\/master\/install)\"\n\n # Install NodeJS.\n brew install node\n\n # Install PostgreSQL.\n brew install postgresql\n\n # Install Bower.\n npm install bower\n\n\n\nSet up database\nCreate a PostgresSQL database. You can name it anything you like.\n createdb pinpointDb\n\nSet DATABASE_URL environment variable.\n export DATABASE_URL='postgresql:\/\/localhost\/pinpointDb'\n\nRun migration script to set up table and load examples.\n psql $DATABASE_URL < build\/migrate.sql\n\nYou may need to start the database server manually:\n pg_ctl -D \/usr\/local\/var\/postgres -l \/usr\/local\/var\/postgres\/server.log start\n\n\n\nInstall dependencies\n # Install server-side dependencies\n npm install\n\n # Install client-side dependencies\n bower install\n\n\n\nConfigure settings\nGenerate a new Google Maps API key by following these instructions and add it to config.json (under the googleMapsAPIKey property).\nOptional: To enable AWS S3 export, set these environment variables:\n export AWS_S3_KEY='XXXXXXXXXXXXXX'\n export AWS_S3_SECRET='XXXXXXXXXXXXXX'\n export AWS_BUCKET='XXXXXXXXXXXXXX'\n\n\n\nRun the server!\n node server.js\n\nYou will then be able to access Pinpoint at http:\/\/localhost:3001.\n\n\nDocker setup\nIf you have Docker installed you can run the Pinpoint editor by simply typing docker-compose up. You can\nthen access Pinpoint at http:\/\/localhost:3001.\nArchitecture\nOn the server, Pinpoint uses the minimal Express framework for routing. Data is stored as JSON using PostgresSQL's native JSON data type, which can then be accessed via a simple API (see below for details). Data can then be exported to S3-hosted static JSON for production use.\nOn the client, Pinpoint is an Angular app made up of multiple custom directives. Key files are script.js and directives\/map-detail.html. Dependencies are managed using Bower.\nAPI routes\n\nGet all maps GET - \/api\/maps\nGet map by id GET - \/api\/maps\/:id\nCreate map POST - \/api\/maps\/\n\nhttp request header must be Content-Type: application\/json\nhttp request body contains the entire data object for that record\nreturns HTTP\/1.1 201 Created - {\"id\": id, \"message\": \"Map created\"}\n\n\nUpdate map PUT - \/api\/maps\/:id\n\nhttp request header must be Content-Type: application\/json\nhttp request body contains the entire data object for that record\nreturns HTTP\/1.1 200 OK - {\"message\": \"Map updated\"}\n\n\n\nConfiguration file\nVarious settings are controlled via config.json. See config-example.json for dummy values.\nEditor interface options\n\ngoogleMapsAPIKey: (required) Google maps API key (get one here)\ntitle: Page title, e.g. The Example Journal Map Tool\ngreeting: Message to go beneath page title. HTML is allowed.\nhelpLink: URL of an external help page\npreviewLink: URL which, with the current map's slug on the end, links to a preview\nliveLink: URL which, with the current map's slug on the end, links to the live production page for the current map\ns3url: URL which, with the current map's slug (and \".json\") on the end, links to the S3-hosted static JSON\ngeojsonStyles: Array\n\nObject\n\nclass: css class for style (string) - eg. \"dashed-clockwise\"\nname: descriptive name for style (string) - eg. \"Dashed, animated clockwise\"\n\n\n\n\n\nMap setting options\nThese are used in all Pinpoint instances in the editor.\n\nbasemap: Leaflet tilelayer URL (string) - eg. \"http:\/\/{s}.somedomain.com\/blabla\/{z}\/{x}\/{y}.png\"\nbasemapCredit: Credit line for tilelayer - eg. \"Leaflet | \u00a9 Mapbox | \u00a9 OpenSteetMap contributors\"\n\nVersion history\nv1.2.1 (27 March, 2017)\n\nBugfixes for editor interface\n\nv1.2.0 (17 February, 2017)\n\nNew feature: basemap selection\nGoogle maps API key controlled via config.json\nEasier customisation of interface text via config.json\nAdd pagination to homepage\n\nv1.1.0 (17 July, 2015)\n\nUpdate bower.json to allow any 1.1.* versions of Pinpoint library\nAdd .bowerrc to fix bower_components location\nAdd helpful error message if server port is in use\n\nv1.0.1\n\nUpdate bower.json to allow any 1.0.* versions of Pinpoint library\n\nv1.0.0\n\nInitial release\n\n","78":"ballot-tally \u534c\nA lightweight node.js wrapper for the U.S. AP Elections API 2.x.\nHere's a minimal example, which fetches results for Presidential primaries held on April 26, 2016.\nvar ballotTally = require('ballot-tally');\nvar client = new ballotTally('v2', process.env.AP_API_KEY);\nvar query = new ballotTally.Query.Elections(client, '2016-04-26');\n\nquery.level('fipscode').officeID('P').raceType('D').raceType('R')\n  .fetch(function(err, data) {\n      if (!err) console.log(data);\n    });\nUse this lightweight library to quickly create a script to fetch raw results data from the AP.\nFor heavy lifting, check out the database-ready library Elex, developed by our friends at The New York Times & NPR.\nInstallation\n$ npm install ballot-tally\nUsage\n\nGet an API key from the AP, then set it as the AP_API_KEY environment variable\n\n$ export AP_API_KEY='your_key_here'\n\nCreate a client object\n\nvar ballotTally = require('ballot-tally');\n\nvar apiKey = process.env.AP_API_KEY;\n\nvar client = new ballotTally('v2', apiKey);\n\nCreate a query builder\n\nvar query = new ballotTally.Query.Elections(client, '2012-11-06');\n\nBuild query to filter results based on criteria\n\nquery.statePostal('FL')\n  .test(false)\n  .level('fipscode')\n  .officeID('P')\n  .raceType('G');\n\nFetch results\n\nquery.fetch(function(err, data) {\n  if (!err) {\n    console.log(\"%j\", data);\n  }\n  else {\n    console.error(err);\n  }\n});\nRaw queries\nInstead of using the query builder to fetch results, you can connect to the API with raw queries.\n\/\/ This makes a query which is identical to the previous example.\n\nvar ballotTally = require('ballot-tally');\n\nvar apiKey = process.env.AP_API_KEY;\n\nvar client = new ballotTally('v2', apiKey);\n\nclient.elections('2012-11-06',\n  {\n    statepostal: 'FL',\n    test: false,\n    level: 'fipscode',\n    officeID: 'P',\n    raceTypeId: 'G'\n  },\n  function(err, data) {\n    if (!err) {\n      console.log(\"%j\", data);\n    }\n    else {\n      console.error(err);\n    }\n  }\n);\nQuery options\nQuery options match with the API filtering parameters. Please refer to API documentation for more information.\nCommon queries\nGet top level (State) results for Primaries:\nquery.level('state').officeID('P').raceType('D').raceType('R')\n  .fetch(function(err, data) {\n      if (!err) console.log(data);\n    });\nGet county level results for Primaries (to render county maps):\nquery.level('fipscode').officeID('P').raceType('D').raceType('R')\n  .fetch(function(err, data) {\n      if (!err) console.log(data);\n    });\nGet town level results for New England states (to render town maps):\nquery.level('ru').officeID('P').raceType('D').raceType('R')\n  .fetch(function(err, data) {\n      if (!err) console.log(data);\n    });\nGet delegate count for Primaries:\nDelegate count is reported only at district level for Presidential Primaries.\nquery.level('district').officeID('P').raceType('D').raceType('R')\n  .fetch(function(err, data) {\n      if (!err) console.log(data);\n    });\nGet county level results for Presidential General election (to render national maps):\nquery.level('fipscode').officeID('P').raceType('G').national(true)\n  .fetch(function(err, data) {\n      if (!err) console.log(data);\n    });\nStates where a winner yet to be called for Presidential General election:\nquery.level('state').officeID('P').raceType('G').national(true).winner('U')\n  .fetch(function(err, data) {\n      if (!err) console.log(data);\n    });\nU.S. House General elections - results for all districts:\nquery.level('district').officeID('H').raceType('G').national(true)\n  .fetch(function(err, data) {\n      if (!err) console.log(data);\n    });\nMore examples\nSee the examples directory for further sample scripts.\nNotes\nThis library was developed independently by The Wall Street Journal. It is not endorsed by Associated Press.\nVersion history\nv1.0.0 (2016-04-26)\n\nInitial public release\n\nLicense\nISC\n","79":"txlege84\nThe lege app to end all lege apps.\nRequirements\n\nDjango 1.7\nvirtualenv\/virtualenvwrapper\nNode\/npm\n\nGulp\n\n\nBower\nSass + Ruby\n\nGetting started\nPlease note \u2013 this guide assumes you are using OS X. If you aren't, you hopefully know the equivalent commands to make these things happen. If you don't, find someone to help you!\nFirst, clone the project.\ngit clone https:\/\/github.com\/texastribune\/txlege84.git\nThen, create the virtual environment for your project.\nmkvirtualenv txlege84-dev\nDon't forget to activate it!\nworkon txlege84-dev\nNext, install the Python development requirements.\npip install -r requirements\/local.txt\nOnce that finishes, run the initial migration, then try to run the project!\npython txlege84\/manage.py migrate\npython txlege84\/manage.py runserver\nYou should be able to check the site out at http:\/\/localhost:8000, but it's gonna look funky. Time to set up styling!\nFirst, we install all our node and bower dependencies.\nnpm install && bower install\nThis project uses gulp to run scss compiling and other various tasks.\nNow here's where the magic happens. Gulp uses BrowserSync to handle live reloading of the page during development. So when you're working on the front end, you'll need two terminal windows or tabs.\nIn the first, you'll run the Django runserver command.\npython txlege84\/manage.py runserver\nIn the second, you'll run the gulp serve command.\ngulp serve\nThis routes the Django's development server through BrowserSync's development server, so whenever anything changes with the templates or styles the page reloads automatically.\nWith both of those running, visit http:\/\/localhost:3000 to view the site.\nNow get to work!\nBootstrapping the data for development\nYou'll need a Sunlight Foundation API key to run these steps. Once you have it, you'll need to add it to your environment. There are a number of ways to do that, but the easiest way is:\nexport SUNLIGHT_API_KEY=<api-key-characters>\nNext, you need to prep your database.\npython txlege84\/manage.py migrate\nThen, run the make command to load the data.\nmake prep_for_development\n","80":"Donation Builder\nYou can also read the Donation Builder docs here.\nAbout the Project\nDonation Builder is a framework to help organizations showcase their work and seek new members and support for funding their projects. The primary audience for this project is nonprofit organizations with small teams who want a better and quick-to-set-up way to inform potential new members and supporters about their mission and work, as well as easily lead them through the donation process.\nWhy We Built It this Way\nWe chose Middleman for the project because we wanted a mostly static site. The Google Drive gem for Middleman provides the ability to easily update the content on the site without touching the code once it's set up, for those elements that are dynamic.\nFeatures\n\nFully responsive framework ready to be customized with an organization's visual brand, including images, colors, and fonts\nAbility to load information from Google Spreadsheets, so that non-technical members of your team can update membership levels and benefits, contact information, organization information, social share messages, content to highlight, and more\nEasily integrated with your Google Analytics account\nAbility to hook up to a variety of payment processors, including Dwolla, Givalike, and DonorPerfect, to accept the donations\nAbility to deploy through Amazon Web Services or Heroku\n\nSupport\n\nChrome, Firefox, Safari, IE9+\nUsers with JavaScript disabled\nTested with Browserstack across a variety of devices\n\nGetting Started\nTo get started, clone down the project repo.\nIf you don't already have Ruby installed, you need to install it. You need Ruby version 2.1.4, which is specified in the .ruby-version file. One way to manage your Ruby versions is to use rbenv.\nYou need the Ruby gem bundler. If you need to install the bundler, run:\ngem install bundler\n\nInstall the necessary gems from the Gemfile by running:\nbundle install\n\nDevelopment\nIf you've never worked with Middleman before, you can check out the docs here.\nMiddleman is configured to live reload the project in the browser as changes are made to files. To start up the Middleman server, run:\nbundle exec middleman\n\nTo build the site, run:\nbundle exec middleman build\n\nWhen Middleman builds, it creates a static file for each file located in the source folder. The build process is configured in config.rb.\nIntegrating with Google Drive\nDonation Builder uses the middleman-google_drive gem to integrate with Google Drive. Configure your Google Docs authentication as detailed in the Setup section.\nThe project is set up so that the site can be updated and customized through Google Spreadsheets. Use the same setup for your Google Spreadsheets to correctly pull in the info. You can view the spreadsheet setup here.\nAnalytics\nYou can hook up the project to your Google Analytics account to track how people are interacting with the site. In the Google Spreadsheet, you should have an Analytics tab. Look for the column labeled 'google_analytics_id', and enter your Google Analytics ID here. A second column labeled 'google_analytics_domain'  is where you should enter your domain.\nSee this screenshot.\nVisual Customization\nBrand Colors\nDonation Builder uses Sass to write its styles. In _settings.scss, there are some color variables. The project uses three theme colors, a main theme color, a secondary theme color, and a tertiary theme color. These colors are then applied to the buttons, navigation links, labels, and other UI elements throughout the project to quickly and easily use your organization's colors.\nTypography\nYou can customize the site with the fonts your organization already uses. This can also be done in the _settings.scss file. Look for the variables $main-font and $secondary-font. The default main font is 'Roboto' with a sans-serif fallback; the default secondary font is 'Open Sans Condensed', which has another sans-serif font as a fallback.\nImages\nTo switch out most images, you can replace an image in the \/source\/images folder that corresponds to the image you'd like to change. These images are named in an intuitive manner so that it's clear where each image appears in the project. The images and where they appear are described below in case the names are not clear. If you still aren't sure, try switching out the image and see where it appears in the project.\nLogo\nYour organization's logo appears in the masthead of each page on the site. The logo should be approximately 300px wide and 60px tall. In the source\/images\/ folder is an image named logo.png. Replace this image with your logo.\nFavicon\nYou may want to customize the favicon that appears on the browser tab. To do this, replace the image called favicon.ico with your favicon.\nBanner Backgrounds\nYou can customize the backgrounds used for the banners at the top of each page with your organization's photos. To set a background for the landing page banner, replace the photo named landing-header-bg.jpg. To set a background for the premium membership page banner, replace the photo named premium-membership-banner.jpg.\nOrganization Info\nIn the AboutOrg tab of the Google Spreadsheet, you can set up the summary for your organization, a short tagline, a call-to-action, your organization's name, and the url for your organization.\nContact Information\nContact information letting people know who to contact with questions can also be configured in the Google Spreadsheet, under the Contacts tab. You can set department, name, title, phone, and email for each contact.\nMembership Levels\nYou can set up your organization's membership level names, donation amounts and benefits in the Google Spreadsheet. You do this on the MemberLevels tab. See this screenshot.\nname\nThis field is your membership level name. For example, \"Activist\" or \"Enthusiast\".\ndonation_amount\nThis is the donation amount that people should give to be at the membership level. Add the amounts without dollar signs. For amounts over 999, include the comma (for example, 1,000).\nmost_popular\nSet this to TRUE for your most popular membership level, and leave this field blank for all other levels. This activates a \"Most Popular\" flag when this level is displayed on the landing page and on the membership grid page.\npremium_member\nSet this to TRUE for any premium membership levels that your organization offers. This activates a \"Premium Member\" flag when this level is displayed on the landing page and on the membership grid page. It also tells the project to feature these premium membership levels on the premium membership page.\nbenefits\nYou can list the benefits for each membership level here. Between each benefit, include <br> to put in a line break. You may also want to include \"Plus all above benefits\" as one of the benefits in this field.\nCustomizing Content\nOn both the landing page and the premium membership page, there are four spots provided for you to showcase either images or videos to inform people what their donations will go toward supporting. You set them in the Showcase tab. See this screenshot. To showcase images, enter 'images' in the 'content_type' column, and to showcase videos, enter 'videos' in the 'content_type' column.\nIf you selected images, upload the images of your choice and name them \"image-one.pg\" (This is the top featured image), \"image-two.png\", \"image-three.png\", and \"image-four.png\".\nIf you selected videos, upload the videos of your choice and name them \"video-one.pg\" (This is the top featured video), \"video-two.png\", \"video-three.png\", and \"video-four.png\". You'll also need to set the Youtube link for your videos in the Showcase tab under \"video_one\", \"video_two\", \"video_three\", and \"video_four\".\nLabels for the images or videos can be set in the spreadsheet with the columns \"label_one\", \"label_two\", \"label_three\", and \"label_four\".\nPage Information\nYou can set your page titles, headers, and urls in the Pages tab of the Google Spreadsheet.\nNavigation Links\nYou can add more navigation links to the navbar as needed. These links are included in both the desktop nav and the mobile nav. For example, you might want to add a \"Quick Donation\" link or a link to more information about your organization. Additional nav links can be set up in the NavLinks tab. For each additional nav link, add the URL target and the link text to display. It's recommended that you add only up to three additional nav links so you don't overwhelm the page.\nFooter Links\nYou can set up the links that should appear in the footer in the FooterLinks tab. For each link you'd like to include in the footer, add the URL and the link text to display. Note that the project is configured to hide the footer by default if the device width is 985px or greater. You can change this in the _footer.scss file if you prefer a different behavior here.\nSocial Sharing\nYou can customize the default messages for when people choose to email, tweet or post to Facebook about donating to your organization. You configure this in Google Spreadsheets on the Social tab. See this screenshot. The Facebook fields set the Open Graph metadata for your site, to ensure Facebook scrapes and displays the information you want when people post.\nTo set your Facebook share image, you also need to include the image you'd like to use there in the \/source\/images folder named facebook-share-image.png.\nIntegrating with a Payment Processor\nThe project is designed to be able to be integrated with the payment processor that works best for you. It's set up to integrate most fluidly through linking to a form where people can then enter their information and submit their contribution. Some examples of payment processors with which it works well include Givalike, Dwolla and DonorPerfect.\nYou can set up the links where people should be sent to enter their information and submit their contribution in the DonationInfo tab. See this screenshot. You can set different URLs here for generic contributions and for renewals.\nPossible Customizations\nComing soon!\nDeploying\nAmazon Web Services\nTo deploy to AWS, the project uses the middleman-s3_sync gem to push and compile the site to Amazon S3 after building.\nIn the config.rb file, look for \"activate :s3_sync do |config|\". Set config.bucket to your AWS bucket. Set config.region to your AWS bucket region. Set your AWS_ACCESS_KEY and your AWS_ACCESS_SECRET as environment variables.\nBy default, in config.rb, config.after_build is set to false. When you're ready to deploy, set this to true. Check that your AWS_ACCESS_KEY and AWS_ACCESS_SECRET environment variables are set, and then run:\nbundle exec middleman build\n\nIn your terminal, you should see s3_sync applying any updates to the files in the project. You can also check the project S3 bucket to ensure that all files have been synced there. Change config.after_build back to its default of false after deploying.\nHeroku\nIf you've never used Heroku before, you'll need to create a Heroku account.\nFor Heroku, you'll need Ruby 2.1.4.\nThe first time you deploy, create your project for Heroku:\nheroku create\n\nBuild the project locally and commit the updates:\nbundle exec middleman build\n\nHeroku is set up in config.ru to use the build directory.\nPush the project to Heroku:\ngit push heroku master\n\nTo see your deployed project:\nheroku open\n\nRsync, FTP, SFTP and Git\nYou can use a middleman-deploy gem to deploy the project over Rsync, FTP, SFTP or Git.\nExamples\nThe Texas Tribune uses this project for its donations site. You can see it live here and visit the repo here. This should give you a good idea of what's possible with the project and what additional customizations you can add to the project so that it fits your particular needs.\nLook at the example project on Heroku here.\nContributors\n\nKathryn Beaty - kbeaty@texastribune.org\n\nThank you to Masonry, the digital creative agency we worked with to reach the design for this project.\nPlease feel free to reach out with any questions or comments. Thanks!\n","81":"Scrollyteller\nA scrollyteller component for React\nUsage\nThe scrollyteller takes a series of panels of content nodes and turns them into a series of elements which scroll over the <Scrollyteller> component's children.\nThe panels prop is in the format of:\n[\n  {\n    data: {\n      info: 'Some kind of config that is given when this marker is active'\n    },\n    nodes: [<DOM elements for this panel>]\n  },\n  {\n    data: {\n      thing: 'This will be given when the second marker is hit'\n    },\n    nodes: [<DOM elements for this panel>]\n  }\n]\n\nWhen a new box comes into view onMarker will be called with the data of the incoming panel.\nimport * as React from 'react';\nimport Scrollyteller from '@abcnews\/scrollyteller';\n\n\/\/ Some kind of dockable visualisation that goes with the scrolling text\nimport GraphicOfSomeKind from '.\/GraphicOfSomeKind';\n\nexport default () => {\n  const [something, setSomething] = React.useState('');\n  const [progressPct, setProgresPct] = React.useState('');\n\n  \/\/ Content is loaded somehow into an array of { data: {...}, nodes: [...DOMNodes] }\n  const panels = ...?\n\n  return (\n    <Scrollyteller\n      panels={panels}\n      onMarker={({thing}) => setSomething(data.thing)}\n      onProgress={({pctAboveFold}) => setProgress(pctAboveFold)}>\n      <GraphicOfSomeKind property={something} \/>\n    <\/Scrollyteller>\n  );\n}\nFor a more complete example using Typescript see the vanilla example app.\nCustomising\nThe Scrollyteller can take a panelClassName prop which it will pass to each panel component for customising the look.\nIt can also take firstPanelClassName and lastPanelClassName props which it will pass to the respective panel components, for customising their specific looks.\nTo completely customise how panels are rendered you can pass in panelComponent. The main requirement for this component is that it calls props.reference with a ref to its outermost wrapper.\nimport * as React from 'react';\n\ninterface Props {\n  nodes: HTMLElement[];\n  reference: (el: HTMLElement) => void;\n}\n\nexport default (({nodes, reference}): Props) => {\n  const base = React.useRef(null);\n  const innerBase = React.useRef(null);\n\n  React.useEffect(() => {\n    reference(base.current);\n    nodes.forEach((node: HTMLElement) => {\n      innerBase.current.appendChild(node);\n    });\n  }, [reference]);\n\n  return (\n    <div ref={base} style={{ zIndex: 1, height: '80vh', fontSize: '40px' }}>\n      <strong>THIS IS A PANEL:<\/strong>\n      <div ref={innerBase} \/>\n    <\/div>\n  );\n};\nAnd then specify <Scrollyteller panelComponent={CustomPanel}>.\nUsage with Odyssey\nWhen developing ABC News stories with Odyssey you can use the loadScrollyteller function to gather panels within a CoreMedia article.\nSee a more complete usage example with Odyssey in the example project.\nCoreMedia text:\n#scrollytellerVARIABLEvalue\nThis is the opening paragraph panel\n#markVARIABLEvalue\nThis is a second panel\n#markVARval\nThis is another paragraph\n#endscrollyteller\n\nJS Code:\nimport Scrollyteller, { loadScrollyteller } from '@abcnews\/scrollyteller';\n\nconst scrollyData = loadScrollyteller(\n  \"\",       \/\/ If set to eg. \"one\" use #scrollytellerNAMEone in CoreMedia\n  \"u-full\", \/\/ Class to apply to mount point u-full makes it full width in Odyssey\n  \"mark\"    \/\/ Name of marker in CoreMedia eg. for \"point\" use #point default: #mark\n);\n\n\/\/ Then pass them to the Scrollyteller component\nReactDOM.render(\n  <Scrollyteller panels={scrollyData.panels} {...scrollyData.config} \/>,\n  scrollyData.mountNode\n);\n\n\/\/ You could also use React Portals to mount on the mount node\nReactDOM.createPortal(\n  <Scrollyteller ... \/>,\n  scrollyData.mountNode\n);\nDevelopment\nThis project uses tsdx for build\/dev tooling and np for release management.\nThe recommended workflow is to run TSDX in one terminal:\nnpm start\nThis builds to \/dist and runs the project in watch mode so any edits you save inside src causes a rebuild to \/dist.\nThen run the example inside another:\ncd example\nnpm i\naunty serve\nThe example imports and live reloads whatever is in \/dist, so if you are seeing an out of date component, make sure TSDX is running in watch mode as recommended above.\nTo do a one-off build, use npm run build.\nTo run tests, use npm test or yarn test.\nExample\nMostly to aid development and demonstrate usage, there is an example project in \/example. It uses aunty as the build tool to match the usual ABC News interactive development work flow.\nJest\nJest tests are set up to run with npm test or yarn test.\nBundle analysis\nCalculates the real cost of your library using size-limit with npm run size and visulize it with npm run analyse.\nReleasing\nTo release a new version to NPM run npm run release and follow the prompts.\nJest\nJest tests are set up to run with npm test or yarn test.\nBundle analysis\nCalculates the real cost of your library using size-limit with npm run size and visulize it with npm run analyze.\nOptimizations\nPlease see the main tsdx optimizations docs. In particular, know that you can take advantage of development-only optimizations:\n\/\/ .\/types\/index.d.ts\ndeclare var __DEV__: boolean;\n\n\/\/ inside your code...\nif (__DEV__) {\n  console.log('foo');\n}\nAuthors\n\nNathan Hoad (nathan@nathanhoad.net)\nSimon Elvery (elvery.simon@abc.net.au)\nJoshua Byrd (byrd.joshua@abc.net.au)\nColin Gourlay (gourlay.colin@abc.net.au)\n\nSee the full list of contributors.\n","82":"Odyssey Scrollyteller\nThis module integrates with Odyssey to implement 'scrollytelling' sections in Odyssey based stories.\nTo use this, include it in the story along with the implementation of the visual element.\nSome text before the scrollytelling portion of the story starts.\n#scrollyteller\n[link to your interactive here]\nA first paragraph of the scrollytelling experience.\n#markIDone\nA second section. These sections can be broken into multiple paragraphs and are divided by the '#mark' anchors.\nThe '#mark' anchors also act as waypoints for which Odyssey Scrollyteller will create events.\n#markIDtwo\nWhen the start of each section scrolls past the bottom of the viewport, a new event is fired on the background element (which can then bubble up the DOM, so you can listen for it anywhere).\n#endscrollyteller\nYou can then carry on your story after the scrollytelling section.\n\nUsage\nTo hook into the events, use some variation on this code:\n\/\/ Initialise\nconst stage = document.querySelector('[selector-for-your-interactive] .scrollyteller-stage');\n\nif (stage) {\n  init({\n    target: stage,\n    detail: stage.__SCROLLYTELLER__\n  });\n} else {\n  \/\/ console.log('waiting for the stage');\n  document.addEventListener('mark', init);\n}\n\nfunction init(ev) {\n    console.log(ev.target); \/\/ the stage element\n    console.log(ev.detail); \/\/ the `activated` and `deactivated` marks (if any)\n}\nConfiguration options\nThe opening #scrollyteller tag takes some options which are specified using an alternating case syntax. For example, the opening tag #scrollytellerHELLOworldMEANING42 will result in a config object which looks like:\n{\n    hello: \"world\",\n    meaning: 42\n}\nThe config options available are:\n\nALIGN (left|right): Align the content to the left or right when the screen is wide enough. Defaults to centre alignment.\nWAYPOINT (integer between 0 and 100): Defines where on the viewport (% distance from top) #mark events are triggered. Defaults to 80.\n\nThere is also an option available on individual #mark tags.\n\nPIECEMEAL (true): Sets all elements between this mark and the next to get their own visual container instead of being grouped together (the default behaviour).\n\nAuthors\n\nNathan Hoad (nathan@nathanhoad.net)\nSimon Elvery (simon@elvery.net)\n\n","83":"Data Life\nHere I am man-in-the-middling myself and analysing the results.\nThis repository contains some of the code written to record and analyse HTTP requests from my phone and laptop as part of a project for ABC News.\nI wanted to find out in great detail what kinds of data my devices were sharing about me without my knowledge.\nThe project uses mitmproxy to do most of the heavy lifting.\nserver\nThe server folder contains code that packages the setup into Docker images. Doing it this way is mostly for reproducibility and so I don't have to deal with python dependencies locally. Blowing it all away and starting again should be simple.\nCheck out server\/README.md for more details.\nanalysis\nThe analysis folder contains (mostly) R code for analysing the data recorded by the proxy.\nCheck out analysis\/README.md for more details.\nscripts\nThe scripts folder is mostly a collection of bash scripts I've used to get the data out of the docker containers. They probably won't just run. They're mostly there so I don't have to remember all the individual commands I ran.\nCheck out scripts\/README.md for more details.\n","84":"scrolly-story\nA project generated from aunty's basic-story template.\nHot Reload\nHot reload is enabled by default on the development server. Your 'app' should be separated into the src\/index.js loader\nand the actual app in src\/components\/app.js.\nIf you want to see how hot reload is set up, have a look in src\/index.js and you'll see something like this:\nif (process.env.NODE_ENV !== 'production' && module.hot) {\n    let renderFunction = render;\n    render = () => {\n        try {\n            renderFunction();\n        } catch (e) {\n            const { Error } = require('.\/error');\n            root = Preact.render(<Error error={e} \/>, element, root);\n        }\n    };\n\n    module.hot.accept('.\/components\/app', () => {\n        setTimeout(render);\n    });\n}\nThis just means that when NODE_ENV is 'development' the app will always be listening for changes and when it detects\na new build (a change to .\/components\/app or its dependencies) it will automatically require in the new code.\nIf there was an error when compiling you will see an error box instead of your app. Once you fix the error it will vanish\nand your app will be back.\nUsing React components with Preact\nThis template comes with preact-compat so any React components should\nJust Work\u2122.\nAuthors\n\nNathan Hoad (nathan@nathanhoad.net)\n\n","85":"Chartbuilder Electron\n\nA desktop version of Quartz's Chartbuilder app.\n\nDownload\nDownload the latest on the Releases page. If you're on OS X, select the Darwin release. Because Chartbuilder isn't on npm, it's currently pegged to the latest commit so we can keep track of what version this was built with.\nWhy\nIn thinking about newsroom adoption, sometimes simply having something in the dock is so much easier than navigating to a (possibly ugly internal) URL. The dock is the new bookmark.\n\nThings to do\nSee the issue tracker for more detail. Help welcome!\n\nSet up auto-updating with electron-gh-releases and command-line releases with publish-release.\nFigure out a way to inject the necessary menu JavaScript into the Chartbuilder JS environment per docs without altering that repo.\nSet default download location to something other than ~\/Library\/Application Support\/Chartbuilder\/Downloads\/.\n\nUsing your customized Chartbuilder build\nIf your build is on GitHub and the install and build processes haven't deviated from Chartbuilder 2.0, then fork this repository and replace the dependency information in package.json near line 31 with your own info.\nFor example, if my fork of Chartbuilder lives at http:\/\/github.com\/mhkeller\/Chartbuilder, the package.json line would look like:\n  \"dependencies\": {\n    \"chartbuilder\": \"mhkeller\/Chartbuilder\",\n    \"electron-debug\": \"^0.1.1\"\n  },\nThen, follow the Developing instructions below to create your build.\nNote on versioning: You can also add a commit sha preceeded by a # to better keep track of which version of that repository we are pulling, but that is optional. Without a specific commit or branch name, it will pull the latest at the time of building. You can also publish your whitelabelled Chartbuilder to npm and call it with a version number, but that isn't usually desirable since your white-labelled version isn't meant for public distribution.\nFor more information on npm dependencies as Git Urls, check out the npm docs.\nDeveloping\nTo install everything\nnpm run init\nTo run the app locally\nnpm start\nTo build out the binaries\nnpm run build\nBuilds the app for OS X, Linux, and Windows, using electron-packager.\nLicense\nMIT \u00a9 mhkeller\n","86":"scrollWatcher \ud83d\udcdc\ud83d\udc40\nA small JavaScript library for scroll-based data-driven graphics (without any nasty scrolljacking). scrollWatcher sticks an element at a fixed position onscreen, and then provides the distance scrolled through its (much taller) parent as a percentage.\n\nDemo here\n\nAs seen on WSJ.com in How Fed Rates Move Markets and What ECB Stimulus Has Done.\nIs scrollWatcher the right library for you?\n\nWant something to always stick on the page? Use CSS position: fixed;.\nWant something to stick when you scroll past it? Use jQuery fixto plugin or jQuery Sticky.\nWant a sticky header that hides on scroll? Use headroom.js.\nWant to trigger events on scroll? Use Waypoints.\nIf you want to use scroll as a way of interacting with a data-driven graphic without scrolljacking, use scrollWatcher.\n\nQuickstart\n\n\nInclude jQuery and the sticky-positioning fixto plugin on the page.\n\n\nIn your HTML, you'll need a tall outer element, with one much shorter element inside of it.\n<div class=\"outer\" style=\"height: 2000px;\">\n    <div class=\"inner\"><\/div>\n<\/div>\n\n\nIn your JavaScript, you'll need to call scrollWatcher with a configuration object using parent and onUpdate arguments. The function passed into onUpdate will run every 20 miliseconds.\nscrollWatcher({\n    parent: '.outer',\n    onUpdate: function( scrollPercent, parentElement ){\n        $('.inner').text('Scrolled '+scrollPercent+'% through the parent.');\n    }\n});\n\n\nExamples\nCheck out the source code to see how these are used.\n\nBasic demo\nUsing a D3 scale\nUsing a D3 time scale\nChecking if the user has got there yet\n\nOther features\nStarting and stopping\nIf you create a new instance of scrollWatcher:\nvar myWatcher = scrollWatcher({\n    onUpdate: function( scrollPercent, parentElement ){\n        console.log('Scrolled '+scrollPercent+'% through the parent.');\n    },\n    parent: '.outer'\n});\n... you can then start, pause and stop at any time.\n\/\/ stop checking but keep stuck\nmyWatcher.pause();\n\n\/\/ stop checking and unstick\nmyWatcher.stop();\n\n\/\/ start checking and restick\nmyWatcher.start();\nCheck if active\nThere are two read-only properties:\n\n\nactive is true when the scrollWatcher instance is currently onscreen (and running).\nvar isActive = myWatcher.active;\n\n\nhasBeenActive is true when the scrollWatcher instance has been onscreen (and run) at least once.\nvar isOrWasActive = myWatcher.hasBeenActive;\n\n\nYou may want to use these to (for example) hide a \"keep scrolling!\" message.\nCheck for device support\nUse scrollWatcher.supported() to check whether or not scrollWatcher will work in the current browser.\nBrowser support\nscrollWatcher works on all modern browsers, including IE 9 and up. However, it does not work on iOS 7 and lower due to the way Safari handles CSS position: fixed;.\nTesting\nTo run tests, open tests.html in your browser and wait a couple of seconds.\nVersion history\nv1.0.0 (April 19, 2016)\n\nInitial public release\n\nLicense\nISC\n","87":"TwoStep\nThis is a JavaScript library for scrollytelling, which is dynamically changing charts (or triggering whatever) as text scrolls into view. It implements best practices for scrollytelling, which means built-in keyboard shortcuts, no scrolljacking and reliable \"sticky\" behaviour.\nTwoStep was developed at The Wall Street Journal and has been used in stories such as:\n\nHow This Tech Rally Is Different From 1999\nThen and Now: The Big Shift at Work\n\nDemos\nTwoStep is highly flexible, and can be used in range of designs.\n\nBasic working example\nOne-column layout, with text scrolling over the top\nTwo instances on the same page\nWith swipe carousel on mobile\nWith static charts on mobile\n\nThese demos are also meant as starting points for new projects.\nSetup\n\n\nInclude jQuery, Waypoints, FixTo and TwoStep on the page.\n\n\nSet up HTML and CSS as best fits your project. See source code from the demos above for inspiration.\n\n\nIn your JavaScript:\n\n\nvar ts = new TwoStep({\n    elements: document.querySelectorAll('.slide-item'),\n    onChange: function(event) {\n        console.log(event.index);\n    }\n});\nOptions\nOnly elements is required. All others are optional.\n\nelements: Array\/NodeList of DOM nodes (i.e. your narrative text).\nnarrative: Array of functions corresponding to elements. Each called with event object as argument.\nonChange: Called when any\/every element is activated. Called with event object as argument.\nstick: DOM node to stick in right rail (i.e. your sticky chart).\noffset: Object to manually set the offset for both directions. Must include both an up and down key and both values should be strings (i.e. {up:\"50%\",down:\"0\"})\n\nEvent object\nWhenever a narrative function or onChange is called, it\u2019s passed an event object as an argument.\nvar ts = new TwoStep({\n    elements: document.querySelectorAll('.item'),\n    onChange: function(event) {\n        console.log(event);\n    }\n});\nHere\u2019s what you can find in a typical event object:\n{\n    index: 0,\n    direction: 'up',     \/\/ or 'down', or null\n    element: < element > \/\/ DOM node corresponding to index\n}\nHow to get scroll direction\nCheck event.direction:\nvar ts = new TwoStep({\n    elements: document.querySelectorAll('.item'),\n    onChange: function(event) {\n        if (event.direction === 'up') {\n            \/\/ do something\n        } else if (event.direction === 'down') {\n            \/\/ do something else\n        }\n    }\n});\nPublic methods\n\n.goTo(index, scroll): Activate item at index. If scroll is true, will animate to position. Returns a promise which resolves when scrolling is complete.\n.disable(): Prevent waypoints from firing and unstick stuck element, if present\n.enable(): Return to standard behaviour\n\nCompiling from scratch\nMake sure you have Node.js installed. Then run:\nnpm install\nnpm run start\n\nRunning tests\nTo run tests, open tests.html in your browser and wait a couple of seconds.\nAlternatives to TwoStep\n\nGraph Scroll\nScrollama\nThe Pudding\u2019s article How to implement scrollytelling with six different libraries\n\nFor something a bit different, see scrollWatcher, also made by WSJ.\nVersion history\nv1.0.0 (November 27, 2017)\n\nInitial public release\n\nLicense\nISC\n","88":"Faces of Death Row\nFaces of Death Row was produced by Jolie McCullough for The Texas Tribune. It is a visualization of all inmates currently on death row in Texas filterable by length of stay, race, age, sex, county and execution date. It could easily be modified to show other filterable data.\nThis app was built on the Texas Tribune's Data Visuals kit. It is assisted by the libraries filter.js and chosen.js.\nData\nThe data in this app was originally collected from the Texas Department of Criminal Justice (TDCJ) in April 2015 via an open records request. The conviction summaries are gathered from TDCJ records, court documents and news articles and summarized by the Texas Tribune. The data is regularly updated by Tribune staff.\nUpdate Process\nPlease note - some static assets required to make this project work are only accessible to Texas Tribune developers.\nClone the project, then run npm install. Then pull down the assets with npm run assets:pull, and the data with npm run data:fetch. Use npm run serve to run the local development server.\nThe data is stored in a Google spreadsheet which can be edited with anyone who has a Tribune email address. Here is a public version. To remove or add an inmate, simply edit the spreadsheet.\n","89":"This is a fork of a New York times package. See below for details of modifications made.\ntext-balancer\nText-balancer is a javascript module that seeks to eliminate typographic widows from text. It does this by setting the max-width of the dom node to the threshold that it would spill onto another line.\nSetup instructions\nInstall it into your project via npm\nnpm install text-balancer --save\nhttps:\/\/www.npmjs.com\/package\/text-balancer\nWe use bramstein's Font Face Observer to check when our fonts have loaded: https:\/\/github.com\/bramstein\/fontfaceobserver\nWe run our text-balancer once our fonts load.\nHow to run it\nimport textBalancer from 'text-balancer';\n\n\/\/ Run it when you want to with any set of selectors\ntextBalancer.balanceText('.headline, .interactive-leadin, #horse-god');\n\n\/\/ OR: Just run it and it will look for anything with the balance-text class\ntextBalancer.balanceText();\nIf you're not running node\/npm\nInclude text-balancer.standalone.js wherever you load your js files\nand then:\nPUT THIS SCRIPT AFTER YOUR MARKUP IDEALLY UNDER THE BODY TAG\nOR JUST WRAP IT IN SOME SORT OF AN ONLOAD EVENT :)\n<script>\n\/\/ Run it when you want to with any set of selectors\ntextBalancer.initialize('.headline, .interactive-leadin, #horse-god');\n\n\/\/ OR: Just run it and it will look for anything with the balance-text class\ntextBalancer.balanceText();\n<\/script>\n\nTexas Tribune changes\nPer license requirements, you can see a list of Texas Tribune modifications by viewing this repository's commit history on GitHub. Additionally, there are comments atop changed files where possible. We have modified\/created the following files:\n\npackage.json\nyarn.lock\n.gitignore\nREADME.md\ntext-balancer.js\n\n","90":"Squaire.js\nThis is not a map. This is a table with strong visual cues.\n\n\nQuick start\nMethods\nData\nOptions (tooltips, breakpoints, layout, labels, colors)\nAdvanced options (other layouts, active box)\nPhilosophy (alternatives, related reading)\nContributing\n\nAbout\nSquaire (pronounced square) is a javascript library to make responsive \"maps\" of equal-area squares.\n\nSquaire requires d3.js and includes a modified version of d3-tip for tooltips.\nUse SVG Crowbar to download the SVG and edit the chart in a vector application such as Adobe Illustrator.\nThe default layout and data labels are for the United States, but the library can be used to make any grid of squares with data, including countries in Europe, legislative districts in California and soccer starting lineup.\nFamiliarity with d3.js will help in using this plugin, however the docs and demo\/ include code samples for the small amount of d3 required to make a simple map.\n\nQuick start\nDownload the files in dist to your project directory and include d3.js, squaire.js (after d3.js) and squaire.css.\n<link rel=\"stylesheet\" href=\"squaire.css\"\/>\n<script src=\"https:\/\/cdnjs.cloudflare.com\/ajax\/libs\/d3\/3.5.6\/d3.min.js\" charset=\"utf-8\"><\/script>\n<script src=\"squaire.js\"><\/script>\n\n<div id=\"map-container\"><\/div>\n\n<script type=\"text\/javascript\">\n\tvar data = { \n\t\t\"NY\" : { \"value\": \"$4\" },\n\t\t\"AL\" : { \"value\": \"$1\" }\n\t};\n\tvar map = new Squaire(data, {\n\t\tcolors: d3.scale.quantize().domain([1,5]).range(['#c9e2f5','#0098db'])\n\t});\n<\/script>\nMethods\nConstructor\nvar map = new Squaire(data, options);\n\nNo options are required to work, but you will want to set colors to see the map colored by data and index if data doesn't include a column named \"value\". Element selectors for the map container and static tooltip (if used) need to exist in the DOM already.\nUpdate\nCall update(data, options) on a Squaire element to update the data, layout, data column used to color the boxes, the color scale and change the data displayed in the tooltip. Update can't enable or disable the tooltip, change the tooltip mode or either element.\nIf you only want to update options and not the data, pass false in place of data.\nvar map = new Squaire(data, options);\n\nmap.update(false, {\n\tindex: index,\n\tcolors: d3.scale.quantize().domain([1,5]).range(['#fdc47c','#d991b2'])\n\twhitelist: [index]\n});\nData\nSquaire takes a JSON object where each property name points to data for a box of the map. The property name maps to the ids in layout and labels in options. Squaire defaults to U.S. states and two-letter abbreviations for the id.\nThe value of each property is an object with property strings equivalent to column headers. Any column may be used to assign a class to boxes or a note to the tooltip.\nSquaire.update may be used to update data directly, but if you only wish to change the data in view and coloring the boxes it is best practice to pass all the data to the constructor and use the index and tooltip options. The data object extends the layout so missing properties and data columns are handled properly.\nValue properties are used as labels in the tooltip and thus should be formatted for display. Values should also be formatted for display, e.g., $5, -6.2%, 16 lbs. When indexType=\"numeric\" (default), Squaire strips symbols of non-numerical symbols as necessary using d.replace(\/[^\\d-\\.]\/gi,'').\nData from a spreadsheet can be processed using Mr. Data Converter (output as JSON - Dictionary) or formatted from a tsv or csv file using d3.js parsing.\nvar data = {}, map, options;\n\nd3.csv(\"path\/to\/file.csv\", function(r){\n\t\/\/ write values to object using id as proprety name\n\t\/\/ id is the column name in the spreadsheet that maps to the layout and labels -- U.S. state two-letter abbreviations in default squaire.js settings\n\tdata[r.id] = r;\n\t\/\/remove id from dictionary values\n\tdelete data[r.id].id; \n\treturn r;\n}, function(csv) {\n\t\/\/callback when file loaded and data formatted\n\tmap = new Squaire(data, options);\n});\nd3.csv and d3.tsv methods are interchangeable. They accept a url and optional accessor and callback functions (d3.csv(url[[, accessor], callback])). The accessor formats each row of data before calling the callback signaling the data is loaded and formatted. The code above uses the accessor to write the array properties to a dictionary object defined previously instead.\nOptions\nSetting options for constructor.\n\n\n\nproperty\ntype\ndefault\ndescription\n\n\n\n\nel\nstring\n\"#map-container\"\nCSS selector for map parent element.\n\n\nlayout\narray or string\nU.S. state layout\nLayout of boxes on the map. Array in the form of aRanger or tsv or csv string. See layout\n\n\nlabels\nobject\nU.S. state full, AP style and two-letter\nObject of label formats for boxes. See labels\n\n\nlabelStyle\nstring\n\"short\"\nLarge breakpoint label format for boxes defined in labels. Required options are \"full\" (Alabama) and \"short\" (AL). Default layout and labels includes \"ap\" too. See labels\n\n\nindex\nstring\n\"value\"\nName of column in data used to color boxes.\n\n\nindexType\nstring\n\"numeric\"\nHow data coloring boxes should be treated. \"numeric\" for linear and quantitative scales or \"string\" for ordinal scales. For \"numeric,\" type of data may still be strings with unit labels.\n\n\ncolors\nfunction\nd3.scale.ordinal().range(['#f3f3f3'])\nd3.js color scale for boxes. See colors\n\n\nclassIndex\nstring\nfalse\nOptional name of column with values to be assigned as classes to each box.\n\n\ndefaultColor\nstring\n\"#f3f3f3\"\nColor for boxes with undefined data.\n\n\ntooltip\nobject\n{...}\nTooltip options. See tooltips\n\n\nbreakpoints\nobject\n{...}\nBreakpoints. See breakpoints\n\n\n\nTooltips\n\n\n\nproperty\ntype\ndefault\ndescription\n\n\n\n\nenabled\nboolean\nfalse\nSet to true to enable tooltips.\n\n\nmode\nstring\n\"dynamic\"\n\"dynamic\" (tooltip next to box on hover), \"static\" (tooltip data in static HTML element outside map), \"toggle\" (\"dynamic\" above small 540px breakpoint, \"static\" below)\n\n\nel\nstring\n\".squaire-toolbox\"\nCSS selector for static tooltip (used with \"static\" and \"toggle\" modes)\n\n\nlayout\nfunction\nfalse (default layout in constructor)\nfunction (d) {...} to write HTML to tooltip.\n\n\nwhitelist\narray or string\n\"*\"\nArray of column names to show in tooltip. Default \"*\" selects all columns.\n\n\ncolumn1\nstring\n\"\"\nLabel for property column in tooltip table of data.\n\n\ncolumn2\nstring\n\"\"\nLabel for value column in tooltip table of data.\n\n\nnoteIndex\nstring\nfalse\nName of column with additional text to append at the bottom of tooltip (e.g., footnote). Text may be styled via .tooltip-note.\n\n\n\nBreakpoints\nA Squaire instance is responsive to its container (el). The breakpoint names are applied to the square container via a data-breakpoint attribute.\nsmall is the primary breakpoint where labels change to short and the tooltip toggles between static and dynamic. It is the only breakpoint used in squaire.js beyond assigning the attribute to the container.\n\n\n\nproperty\ntype\ndefault\ndescription\n\n\n\n\n\"small-thumbnail\"\nint\n270\nReferenced by CSS (labels are hidden, possible use for thumbnails or icon). Considered small for breakpoints referenced in squaire.js.\n\n\n\"small-xsmall\"\nint\n350\nReferenced by CSS (smaller labels). Considered small for breakpoints referenced in squaire.js.\n\n\n\"small\"\nint\n540\nPrimary breakpoint where labels change to short and the tooltip toggles between static and dynamic.\n\n\n\"medium\"\nint\n940\nReferenced by CSS (larger labels).\n\n\n\"large\"\nint\n\"\"\nReferenced by CSS.\n\n\n\nLayout\nThe layout of the boxes can be in one of two formats. You will probably want to edit the breakpoints if you change the layout or labels.\nCode for a custom layout and labels.\nArray (best for U.S. states)\nMake a custom layout using MinnPost's aRanger to move around the boxes. The tool is preloaded with data for U.S. states and makes the array.\nEach box in the array is a triplet, [x,y,id] of location data and the box id (string, e.g., \"NY\") on a chart where the top-left corner is [0,0] and each box is 1.\nTSV or CSV\nUse a spreadsheet to make the layout where each cell starting at A1 is a box in a Squaire map. Enter the box label, e.g., NY, in the cell. The resulting tab-separated or comma-separated data can be used as the layout.\nSee other layouts for more information on non-U.S. state maps and an example.\nLayout format and parser from Squared.\nCustom format\nOverwrite Squaire.prototype.prepareData(layout, data) to process a custom layout format and return an array with an object for each box in the following format:\n[{\n\tx: 8, \/\/horizontal position of box (0 is the left edge)\n\ty: 2, \/\/vertical position of box (0 is the top edge)\n\tbox: 'NY', \/\/string connecting box and label. U.S. state two-letter abbreviations in default settings.\n\tdata: {...} \/\/data for the box\n}]\nLabels\nObject defining label formats for Squaire boxes. Properties of the object map to properties in data and layout.\nRequired format types are \"full\" (used in default tooltip) and \"short\" (used for the small breakpoint). Default label object includes AP style \"ap\" for U.S. states and is only used if labelStyle is set to \"ap\".\n\"AL\" : {\n\t\"full\"  : \"Alabama\", \/\/used in tooltip (required)\n\t\"short\" : \"AL\", \/\/used in small breakpoint (required)\n\t\"ap\"    : \"Ala.\" \/\/optional label format included in default labels and recommended for `labelStyle` for U.S. states. May be excluded or renamed as the only reference to the format would be from the user-defined \"labelStyle\" option.\n}\nCode for a custom layout and labels.\nColors\nColor scales are complicated. That's why Squaire leaves it to the professionals, a.k.a., d3.js. The range is an array of color hex strings.\nIf you're not fluent in d3 color scales, below are some examples to get you started. Use quantitative scales for numeric data (values may include labels) and ordinal for categorical data where a string should map to a color.\nIf you are testing a quantitative scale, use scale.invertExtent('range value') to see the range of domain values that map to the range. In the return array, the max value is exclusive, e.g., [0,1] means 0 and .99 return the given range value, but 1 does not.\nQuantitative (numbers)\nQuantize\nSet indexType: \"numeric\". Range values should be hex strings for Squaire. Color names used for example only.\nDivide the domain of data values (given by a max and min) into equal buckets where each color represents the same size range of values.\n\/\/quantize divides [min,max] of domain by length of range to make buckets\nvar quantize = d3.scale.quantize()\n\t.domain([0,100])\n\t.range([\"yellow\",\"orange\",\"red\"]);\n\nconsole.log( quantize(0), quantize(33), quantize(34), quantize(90) );\n\/\/output: yellow yellow orange red\nquantize.invertExtent(\"yellow\")\n\/\/output: [0, 33.333333333333336]\nQuantile\nSet indexType: \"numeric\". Range values should be hex strings for Squaire. Color names used for example only.\nDivide the full array of data values into equal-length buckets where each color maps to the same number of values.\n\/\/quantile groups values into equal-length buckets, e.g., smallest third, middle third, largest third\nvar quantile = d3.scale.quantile()\n\t.domain([0,1,5,50,51,100])\n\t.range([\"yellow\",\"orange\",\"red\"]);\n\nconsole.log( quantile(0), quantile(1), quantile(5), quantile(50), quantile(51) );\n\/\/output: yellow yellow orange orange red\nquantile.invertExtent(\"yellow\")\n\/\/output: [0, 3.666666666666666]\nquantile.invertExtent(\"red\")\n\/\/output: [50.333333333333336, 100]\nThreshold\nSet indexType: \"numeric\". Range values should be hex strings for Squaire. Color names used for example only.\nDivide the domain of data values into custom irregular buckets where each color represents a custom size range of values.\n\/\/threshold converts domain values into buckets where the max value is excluded.\nvar threshold = d3.scale.threshold()\n\t.domain([0,1,5,50])\n\t.range([\"less than 0\",\"yellow\",\"orange\",\"red\"]);\n\nconsole.log( threshold(0), threshold(1), threshold(5), threshold(49), threshold(50) );\n\/\/output: yellow orange red red undefined\nthreshold.invertExtent(\"yellow\")\n\/\/output: [0, 1]\nthreshold.invertExtent(\"red\")\n\/\/output: [5, 50]\nOrdinal (categories)\nSet indexType: \"string\". Range values should be hex strings for Squaire. Color names used for example only.\nMap a string domain value directly to a range color.\nvar ordinal = d3.scale.ordinal()\n    .domain([\"small\", \"medium\", \"large\"])\n    .range([\"yellow\",\"orange\",\"red\"]);\n\nconsole.log( ordinal(\"small\") );\n\/\/output: yellow\nAdvanced options\nAfter loading squaire.js, you can overwrite methods for special use cases. Browse the source code to see all the methods, e.g., Squaire.prototype.mapInteraction() where events are bound to the boxes.\nOther layouts\nTo make a map or any graphic with a box grid other than U.S. states, pass a new layout array or string and labels object. You will likely want to edit the breakpoints too.\nEach box needs a unique string to connect the data, layout and labels.\nvar labels = {\n\t\"N\" : {\n\t\t\"full\"  : \"North\", \/\/used in tooltip\n\t\t\"short\" : \"N\", \/\/used in small breakpoint\n\t\t\"ap\"    : \"N\" \/\/label format used in larger breakpoints. Can be renamed from \"ap\" if option `labelStyle` is changed to match or not included if `labelStyle` is set to \"full\" or \"short\".\n\t},\n\t\"E\" : {\n\t\t\"full\"  : \"East\",\n\t\t\"short\" : \"E\"\n\t},\n\t\"S\" : {\n\t\t\"full\"  : \"South\",\n\t\t\"short\" : \"S\"\n\t},\n\t\"W\" : {\n\t\t\"full\"  : \"West\",\n\t\t\"short\" : \"W\"\n\t}\n}\n\nvar data = { \n\t\"N\" : { \"value\": \"longitude\" },\n\t\"E\" : { \"value\": \"latitude\" },\n\t\"S\" : { \"value\": \"longitude\" },\n\t\"W\" : { \"value\": \"latitude\" }\n};\n\nvar map = new Squaire(data, {\n\tlayout: \",N,\\nW,,E\\n,S,\",\n\tlabels: labels,\n\tlabelStyle: \"full\",\n\tcolors: d3.scale.ordinal([\"longitude\",\"latitude\"]).range([\"#c9e2f5\",\"#c6e2ba\"]),\n\tindexType: \"string\"\n});\nSee data and layout for more information on formatting.\nActive box\n.active is applied to the <g> element (box) in focus via mouseover. The element is also moved to the front.\nPhilosophy\nSquaire aims to be an extendable base to cover 90% of uses of square bin maps online. The tables are responsive and easily adopted for print use via downloading the SVG. Using d3.js makes the base extendable for advanced uses and keeps colors separate. Tooltips add an additional level of data. Prototypical inheritance makes it easier to extend parts of the library without editing the source code. Legends and colors are separate from the tool as these vary greatly depending on the user and data.\nIf Squaire seems like overkill to you or you only need an image or simple static representation of the map, keep reading.\nAlternatives\nSquaire may be overkill for your needs, and it certainly isn't the first of its kind. Check out these other tools and people making charts with squares:\n\nSquared.js: Quick and lightweight script to make any box layout in HTML. It doesn't handle data directly. Apply classes which you style color in your CSS. Good for static visuals with short labels. Rendering in HTML means no SVG download for advanced styling.\nstateblocks:\nThe Marshall Project's tool. Code unknown. Seems to make <div>s and apply styles directly. Result is not responsive (yet).\nstatebins: R plugin to make an image in R. Examples are of U.S. state maps.\n\nRelated reading\n\nVarious layouts of square states in a U.S. map.\nFormat spreadsheet data into JSON and Squaire-friendly data using Mr. Data Converter.\nMake a new layout with MinnPost's aRanger.\n\nContributing\nPull requests are welcome and I will do my best to stay on top of them. However, I may not have time to carefully consider and test every contribution, so please do your best to test code changes and include what you did, what it changed and why you did it.\nContributions to squaire.js and squaire.css\nSquaire uses Grunt to compile files for distribution and publish the demo on github pages. If you wish to contribute, please use Grunt to compile edits to the source files. (Get started with Node.js and Grunt.)\ngrunt watch : Watch the files in src for a change. On change, compile the files into dist\/ (build) and move a copy to demo\/ (demo).\ngrunt publish : demo and push demo\/ to gh-pages.\nVersion history\nv0.5.0\n\nInitial release\n\nLicense\nISC\n","91":"The Meta Tag Checker.\nA customisable validator for web page meta tags, including Facebook open graph tags and Twitter card tags. Having correct meta tags can improve SEO and clickthroughs on social networks.\nLive demo here.\nFeatures\n\nCheck tags exist and have valid contents.\nResults can be accessed programmatically via API.\nEasy to customise which tags are checked and their validation rules.\n\nGetting started\n\nDownload this repository.\ncomposer install (If you don't have Composer installed, installation instructions are here.\nUpload to your web host, in any directory.\nOptional: Customise the JSON schema (see below for further instructions).\n\nResults API\nIn addition to the main results page, you can get a JSON representation of the results by adding \/api\/ to the URL.\nCustomising the validator schema\nThe meta tags and their rules are specified in config\/schema.json.\n\n\n\nKey\nExample value\nDescription\n\n\n\n\nselector\nlink[rel='image_src']\nCSS selector for meta tag.\n\n\nattribute\nhref\n(optional) Tag property to get content from. Defaults to inner text.\n\n\npattern\n\/(^http:\\\/\\\/)\/\nRegex (regular expression) which validates content of tag. Returns 'ok' if it finds a match. Make sure to escape slashes (so it's valid JSON) and leave out the global 'g'.\n\n\nhint\nURL of a promo image.\nHuman-readable description of the pattern.\n\n\ngroup\nmeta\n(optional) Used to sort tags into groups.\n\n\nlongname\nTeaser image\nDescriptive name for humans.\n\n\ntype\nimage\n(optional) Set to 'image' if result should be a JPG\/PNG\/GIF. Other options: 'url', 'strict-url' (see below for more details).\n\n\nurl\nhttps:\/\/dev.twitter.com\/cards\/types\n(optional) URL with further information about meta tag.\n\n\n\nExample regex rules\nIf you're customising the schema, you'll need to write your rules using regular expressions (regexes). The meta tags's contents are considered 'ok' if the regex finds a match.\nBelow are a few examples to get you started. For writing your own rules, Regexr is a useful resource.\n\n\n\nRegex\nEscaped regex\nDescription\n\n\n\n\n\/.\/\n\\\/.\\\/\nMatches non-blank values.\n\n\n\/[^(replace this)]\/\n\\\/[^(replace this)]\\\/\nMatches values which do not contain replace this.\n\n\n\/(^http)\/\n\\\/(^http)\\\/\nMatches values beginning with http.\n\n\n\/(^http).*(_NS_)[^\\s]*\/\n\\\/(^http).*(_NS_)[^\\\\s]*\\\/\nMatches values beginning with http and containing the string _NS_.\n\n\n\nCustom selectors\nNeed to find something in the document by something other than a CSS selector? Simply add a new function to the $custom_functions array in custom.php. You can specify it using a selector custom:yourFunctionName.\nCustom validators\nNeed to validate a value with something more complex than a regular experession? As with custom validators, simply add a new function to the $custom_functions array in $custom.php. You can specify it using a validator custom:yourFunctionName.\nExample custom validator\nThis function checks that an image is square.\n$custom_functions = Array(\n    'squareImage' => function($image_url) {\n        \/\/ use PHP's builtin getimagesize function to get height and width of image\n        $size = getimagesize( $image_url);\n        $w = $size[0];\n        $h = $size[1];\n        \/\/ divide to get ratio\n        $ratio = $w\/$h;\n        \/\/ return false if not square\n        if ($ratio !== 1) {\n            return false;\n        }\n        return true;\n    },\n    \/\/ ... more functions here ...\n);\nThen in schema.json, specify the function's name in the pattern field of your item:\n{\n    \"selector\": \"link[rel='image_src']\",\n    \"attribute\": \"href\",\n    \"longname\": \"Promo image\",\n    \"type\": \"image\",\n    \"pattern\": \"custom:squareImage\",\n    \"hint\": \"Should be a URL of a square image.\"\n}\nContent types\n\n'image': Renders content on frontend in  tag.\n'url': Checks header of URL to make sure link is valid, and renders in  tag.\n'strict-url': Same as 'url', but rejects redirect headers (eg. 301s). Useful for canonical URLs.\n\nRunning behind a proxy\nAdd a file called proxy.php to the config directory. Within the file, use stream_context_set_default to configure the proxy.\nChangelog\n2.0.1\n\nShorter timeout when checking URL validity\n\n2.0.0\n\nSimplified proxy configuration\n\n1.0.1\n\nNew content types: 'url' and 'strict-url'\n\n1.0.0\n\nInitial release\n\n","92":"NPR Story API\nA collection of tools for publishing from and to NPR's Story API. Find this plugin on the Wordpress.org Plugin Repository.\nContributors: nprds, innlabs\nRequires at least: 3.8.14\nTested up to: 4.9\nStable tag: 1.8\nLicense: GPLv2\nLicense URI: https:\/\/www.gnu.org\/licenses\/gpl-2.0.html\nDescription\nThe NPR Story API Plugin provides push and pull functionality with the NPR Story API along with a user-friendly administrative interface.\nNPR's API is a content API, which essentially provides a structured way for other computer applications to get NPR stories in a predictable, flexible and powerful way. The content that is available includes audio from most NPR programs dating back to 1995 as well as text, images and other web-only content from NPR and NPR member stations. This archive consists of over 250,000 stories that are grouped into more than 5,000 different aggregations.\nAccess to the NPR Story API requires an API Key to NPR's legacy APIs. If you are an NPR member station or are working with an NPR member station and do not know your key, please ask NPR station relations for help.\nThe WordPress plugin is being developed as an Open Source plugin by NPR. If you would like to suggest features or bug fixes, or better yet if you would like to contribute new features or bug fixes please contact NPR station relations.\nInstallation\n\nUpload the plugin files to the \/wp-content\/plugins\/plugin-name directory, or install the plugin through the WordPress plugins screen directly.\nActivate the plugin through the 'Plugins' screen in WordPress\nUse the Settings -> NPR API screen to configure the plugin. Begin by entering your API Key, then add your Push URL and Org ID.\n\nFrequently Asked Questions\nCan anyone get an NPR Story API Key?\nWe are no longer provisioning public API key for our legacy APIs. If you are an NPR member station or are working with an NPR member station and do not know your key, please ask NPR station relations for help.\nCan anyone push content into the NPR Story API using this plugin?\nPush requires an Organization ID in the NPR Story API, which is typically given out to only NPR stations and approved content providers. If that's you, you probably already have an Organization ID.\nWhere can I find NPR's documentation on the NPR Story API?\nThere is some documentation in the NPR Story API site: www.npr.org\/api\/index.php.\nIs there an easy way to directly query the NPR Story API?\nYou bet, just visit the NPR Query Generator: www.npr.org\/api\/queryGenerator.php\nScreenshots\nNPR Story API Plugin Settings screen\n\nNPR Story API multiple get settings\n\nGet NPR Stories link in the dashboard\n\nGetting an NPR Story by Story ID\n\nNPR Stories having got gotten\n\nChangelog\nV1.8\n\nFixes issue preventing pushing to the API, introduced in V1.7. PR #60 for issue #57.\nFixes issue where images were not properly sideloaded. PR #60 for issue #59.\nFixes invalid GMT offset error when creating new DateTimeZone object in post metabox. PR #53 for issue #52.\nWhen interacting with a site using the plugin in an unconfigured state, users will be prompted to set the NPR Story API Push URL. PR #56 for issue #51.\nMiscellaneous code quality improvements.\n\nV1.7\n\nThe Story API box that appears in the post editor has been refreshed:\n\nInstead of requiring a separate action to push the story to the Story API, the content will be pushed whenever the content is saved in WordPress, if the \"Send to NPR Story API\" box is checked.\nThe box now includes options to include the story for listening in NPR One, and to set the story as \"featured\" in NPR One. This feature includes the option to set an expiration date, after which time the story will not appear in NPR One.\n\n\nHTTPS is now supported for accessing the Story API. (#44)\nThe push and pull post types are now respected, thanks to #41 from @chrisenterey.\nPHP 7 is now supported, thanks to #42 from @tjuddill.\nSeveral broken links in the documentation have been repaired. (#44)\nAutomated tests are now run against an expanded list of WordPress and PHP versions, as described in pull request #46.\n\nV1.6\n\nAdded meta box to post edit page to explicitly push a story to NPR One\nAdded documentation\nIf pushing a story to NPR fails, the error message is displayed on the post edit page\nIf it succeeds, a success message is displayed\nRename the plugin to \"NPR Story API\"\nThe plugin now requires certain WordPress capabilities before performing API actions:\n\ndeleting posts in the API now requires the delete_others_posts capability for the user trying to delete the post. In effect, this limits deletion of stories from the NPR Story API to admins and editors.\npushing stories to the API requires the publish_posts capability, which means that contributors and guests can't push to NPR.\npulling posts from the API requires the edit_posts capability, meaning that contributors and guests can't pull from NPR. We may want to revisit this in the future, or make the permissions filterable if sites using the plugin want to lock permissions down or open them up.\n\n\nA number of settings in the admin now use the number input type instead of text fields\nAdded unit tests for much of the PHP code\nMost functions were renamed to use the prefix nprstory_ instead of ds_npr_ or no common prefix. This does not affect\nAdded nonces, input sanitization, and input validation for settings.\nDS_NPR_API class now uses new-style __construct() constructor.\nRemoved \/wp-admin\/includes\/admin.php, \/wp-load.php, \/wp-includes\/class-wp-error.php from the cron job function, see 4b3d65a and cf2dfa3 - the cron job works without them.\n\nV1.5.2\n\nAdding support for enclosures created as metadata by the PowerPress plugin.\nAdded NPR One checkbox that makes pushed stories available to the NPR One app (unchecked by default).\nUsers were getting a whitescreen when attempting to push posts. The default byline element -- when used with no mapping or co-authors plugin -- was being sent without a tag name, causing a fatal error.)\nGeneral clean up; small doc tweaks; bug fixes.\n\nV1.5.1\n\n\nMultiple Bylines for pulled stories -  When an API story has multiple bylines, we will now populate a meta field npr_multi_byline with a pipe (|) deliminated list of names.  If the author also has a link for thier byline then there'll be a tilde (~) separating the author's name and the link.\n\n\nAn example of data for multiple bylines:\nAri Shapiro~https:\/\/www.npr.org\/people\/2101154\/ari-shapiro|kmoylan@npr.org|Tommy Writer\n\n\n\nHere there are three authors for this story, Ari Shapiro (and his bio link), then \"kmoylan@npr.org\" and a third author \"Tommy Writer\". Single author stories will not be changed. (This fix came from Ryan Tainter at KERA. Thanks, Ryan!)\n\n\n\n\nDo not re-pull stories that are already in draft status - The get multi process would repleatedly re-pull stories into draft mode if there is already a draft copy. This fix will check for status=any when checking to see if a story already exists in the WordPress database.\n\n\nV1.5\n\n\nReversed the order of images and audio that are being pushed to the NPR Story API.  We're now sending them in the order the file was created.\n\n\nMapping of media credit and agency - If you have a field that you are storing image credit and agency, you can now send these to the NPR Story API. In making this change we needed to expose meta fields that begin with an underscore.  So you may see more meta fields visable on the mapping page.\n\n\nMultiple bylines are now being pushed to the NPR Story API if you use the plugin co-author-plus (https:\/\/wordpress.org\/plugins\/co-authors-plus\/) We insert a byline row for each of the co-authors of a post when we push. You can still use a custom byline field if you want to use that along with the co-authors plugin.\n\n\nRetrieved story dates -  Now when you retrieve a story from the NPR Story API the published date for WordPress will be the date from the  field in the NPR Story API. This will better allow stories to fall into their natural order when publishing from the Get Multi page. It's always possible to edit the date by hand when pulling single stories from the NPR Story API. Do remember though that you are contractually obligated to show the original date when rendering a story from the NPR Story API.\n\n\nRetrieved story order -  When you retrieve stories from the API, if you do not include a sort parameter in your query, we will insure that the order of the stories is in reverse cron for any stories that have the same storyDate in the API. Often some aggregate groups in the API will publish stories with the same storyDate. Shows like Morning Edition do this often, and prior to this fix stories would appear to be published in reverse order from what the API shows.  Now if you do not include sort in your query, the stories with the same time will be published in the same order as the API shows them.\n\n\nConfigurable Cron interval -  When you use the Get Multi page, which fires off of wp-cron, you can now set the interval for the wp-cron to fire. This is on the Get Multi configuration page, and allows you to enter a number of minutes you want as your wp-cron interval. If you enter any thing that is not a number, or a number less than 1, we will resolve that to 60 minutes...because, well, that's a resonable interval, and the wp-cron doesn't understand 'every now and then' as an interval.\n\n\nImages from crops - In earlier versions of the plugin we would pull down the image from the URL in <image src=>. Alas, a lot of those images were small, with ?s=12 in the URL, or thumbnails. So now we're bringing down any bigger versions that we can find. We'll look for the following elements under  tags. In order <enlargement>, <crop type=enlargement>, <crop type=standard>, and then get <image src=> if those other elements do not exist. This should help with getting bigger images. If you need a smaller image, just theme the bigger one.\n\n\nWhen pushing a story to the NPR Story API, we will now be checking to see if images exist in the content, if so, add the query string parameter origin=body to the image url so CorePublisher will like it. This is only something the Core Publisher uses, but it will help greatly for stories published in Wordpress and retrieved into Core Publisher.\n\n\nAdding meta data as meta fields to each pulled image.  The following meta fields will be attached to the posts for pulled images:\n  npr_image_credit =>$api_image->producer\n  npr_image_agency =>$api_image->provider\n  npr_image_caption =>$api_image->caption\n\n\n\nWhen pushing a story with Audio, the audio description will be pushed as well, provided there is one.\n\n\nV1.4\n\nFilters for Shortcodes - We've now implemented a hook to a filter that will be used to alter any short codes that a plugin may own in a post before pushing that post to the NPR Story API. The filter (npr_ds_shortcode_filter) will fire before we remove shortcodes when we're pushing the post. Any plugin that has shortcodes should alter those shortcodes via this filter to something that's more HTML-friendly, so that other clients can access that information. As an example of what needs to be done in a plugin to take advantage of this new filter please see the branch for the following plugin: https:\/\/github.com\/argoproject\/navis-documentcloud\/tree\/kmoylan-filter What we've done here is write a function my_documentCloud_filter that is linked to the filter npr_ds_shortcode_filter (all near the bottom of the PHP file).  This function will turn any shortcode created by this plugin into an HTML <a> tag to link to what was an embedded document. As with any other filter in WordPress, nothing will happen if you do not have any plugins installed that have implemented the filter. It will be up to any other plugin's maintainer to implement the filter correctly if they wish to take advantage of this functionality.\nBulk Push - From the post list page for you NPR Push post type you can now select multiple posts and using the bulk operation dropdown on that page, push the selected posts to the NPR Story API. This should helpful for posts that have been living on a site before the NPR Story API plugin was installed. Note that this will only push a maximum 20 posts at one time.\nPublish or Draft for Get Multi -  It's now possible for an admin to select Draft or Publish for the posts that come from a query on the get multi page. This way, the return from each query can be reviewed before it's published to a site.\nRun Get Multi on Demand -  An admin can now select a checkbox if they would like the get multi queries to run when the page is saved. This will allow admins to immediately check queries instead of having to wait for the cron to run.\n\nV1.3\n\nPermissions - If you have created any NPR Story API Permissions Groups you can select which (if any) group you would like to use as your default group when pushing content to the NPR Story API.  Learn more about NPR Story API Permissions Groups here: https:\/\/digitalservices.npr.org\/post\/npr-api-content-permissions-control\nMulti-Site - Cron set up and other activation tasks will now happen on every site in a multi-site configuration.\nCron queries - We won't try to query for any of the configured queries on the Get Multi page if the field isn't filled in with data.\nContent and short codes - Instead of blindly removing all shortcodes from a story's content, we are now trying to let any plugins replace the shortcodes with HTML, and then if there are any leftover shortcodes (we delete those?).\nByline URL - We are now saving any links to the author of a story's bio page in the meta field npr_byline_link. This should allow you to supply a link for any author who has a biography registered with the NPR Story API.\n\nV1.2\n\nEnhance error messages when there are connectivity or permissions issues with your NPR Story API work. We have also included the ability for admins to map custom Meta fields to certain Post fields that are pushed to the NPR Story API. And finally, we've add the ability to retrieve transcripts for stories that have transcripts associated with them. Along with these changes we've added the the ability to save urls for the .m3u files attached to NPR stories.\n\nV1.1\nThis version will allow admins to configure their WordPress site to retrieve multiple NPR Story API stories that will be automatically published (via cron) to their site.\n\n\nFrom the Settings -> NPR API Get Multi page (wp-admin\/options-general.php?page=ds_npr_api_get_multi_settings) an admin can add a number of queries.\n\n\nThese query entries can contain an API ID for a single story, or an ID for a specific category, program, topic, etc.\n\n\nThe query can also contain the full query string can be created from the NPR Story API Query Generator: https:\/\/www.npr.org\/api\/queryGenerator.php\n\n\nYou can also enter the URL for a story you found on npr.org.\n\n\nThe entered queries will be executed via the Wordpress cron functionality, hourly.\n\n\nAny new stories that are available will be automatically published.  You can find a list of query filters at the npr.org's API documentation page: https:\/\/www.npr.org\/api\/inputReference.php\n\n\nStories retrieved from the NPR Story API will be created as Posts in WordPress.  Each post will have a number of meta fields associated with the post.  These meta fields will all begin with npr_ and can be viewed on a post edit screen with Custom Fields option enabled through Screen Options. A story from the API that has a primary image defined will have that image set as the featured image of the Wordpress post.  Any bylines for the NPR Story will be stored in the meta field npr_byline. The list of npr_ meta fields is:\n  npr_api_link\n  npr_byline\n  npr_html_link\n  npr_last_modified_date\n  npr_pub_date\n  npr_retrieved_story\n  npr_story_content\n  npr_story_id\n\n\n\nOn the All Posts admin screen we've made a couple of modification\/additions to help you manage your NPR Story API content.\n\nThere is a new Bulk Action available, 'Update NPR Story'.  When one or many NPR Story API Stories are selected and the Update NPR Story action is applied, this plugin will re-query the NPR Story API for those stories, and if the story's content has changed it will update the post in WordPress.\nThere is also a new column added to this page titled \"Update Story\".  For any Post retrieved from the NPR Story API there will be a link to \"Update\" the story. Pressing this link will bring the user to the query NPR Story API page with the story ID pre-filled. Pressing \"Publish\" on this screen will re-query the NPR Story API for the story and update the Post with any changes that may have taken place on the NPR Story API for that story.\n\n\n\nUpdate and Delete of pushed stories - A story that was written in your Wordpress site and pushed to the NPR Story API will be automatically be updated in the API when your story is updated in Wordpress.  When your Post is moved to the Trash in Wordpress, your story will be deleted from the NPR Story API.  If a Trashed Post is resorted, your story will be made available to the NPR Story API again.\n\n\nV1.0\nAs not a lot of users have installed the V1.0 of the NPR Story API Plugin, there are a couple of things to keep in mind.\n\nOn the NPR Story API settings page (wp-admin\/options-general.php?page=ds_npr_api) there are 4 fields.\n\nAPI KEY - This is your NPR Story API Key that you can get from NPR.  If you wish to push stories to the NPR Story API you'll need to have your key configured by NPR Digital Services.  Please contact Digital Services with a support request at https:\/\/info.ds.npr.org\/\/support.html\nPull URL - This is the root url for retrieving stories.  For testing purposes, you should configure this to be https:\/\/api-s1.npr.org. NOTE: this url should not contain a trailing slash.\nPush URL - Much like the pull url, this url is used to pushing stories to the NPR Story API. Again, for testing purposes, you can utilize NPR's staging server  at https:\/\/api-s1.npr.org. If you do not wish to push your content, or your NPR Story API has not been authorized, you should leave this field empty and the WordPress plugin will not attempt to push you content to the NPR Story API.\nOrg ID - This is your organization's ID assigned by NPR.  If you don't know your Org ID, please contact Digital Services at: https:\/\/info.ds.npr.org\/support.html\n\n\nYou can pull stories one at a time from the NPR Story API by using the Get NPR Stories page under admin Posts menu (wp-admin\/edit.php?page=get-npr-stories). This can be story ID from the API, or the URL for the story from npr.org. For help in finding possible query options, please use the Query Generator at https:\/\/www.npr.org\/api\/queryGenerator.php Documentation is at: https:\/\/www.npr.org\/api\/inputReference.php\n\nUpgrade Notice\n1.5.2\nThis version adds export functionality for the NPR One mobile app, in addition to assorted bug fixes.\n","93":"NPR One Backend Proxy\nA PHP-based server-side proxy for interacting with the NPR One API's authorization server. Use this proxy to secure your OAuth2 credentials.\n    \nTable of Contents\n\nBackground\nSetup\n\nPrerequisites\nInstallation\nIntegration\n\nRequired Classes\n\nRouter\nConfigProvider\n\n\nConditionally Required\n\nStorageProvider\n\n\nOptional\n\nEncryptionProvider\nSecureStorageProvider\n\n\n\n\n\n\nImplementation Details\n\nAuthorization Code Grant\nDevice Code Grant\nRefresh Token Grant\nLogout\/Disconnect\n\n\nDocumentation\nContributing\nLicense\n\nBackground\nThe NPR One API provides a lightweight REST\/Hypermedia interface to power an NPR One experience. To secure our API, we have implemented an authorization server based on the OAuth 2.0 protocol, a well-accepted Internet standard.\nThird-party developers have two primary methods for obtaining the access tokens required by our API to interact with any of our other micro-services:\n\nthe authorization_code grant\nthe device_code grant (a custom grant based on Google's proposed spec for OAuth2 for Limited Input Devices)\n\nThe NPR One authorization server does not currently accept the implicit grant type described in the OAuth2 spec due to security concerns.\nBoth the device_code and authorization_code grant types require an OAuth2 client_secret to generate an access token. However, since the source code for web applications written in a client-side language (like Javascript) cannot be kept private, a server-side proxy is required to safely make calls to the authorization server and ensure the security of your OAuth2 credentials.\nIn order to make this requirement less painful for third-party developers, we are providing this PHP-based proxy as an open-source package to help you get up-and-running quickly and prevent NPR One client credentials from being compromised in public source code.\nSetup\nThis project is designed to be executed in a server environment with Apache HTTP Server or Nginx.\nPrerequisites\nA recent version of PHP, equal to or greater than 7.2.0 is required.\nThe default EncryptionProvider class provided in this package relies on the OpenSSL extension. If OpenSSL is unavailable, the consumer has the option to implement a custom EncryptionProvider class that implements our EncryptionInterface. (For more information, see the EncryptionProvider section.)\nUsage of NPR's authorization server requires a registered developer account with the NPR One Developer Center. If you do not already have a Dev Center account, you can register for a personal account and get started immediately.\nInstallation\nThis project is intended to be run as a sub-module (or dependency) of a larger project and should be installed using Composer (an open-source dependency manager for PHP projects):\n[sudo] composer install npr\/npr-one-backend-proxy\n\nIf you do not already have a Composer project set up, you can start one quickly with:\ncomposer init --require=\"npr\/npr-one-backend-proxy\" -n\n\nIntegration\nThe following 2 PHP classes must be created to integrate this package into your project:\n\nRouter\nConfigProvider\n\nAdditionally, if you are using the authorization_code grant, a StorageProvider class will be required. Examples of each can be found in the examples folder.\nRequired Classes\nRouter\nCreate a router which calls the relevant public methods in either AuthCodeController or DeviceCodeController, depending on which grant type will be used (authorization_code or device_code, respectively).\nAll consumers, regardless of grant type, MUST implement a route that maps to the generateNewAccessTokenFromRefreshToken() function in the RefreshTokenController class. This route allows your server to seamlessly request a new access token when the original one has expired. If you do not implement this route, your users will automatically be logged out after 2 weeks and required to log back in to resume listening, which is not the desired user experience.\nSimilarly, all consumers (assuming they provide some kind of 'Logout' or 'Disconnect from NPR One' functionality) SHOULD implement a route that maps to the deleteAccessAndRefreshTokens() function in the LogoutController class. This route allows your app to ensure that all persistent data related to a logged-in state (such as access tokens and refresh tokens) are removed from NPR's authorization server, as well as ensuring that the refresh_token is removed from the secure storage layer. This function takes in an access token but it can also work without any input, assuming that refresh tokens are being stored persistently in the secure storage layer.\nThe Router.php file in the examples folder provides a hypothetical Laravel-esque example of what this might look like. Please note, this code is intended only as an example to provide guidance on how to get started and has not been tested. This example includes code for both the authorization_code and device_code grant types, but in your actual implementation, include only the code relevant to whichever grant type you are using in your application.\nConfigProvider\nCreate a ConfigProvider class that implements our ConfigInterface to power your controller classes. The ConfigProvider class will encapsulate the consumer-specific variables (your client ID and client secret) needed to power this OAuth2 proxy.\nThere is a sample ConfigProvider.php in the examples folder to help you get started. This class does not need to be complicated and can mostly just return hard-coded strings. However, do not include your client secret (or your encryption salt) in any files that will appear in public repositories, as this could compromise your application. We assume that you either plan to keep your code private or you have some form of private secrets file that is not included in any public repositories or publicly-accessible locations.\nConditionally Required\nStorageProvider\nIf you are using the authorization_code grant (and thereby the AuthCodeController), create a StorageProvider class which implements our StorageInterface. The StorageProvider is required to validate the OAuth2 state param.\nYou will find a sample StorageProvider.php file in the examples folder. The example utilizes Predis, a PHP Redis client, but there are many other options, including Memcached and PHP sessions. MySQL is also an option, but not recommended because it is likely to be much slower. We picked Predis for demonstration purposes because the syntax is very simple and applicable to many other storage layers.\nOptional\nEncryptionProvider\nThe Controller classes will save the refresh token and access token in a cookie by default. In order to keep those refresh tokens secure, we encrypt them before saving and decrypt them when we need to retrieve them. To make this process less cumbersome, a default EncryptionProvider has been provided. However, this particular EncryptionProvider relies on the OpenSSL extension being available, which may not be an option for all developers. If OpenSSL is unavailable, or if you want to use a different method of encryption, you can use a custom encryption provider that implements our EncryptionInterface.\nIf you choose to implement a custom encryption provider, use the default implementation as your example. The syntax for including your own custom encryption provider is as follows:\nauthorization_code grant type:\nuse NPR\\One\\Controllers\\AuthCodeController;\nuse Your\\Package\\Here\\ConfigProvider;\nuse Your\\Package\\Here\\EncryptionProvider;\nuse Your\\Package\\Here\\StorageProvider;\n\n$controller = (new AuthCodeController())\n        ->setConfigProvider(new ConfigProvider())\n        ->setStorageProvider(new StorageProvider())\n        ->setEncryptionProvider(new EncryptionProvider());\ndevice_code grant type:\nuse NPR\\One\\Controllers\\DeviceCodeController;\nuse Your\\Package\\Here\\ConfigProvider;\nuse Your\\Package\\Here\\EncryptionProvider;\n\n$controller = (new DeviceCodeController())\n        ->setConfigProvider(new ConfigProvider())\n        ->setEncryptionProvider(new EncryptionProvider());\nSecureStorageProvider\nAs explained above, encrypted cookies are used to store refresh tokens across sessions. However, cookies are not the only possible storage method: Redis and Memcached are good options (as long as you have a mechanism for identifying the user across sessions, which may still require cookies). If you are considering using PHP's session storage, you may want to take a look at PHP-Secure-Session, which provides an extra layer of security through encryption.\nAll of the Controller classes are configured to use the SecureCookieProvider as the default secure storage layer, but you can easily override this using the setSecureStorageProvider() function:\nauthorization_code grant type:\nuse NPR\\One\\Controllers\\AuthCodeController;\nuse Your\\Package\\Here\\ConfigProvider;\nuse Your\\Package\\Here\\SecureStorageProvider;\nuse Your\\Package\\Here\\StorageProvider;\n\n$controller = (new AuthCodeController())\n        ->setConfigProvider(new ConfigProvider())\n        ->setStorageProvider(new StorageProvider())\n        ->setSecureStorageProvider(new SecureStorageProvider());\ndevice_code grant type:\nuse NPR\\One\\Controllers\\DeviceCodeController;\nuse Your\\Package\\Here\\ConfigProvider;\nuse Your\\Package\\Here\\SecureStorageProvider;\n\n$controller = (new DeviceCodeController())\n        ->setConfigProvider(new ConfigProvider())\n        ->setSecureStorageProvider(new SecureStorageProvider());\nYour custom secure storage provider class needs to implement the StorageInterface, but aside from that there are no special requirements. If you are using a tool like Redis or Memcached, you are not required to encrypt or decrypt your tokens since those systems are typically already implicitly secure. Encryption is only explicitly required by the SecureCookieProvider class.\nImplementation Details\nRead on for more information about how this package operates behind-the-scenes, which will help guide how your client application interacts with this backend proxy.\nAuthorization Code Grant\nThe authorization_code flow has two phases, which in our case correspond to the startAuthorizationGrant() and completeAuthorizationGrant() functions in the AuthCodeController class:\n\n\nPhase 1: startAuthorizationGrant() constructs the query parameters that are needed for the call and appends them to https:\/\/authorization.api.npr.org\/v2\/authorize. Your router should then redirect the browser to that URL (either using a framework's built-in function such as Laravel's redirect()->away($url), or otherwise just using a good old-fashioned header(\"Location: $url\")).\n\n\nPhase 2: completeAuthorizationGrant() should be mapped to the redirect_uri that you added to your client application in the NPR One Developer Console. This function has two primary responsibilities:\n\nValidating the state parameter that was generated during the startAuthorizationGrant() phase. This extra check ensures that your call was not intercepted by a malicious third party.\nExchanging the authorization code for an actual access token using the POST https:\/\/authorization.api.npr.org\/v2\/token endpoint.\n\n\n\nIt then saves the token to an unencrypted cookie called access_token using our CookieProvider class. NOTE: it is highly recommended that your client application retrieves the value of the cookie, stores it somewhere locally (HTML5 localStorage is a good option), and then deletes the cookie. Otherwise, since the cookie is not encrypted it is not considered secure, and may also result in extra overhead on subsequent HTTP requests.\nNote that the completeAuthorizationGrant() function does return an AccessTokenModel, but since the authorization_code grant is designed to work by redirecting the browser, it is not recommended that you actually return JSON from this endpoint. Instead, you will want to use the getRedirectUri() function to return to your client application and then retrieve the access token from the cookie as described above.\nDevice Code Grant\nThe device_code grant similarly has two phases, but requires a little more work on the part of the client. The DeviceCodeController class has two public methods: startDeviceCodeGrant() and pollDeviceCodeGrant(); each should be mapped to a unique endpoint in your router.\n\n\nPhase 1: The client starts off the process by calling the route that corresponds to the startDeviceCodeGrant() function, which calls the POST https:\/\/authorization.api.npr.org\/v2\/device endpoint and then does two things: one, it safely stores the device_code (value) itself, either in an encrypted cookie or using a custom secure storage provider as described here; and secondly, it returns everything else as a JSON object to the consumer. The consumer is then responsible for displaying the user_code and verification_uri on the screen.\n\n\nPhase 2: Next, the client is responsible for polling the route that corresponds to the pollDeviceCodeGrant() function, which calls the POST https:\/\/authorization.api.npr.org\/v2\/token endpoint with the securely-stored device_code and checks to see whether the user has logged in yet (returning an access token if so, and throwing an Exception if not). This polling should occur at a rate not exceeding the interval value in the JSON object returned by the previous call.\n\n\nAll device code\/user code pairs will expire within the expires_in value in the JSON object returned by the previous call (this value represents a TTL in seconds). The client application is responsible for calling the route that corresponds to the startDeviceCodeGrant() function to restart this process if the user fails to log in before the device code expires.\nRefresh Token Grant\nThe refresh_token that is generated in association with every new access token should be stored securely either in an encrypted cookie or by using a custom secure storage provider as described here. The RefreshTokenController class is thus refreshingly simple and has one method:\n\ngenerateNewAccessTokenFromRefreshToken() looks for this refresh_token in the secure storage provider and (if found) uses the refresh_token grant provided by the POST https:\/\/authorization.api.npr.org\/v2\/token endpoint to obtain a new access token for the user. (And in case you were wondering: yes, that call will result in a new refresh_token being generated, which is then saved to the secure storage layer in the exact same way.)\n\nThis method should be called when any client application that has previously obtained a valid access token suddenly receives a 401 Unauthorized response from any of our micro-services, indicating that the access token has expired. This error should call the endpoint in your router that calls generateNewAccessTokenFromRefreshToken(). A new access token will be generated and returned as raw JSON (where it is up to the client application to store it securely). If a new access token could not be generated, the client may retry the call up to 2-3 times, but after that point the user should be considered logged out and prompted to log in again.\nOptional implementation: The AccessTokenModel and the corresponding JSON output do include an expires_in value (TTL in seconds) for the access token, so the client application may choose (but is not required) to call the route corresponding to generateNewAccessTokenFromRefreshToken() before the token actually expires, or after it was set to expire but before another API call is attempted. Note that regardless of whether it had already expired or not, the original access token will be deleted immediately as part of that call.\nLogout\/Disconnect\nWe ask all clients to help secure user data and free up unused resources in our system by implementing a form of logout functionality that will revoke the user\u2019s previously-generated access tokens and refresh tokens through the POST https:\/\/authorization.api.npr.org\/v2\/token\/revoke endpoint. The deleteAccessAndRefreshTokens() function in the LogoutController class will perform this task, in addition to deleting the refresh_token that was previously saved to an encrypted cookie or your custom secure storage provider. Your client application can be ignorant of whatever mechanism you're using to securely store the refresh token and safely assume that it is properly removed as part of logout.\nAs described in the NPR One API Reference, the POST https:\/\/authorization.api.npr.org\/v2\/token\/revoke endpoint takes in either an access token or a refresh token. By default, it's assumed to be an access token, but it will delete both regardless of which of the two is passed in. Therefore, the deleteAccessAndRefreshTokens() function can take in an access token, but if none is provided, it will look for a refresh token and, if found, use that to revoke the pair of tokens. It is recommended to pass in the access token if you have it (especially for client applications developed prior to summer 2016, when refresh tokens were first introduced). If you are certain that refresh tokens have been issued for all your users and there is no chance that they have been removed by other client-side code, you can safely call deleteAccessAndRefreshTokens() without any parameters.\nThis proxy does not impose any requirements for how you set up and call your endpoints (save for what is strictly required by the OAuth 2.0 spec), so the access token parameter needed for the deleteAccessAndRefreshTokens() function can be obtained from a variety of sources: via a query parameter, form POST data, a POST with a JSON body, and potentially even a cookie, if that is how you are storing your access tokens client-side. The example Router.php file uses a query parameter for simplicity's sake. In most cases, POST requests with form data or JSON bodies are preferable because they are slightly harder to intercept over insecure networks, but since the assumption here is that the access token will be revoked almost immediately, keeping the token secure is not a huge concern.\nDocumentation\nFurther information about the public API of this package can be found in the docs folder.\nFor background information about the NPR One API and our use of OAuth2, please see the developer guide at the NPR One Developer Center. In particular, the section on the Authorization Service may be of interest.\nContributing\nIf you're interested in contributing to this project by submitting bug reports, helping to improve the documentation, or writing actual code, please read our contribution guidelines.\nLicense\nCopyright (c) 2016 NPR\nLicensed under the Apache License, Version 2.0 (the \u201cLicense\u201d) with the following modification; You may not use this file except in compliance with the License as modified by the addition of Section 10, as follows:\n10. Additional Prohibitions\nWhen using the Work, You may not (or allow those acting on Your behalf to):\na.\tPerform any action with the intent of introducing to the Work, the NPR One API, the NPR servers or network infrastructure, or any NPR products and services any viruses, worms, defects, Trojan horses, malware or any items of a destructive or malicious nature; or obtaining unauthorized access to the NPR One API, the NPR servers or network infrastructure, or any NPR products or services;\nb.\tRemove, obscure or alter any NPR terms of service, including the NPR services Terms of Use and the Developer API Terms of Use, or any links to or notices of those terms; or\nc.\tTake any other action prohibited by any NPR terms of service, including the NPR services Terms of Use and the Developer API Terms of Use.\nYou may obtain a copy of the License at https:\/\/www.apache.org\/licenses\/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License with the above modification is distributed on an \u201cAS IS\u201d BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  See the License for the specific language governing permissions and limitations under the License.\n","94":"PMP WordPress Plugin\n\n\nUse this plugin to integrate the Public Media Platform with WordPress. Integration includes pulling stories and other content from the PMP into WordPress drafts and posts, and pushing WordPress posts into the PMP. Define simple or advanced searches for PMP content and save custom searches for reuse, with or without autopublishing on your WordPress site. Create PMP Groups, Properties, and Series from the plugin and push them to the PMP. Use of this plugin requires that you register for a PMP account to get API keys.\nDownload this project from the official Wordpress plugin directory.\nBuilt by INN Labs.\nTable of contents\n\nInstallation and Settings\nPulling Content from the PMP\nPushing content to PMP\nGroups & permissions\nPMP Series\nPMP Properties\nSaved search queries\nPMP Distributor Plugin and Settings\n\nAll Set\nNow you're ready pull great content from the PMP for use on your WordPress site, and push you own fine work to the PMP for others in the public media ecosystem to share with their digital audiences. Do great things!\n","95":"PMP PHP SDK\n \nA PHP API client for the Public Media Platform.\nRequirements\nPHP version >= 5.5.  And a PMP client-id\/secret for your PMP user.\nInstallation\nVia Composer\n\nDownload Composer (if you don't have it already), and install the publicmediaplatform\/pmpsdk package:\n\ncurl -sS https:\/\/getcomposer.org\/installer | php\nphp composer.phar require publicmediaplatform\/pmpsdk\n\nRequire the Composer-generated autoload.php\n\nrequire 'vendor\/autoload.php';\nVia PHAR file\n\nGo to the Latest Release of the pmpsdk\nClick the link to download pmpsdk.phar\nRequire the file in your project:\n\nrequire 'path\/to\/pmpsdk.phar`;\nNOTE: if you see a strange error message full of question marks like ?r??PHP Fatal error:  Class 'Pmp\\Sdk' not found, make sure you turn off detect unicode.\nUsage\nConnecting\nSimply instantiate a new SDK object using your credentials.  Errors will be immediately thrown if there's a problem fetching the API home doc or authenticating.\ntry {\n    $host = 'https:\/\/api-sandbox.pmp.io';\n    $sdk = new \\Pmp\\Sdk($host, 'client-id', 'client-secret');\n}\ncatch (\\Pmp\\Sdk\\Exception\\HostException $e) {\n    echo \"Invalid API host specified: $e\";\n    exit(1);\n}\ncatch (\\Pmp\\Sdk\\Exception\\AuthException $e) {\n    echo \"Bad client credentials: $e\";\n    exit(1);\n}\nHome Document\nAfter successfully connecting, you can immediately interrogate the API home document - an instance of \\Pmp\\Sdk\\CollectionDocJson.\necho \"HOME doc guid  = {$sdk->home->attributes->guid}\\n\";\necho \"HOME doc title = {$sdk->home->attributes->title}\\n\";\nFetching\nTo directly fetch a document (by guid or alias), the SDK provides shortcuts for locating links such as urn:collectiondoc:hreftpl:docs in the home document.  These shortcuts will always return null for HTTP 403 or 404 errors.\n$ARTS_TOPIC = '89944632-fe7c-47df-bc2c-b2036d823f98';\n$doc = $sdk->fetchDoc($ARTS_TOPIC);\nif (!$doc) {\n    echo \"failed to fetch the ARTS topic - must have been a 403 or 404.\\n\";\n    exit(1);\n}\necho \"ARTS topic href    = {$doc->href}\\n\";\necho \"ARTS topic guid    = {$doc->attributes->guid}\\n\";\necho \"ARTS topic title   = {$doc->attributes->title}\\n\";\necho \"ARTS topic profile = {$doc->getProfile()}\\n\";\nif ($doc->scope == 'write') {\n    echo \"And I can write to this, for some reason!\\n\";\n}\nelse {\n    echo \"At least I can read it.\\n\"\n}\nCurrent \\Pmp\\Sdk fetch methods include:\n\n$sdk->fetchDoc($guid)\n$sdk->fetchProfile($guid)\n$sdk->fetchSchema($guid)\n$sdk->fetchTopic($guid)\n$sdk->fetchUser($guid)\n\nQuerying\nTo query documents (by any PMP search params), the SDK provides shortcuts for locating links such as urn:collectiondoc:query:docs.  These shortcuts will always return null for HTTP 404 errors, indicating that your search yielded 0 total results.\n$doc = $sdk->queryDocs(array('limit' => 3, 'text' => 'penmanship'));\nif (!$doc) {\n    echo \"got 0 results for my search - doh!\\n\";\n    exit(1);\n}\n\n\/\/ use the \"items\" directly\n$count1 = count($doc->items);\n$title1 = $doc->items[0]->attributes->title;\necho \"SEARCH - $count1 - $title1\\n\";\n\n\/\/ or get a fancy items object with some helpers\n$items = $doc->items();\n$count2 = count($items);\n$count3 = $items->count();\n$title2 = $items[0]->attributes->title;\nforeach ($items as $idx => $item) {\n    echo \"SEARCH item($idx) = {$item->attributes->title}\\n\";\n    if ($item->scope == 'write') {\n        $item->title = 'Wow, I can change the titles if I wanted to';\n        $item->save();\n    }\n}\nCurrent \\Pmp\\Sdk query methods include:\n\n$sdk->queryCollection($collectionGuid, $params)\n$sdk->queryDocs($params)\n$sdk->queryGroups($params)\n$sdk->queryProfiles($params)\n$sdk->querySchemas($params)\n$sdk->queryTopics($params)\n$sdk->queryUsers($params)\n\nDocument Items\nAs seen above, you can use a Document's items (the expanded links.item array) directly as an array of stdClass objects.  And you can also expand them into a \\Pmp\\Sdk\\CollectionDocJsonItems object for further shortcuts.\n$items = $doc->items();\n\n\/\/ access the paging links via helpers\necho \"SEARCH total   = {$doc->items()->totalItems()}\\n\";\necho \"SEARCH items   = {$doc->items()->count()} items\\n\";\necho \"SEARCH pagenum = {$doc->items()->pageNum()}\\n\";\necho \"SEARCH pagenum = {$doc->items()->totalPages()}\\n\";\nSometimes you'll want to iterate over several pages of search results without directly following the links.navigation previous\/next\/first\/last links.  You can easily get a \\Pmp\\Sdk\\PageIterator for this purpose.  It accepts a $pageLimit paramater to limit the number of returned pages - or exclude the param to iterate over all pages.\n$pageLimit = 3;\nforeach($doc->itemsIterator($pageLimit) as $pageNum => $items) {\n    if ($pageNum < 1 || $pageNum > 3) {\n        echo 'i did not see that one coming!';\n        exit(1);\n    }\n    echo \"SEARCH page $pageNum\\n\";\n    foreach ($items as $idx => $item) {\n        echo \"  item($idx) = {$item->attributes->title}\\n\";\n    }\n}\nOften, when fetching container docs such as \"stories\", you'll want to interrogate child items based on their profile types.  This SDK handles this for you, allowing the retrieval of just items-of-profile.\necho \"looking at a cdoc of profile = {$story->getProfileAlias()}\\n\";\n$items = $story->items();\n$audios = $story->items('audio');\n$images = $story->items('image');\n$videos = $story->items('video');\n\necho \"  contains {$items->count()} items\\n\";\necho \"           {$audios->count()} audios\\n\";\necho \"           {$images->count()} images\\n\";\necho \"           {$videos->count()} videos\\n\";\nDocument Links\nTo navigate links, we can interrogate them directly on the \\Pmp\\Sdk\\CollectionDocJson object, or browse them via the \\Pmp\\Sdk\\CollectionDocJsonLinks object, containing a collection of \\Pmp\\Sdk\\CollectionDocJsonLink objects.\nNote that links have either an href or an href-template attribute.  To get a full URL from the link either way (and optionally passing an array of parameters), use the expand() method.\n$queryLinks = $doc->links('query');\nif (empty($queryLinks)) {\n    echo \"document didn't have any links of reltype = query!\\n\";\n    exit(1);\n}\nforeach ($queryLinks as $link) {\n    $fakeParams = array('guid' => 'foobar');\n    $url = $link->expand($fakeParams);\n    echo \"link = $url\\n\";\n}\nIn some cases, we may know an URN (uniform resource name) of the link we're looking for.  In this case, we can directly fetch the link from the document.\n$link = $doc->link('urn:collectiondoc:query:profiles');\nif (!$link) {\n    echo \"failed to find link in the document\\n\";\n    exit(1);\n}\n\n\/\/ or only look in a specific link relType (links.query[])\n$link = $doc->link('query', 'urn:collectiondoc:query:profiles');\nTo fetch the \\Pmp\\Sdk\\CollectionDocJson at the other end of a link, simply follow() it.  This method optionally accepts the same array of href-template params as expand().  If the url can't be loaded (404) or is inaccessible (403), null will be returned.\n$ownerLinks = $doc->links('owner');\nif (empty($ownerLinks)) {\n    echo \"document didn't have an owner!\\n\";\n    echo \"which is really strange, because it's auto-generated by the API\\n\";\n    exit(1);\n}\n$ownerLink = $ownerLinks[0];\n\n\/\/ or use the shortcut\n$ownerLink = $doc->getOwner();\n\n\/\/ now follow the link\n$ownerDoc = $ownerLink->follow();\nif (!$ownerDoc) {\n    echo \"owner link must have been a 403 or 404!\\n\";\n    exit(1);\n}\necho \"owner = {$ownerDoc->attributes->title}\\n\";\nA document's links.collection will often contain PMP topics, series, properties, and contributors.  These links are normally distinguished by rels such as urn:collectiondoc:collection:property.  As a shortcut for finding these links, you can just refer to the last property segment of that urn.\n\/\/ these statements are equivalent\n$links = $doc->links('collection');\n$links = $doc->getCollections();\n\n\/\/ these statements are also equivalent\n$topicLinks = $doc->links('collection', 'urn:collectiondoc:collection:topic');\n$topicLinks = $doc->getCollections('urn:collectiondoc:collection:topic');\n$topicLinks = $doc->getCollections('topic');\n\n\/\/ more examples...\n$contribCount = $doc->getCollections('contributor')->count();\n$firstSeriesLink = $doc->getCollections('series')->first();\n$firstPropertyLink = $doc->getCollections('property')->first();\nif ($firstPropertyLink) {\n    $propertyDoc = $firstPropertyLink->follow();\n    echo \"Got a property - {$propertyDoc->attributes->title}\\n\";\n}\nModifying documents\nTo create a document, you should first know which Profile Type you'd like to create.  Then use the SDK to instantiate a new \\Pmp\\Sdk\\CollectionDocJson of that type.\n$data = array('attributes' => array('title' => 'foobar'));\n$doc = $sdk->newDoc('story', $data);\n\n\/\/ or alter the document data manually\n$doc->attributes->title = 'foobar2';\n$doc->attributes->valid = new \\stdClass();\n$doc->attributes->valid->to = '3013-07-04T04:00:44+00:00';\n\n\/\/ save, but catch any pmp errors\ntry {\n    $doc->save();\n}\ncatch (\\Pmp\\Sdk\\Exception\\RemoteException $e) {\n    echo \"unable to create document: $e\\n\";\n    exit(1);\n}\nTo update documents, all you need is an instance of \\Pmp\\Sdk\\CollectionDocJson that you can modify.  You can also catch any \\Pmp\\Sdk\\Exception\\ValidationException separately, to handle PMP schema violations separately.\n$doc->attributes->title = 'foobar3';\ntry {\n    $doc->save();\n}\ncatch (\\Pmp\\Sdk\\Exception\\ValidationException $e) {\n    echo \"invalid document: {$e->getValidationMessage()}\\n\";\n    exit(1);\n}\ncatch (\\Pmp\\Sdk\\Exception\\RemoteException $e) {\n    echo \"unable to save document: $e\\n\";\n    exit(1);\n}\nTo delete a document, just get an instance of \\Pmp\\Sdk\\CollectionDocJson that you can modify.\n$doc->attributes->title = 'foobar3';\ntry {\n    $doc->delete();\n}\ncatch (\\Pmp\\Sdk\\Exception\\RemoteException $e) {\n    echo \"unable to delete document: $e\\n\";\n    exit(1);\n}\nLinking\nOne of the main concerns of the PMP are document links.  The syntax basically involves manipulating PHP array and stdClass to represent your links.  For instance, to set an alternate link on a story:\n$doc->links->alternate = array(new \\stdClass());\n$doc->links->alternate[0]->title = 'Foobar Home Page';\n$doc->links->alternate[0]->href = 'http:\/\/foo.edu\/bar';\n$doc->save();\nWhen linking to other docs within the PMP, you should first load those documents to make sure they exist.  The exception to this is when you're using a known alias for a topic, profile, etc.  Here's an example of several different kinds of links:\n\/\/ let's create-or-update an image, with a known guid\n$img = $sdk->newDoc('image', array(\n    'attributes' => array(\n        'guid'        => $KNOWN_IMAGE_GUID,\n        'title'       => 'Alternate text here',\n        'byline'      => 'Myimage Credit',\n        'description' => 'This is a caption for this image',\n    ),\n    'links' => (\n        'enclosure' => array(\n            array(\n                'href' => 'http:\/\/path\/to\/image\/thumbnail.jpg',\n                'type' => 'image\/jpeg',\n                'meta' => array(\n                    'crop'   => 'small',\n                    'height' => '100',\n                    'width'  => '50',\n                ),\n            ),\n            array(\n                'href' => 'http:\/\/path\/to\/image\/fullsize.jpg',\n                'type' => '',\n                'meta' => array(\n                    'crop'   => 'primary',\n                    'height' => '1000',\n                    'width'  => '500',\n                ),\n            ),\n        ),\n    ),\n));\n$img->save();\n\n\/\/ now attach it to the story\n$doc->links->item = array(new \\stdClass());\n$doc->links->item[0]->href = $img->href;\n\n\/\/ put the story in a topic, while we're at it\n$doc->links->collection = array(new \\stdClass());\n$doc->links->collection[0]->href = $sdk->hrefTopic('arts');\n$doc->save();\nCaching\nDocument-level caching is not yet implemented (see #21).  But you can cache the \\Pmp\\Sdk itself, to optimize requests for the PMP home-doc and Oauth tokens across your application requests.\n$sdk = new \\Pmp\\Sdk($host, 'client-id', 'client-secret');\n$cache_str = serialize($sdk);\n$my_cache_mechanism->set('pmpsdk', $cache_str, 3600);\n\n\/\/ awhile later, in a different HTTP request\n$cache_str = $my_cache_mechanism->get('pmpsdk');\nif ($cache_str) {\n    try {\n        $sdk = unserialize($cache_str);\n        if ($sdk === false) {\n            throw new \\RuntimeException('Failed to unserialize SDK');\n        }\n    } catch (\\RuntimeException $e) {\n        \/\/ failed to unserialize SDK, so need to start fresh\n        $sdk = new \\Pmp\\Sdk($host, 'client-id', 'client-secret'); \n    }\n    \/\/ if retrieved from cache successfully, this would only generate 1 request, \n    \/\/ since we would already have both the PMP home doc and an auth token\n    $doc = $sdk->fetchDoc('SOME-GUID');\n}\nNote that if the serialized string is somehow corrupt:\n\nfor PHP 7.2.8 and later, the call to unserialize() will return false and also might generate a PHP RuntimeException\nfor PHP 7.2.7 and earlier, the call to unserialize() will generate a PHP RuntimeException\n\nDeveloping\nTo get started on development, check out the this repo, and run a make install.  (This requires Composer be present on your system).\nThis module is tested using the TAP protocol and requires the prove command, part of the standard Perl distribution on most Linux and UNIX systems.  You'll also need to provide some valid PMP credentials.\nThe test suite can be invoked as follows...\n$ export PMP_HOST=https:\/\/api-sandbox.pmp.io\n$ export PMP_USERNAME=myusername\n$ export PMP_PASSWORD=password1234\n$ export PMP_CLIENT_ID=my_client_id\n$ export PMP_CLIENT_SECRET=my_client_secret\n$\n$ make test\nTo debug the HTTP calls occurring during the tests, set the DEBUG environment variable to 1 with DEBUG=1 make test.\nTo build a PHAR version of this SDK, run make build.\nIssues and Contributing\nReport any bugs or feature-requests via the issue tracker.\nLicense\nThe PMP PHP SDK is free software, and may be redistributed under the MIT-LICENSE.\nThanks for listening!\n","96":"##Installing the PMPAPI Module(s)\n\n\nDownload modules as you would normally.\n\n\nGet a copy of the PHP SDK via git:\ngit clone -b v0.1.2 --depth 1 https:\/\/github.com\/npr\/pmp-php-sdk.git\n\n\nOr get the zip at\nhttps:\/\/github.com\/npr\/pmp-php-sdk\/archive\/v0.1.2.zip\n\n\nMake sure you do not use a newer version of the SDK. It can and will break the modules.\n\n\nThe PHP SDK can be placed inside the PMPAPI module directory or, if using version 7-1.2 or above, you can place it in sites\/all\/libraries (recommended).\n\n\nEnable the pmpapi module (or more)\n\n\nMake sure you have the proper PMP credentials. https:\/\/support.pmp.io\/register\n\n\nConfigure your PMP settings at: admin\/config\/services\/pmp\n\n\n##Architecture\n\nThis package is composed of 8 distinct modules.\npmpapi provides basic, minimal PMP API functionality.\npmpapi_push (push) allows one to turn local entities into PMP docs, and then write them to the API.\npmpapi_pull (pull) allows one to pull down various API docs, and then them turn the into (local, Drupal) entities.\npmpapi_query_tools allows the user to build, save and edit complicated pull queries that are run on cron.\npmpapi_groups allows the user to create\/edit\/delete PMP user groups. There is no strict dependence between the groups and permissions module.\npmpapi_permissions (perms) allows one to manage permissions on API content. Do not confuse these with native Drupal permissions.\npmpapi_updates (updates) allows one to have pulled stories automatically updated, with a minimal amount of effort.\npmpapi_remote_files extends functionality of pull, via the Remote stream wrapper module. It allows certain file-based profiles to be pulled, without the file being copied locally.\npmpapi, pmpapi_push, pmpapi_pull, and pmpapi_update rely heavily on classes that, in theory, could be extended or replaced with custom classes.\nThese modules try to be very particular about vocabulary, in order to reduce confusion. When referring to \"low-level\" functionality of the base module (pmpapi.module), the verbs are: fetch (a single doc), query (make a general query), send (data to the API), and remove (doc(s) from the API). When referring to functionality that maps drupal entities to PMP docs (and vice versa), the verbs are push and pull.\n\n##Push\n\nIn general, the push module maps an entity on your site to a PMP profile. In addition, fields attached to that entity can be mapped to attributes and items of that profile.\nPush is dependent upon pmpapi and, for the sake of much more readable code, Entity API\nEntities will be re-pushed any time they are edited.\nDeleted entities will also be deleted from the PMP API. (There is a failure queue as part of the PMPAPI base module, in an attempt to ensure their deletion.)\nEntities that are unpublished (i.e., $entity->status = 0) will also be removed from the API.\nSome entities, such as file entities, have no status property (or a different notion of status) and, thus, can only be removed from the API by deleting the corresponding entity.\nIf you have a node (or other entity) with attached images or other entities, those images will be linked to your doc in the PMP if and only if they are first pushed to the API as separate docs (with a completely separate mapping of entity<-->profile and field<-->attribute).\nIn other words, if you'd like to push images and other files, the file_entity module is highly recommended, but, strictly speaking, not a dependency.\nThis granular, non-recursive process of pushing also favors new sites -- or pre-existing sites interested in pushing only new content. Re-stated: pushing random, old pieces of content with deep entity associations could be problematic.\nEntities -- if they are deemed \"OK to push\" and do not already have a GUID -- are assigned a GUID on presave. That GUID is then saved to a pmpapi-specfic lookup table.\nSee pmpapi_push_entity_ok_to_push() for more info on deeming an entity \"OK to push.\"\n\nPull\n\nPull works in a very similar fashion to push, except, of course, in reverse. Incoming docs and the attributes and items are mapped to Drupal entities and their fields.\nUnilke push, pulled entities will not be re-pulled on entity save.\nAlso unlike push, attached entities are also pulled. Though there are some rules: the attached entity much be file-based (i.e. - has an enclosure link). For now, the allowed profiles are hard-coded to just audio, image, and video. In addition, the pull will only recurse to a depth of 1. That is, if a story is linked to an image, and that image is linked to, say, another image, only the first image will be pulled.\nPull has hard dependencies on the pmpapi module and the Entity API module. As with push, if you'd like to push images and other files, the file_entity module is highly recommended, but, strictly speaking, not a dependency.\n\nQuery Tools\n\nGo to admin\/config\/services\/pmp\/queries to start creating pull queries.\nQueries will automatically add content at cron.\n\nGroups\n\nPMP user groups are simply collections of PMP user\/org docs. By themselves, they have no notion of blacklist\/whitelist, or even any explicit relation to PMP doc permissions.\nThis module is a very simple UI that allows one to add, edit, and delete PMP groups.\nFor users planning on creating involved whitelist\/blacklist permissions, this module is highly recommended.\n\nPermissions\n\nThis module allows the user to set explicit read permissions around a doc, with an entity-level granularity.\nDefault permissions can be set at a entity-type level (e.g., on the \/admin\/structure\/types\/manage\/ config page).\nThe module is actually a simplified version of the PMP permission system. Users can modify access to a doc by adding a single whitelist permission to a doc. Still, this should address 90+% of permission use-cases.\nThere are three basic permissions provided by this module:\n\nGrant access to everyone (no actual permissions on the doc).\nMake private (doc is whitelisted to only the current user).\nDoc is whitelisted to a selected group.\n\n\nDealing with blacklists is much more complicated, as they usually require a complementary whitelist and, as of this writing, they don't really work as you might expect (e.g., there is no conecpt of whitelisting all users).\nIf there is enough demand for more complicated permissioning, we might write a pmpapi_advanced_permissions module that allows one to add multiple, additive, whitelist or blacklist permissions to a given entity.\nCurrently, permissions can only be added to nodes and file entities (if the site is using the file_entity module).\nFor much more about PMP permission, see: http:\/\/cdoc.io\/spec.html#content-rights\n\nUpdates\n\nThis module allows one to have pulled stories automatically updated, with a minimal amount of effort.\n\nRemotes Files\n\nThis module extends functionality of pull, via the Remote stream wrapper module.\nIt allows certain file-based profiles to be pulled, without the file being copied locally.\nWhen enabled, the user will see an extra fieldset on the pull configuration page: a list of all file entities.\nChecked file entities will still be pulled, but Drupal will not make a local copy of the actual accompanying file (mp3, jpg, etc.)\nThis is particularly useful for, say, large video and audio files that are already hosted on a CDN.\n\n","97":"Zaphpa\n\n\n\n\n\nInstalling Zaphpa with Composer\nAdd zaphpa\/zaphpa to your composer.json.\n{\n  \"require\": {\n    \"zaphpa\/zaphpa\": \"^2.1.1\"\n  }\n}\n\nFull Documentation: http:\/\/zaphpa.org\nIf, for whatever reason, you are looking for older version (1.x) of Zaphpa you can download it from: https:\/\/github.com\/zaphpa\/zaphpa\/releases\/tag\/1.2.2 (not recommended).\nMaintainers\n\nIrakli Nadareishvili (inadarei)\nJohn Nelson (johnymonster)\n\nHistory\nFirst major user of Zaphpa, in production, was NPR's API. Zaphpa hugely benefited\nfrom this use through meticulous bug-fixing, various contributions and overall creativity of the\nwonderful NPR software teams.\nZaphpa is now used by a variety of organizations worldwide. If you use Zaphpa and want to be listed here,\nplease open a pull request.\nLicense & Credits\nReleased under the MIT open source license. See LICENSE for details.\nThe predecessor to Zaphpa was originally created by Ioseb Dzmanashvili. Project\nwas later adopted and iterated upon by Irakli Nadareishvili.\nAdditional contributors:\n\nThomas Bracher\nPaul Williams\nJeff Pritchard\nAndrew Winder\nRandall Randall\nJason Grosman\nStephan L. Smith\n\n","98":"This module depends on Opauth which is no longer maintained - we don't recommend continuing to use this module as we aren't actively developing it any longer.\nSilverStripe Opauth Module\n\n\n\nIntroduction\nUses the Opauth library for easy drop-in strategies for social login. See their documentation\nCurrent Status\n1.1 - stable. No known major issues. Report issues using the bug tracker.\nHow does it work?\nThe module provides an additional login form which the developer has control over, that allows users to instantly sign in to your website with an identity provided by any Oauth provider. The providers are each handled by using an OpauthStrategy, many of which are freely available. There are strategies for Facebook, Twitter, Google, and many more.\nBased on the identity data from the Oauth provider, the module will find or create a new Member object based on the provided email address in the identity. This also means a Member can have many Oauth identities linked to a single account; these are saved in to the OpauthIdentity object.\nIf the resultant Member generated from the provider's response doesn't have an email address, or any other piece of data you require, there is functionality built in to handle this. You can enforce required fields, or any other kind of validation, by setting the OpauthValidator's custom_validator property to the class name of the validator you require.\nOther than that, the user flow is quite simple. Provided all required data is there, the member is logged in with Member::login and then redirected to the page they were looking at or the default destination, settable in your config - just like the default MemberAuthenticator.\nRequirements\n\nSilverStripe 3.1 (maybe 3.0, but untested so far)\nAt least one Opauth strategy\nPreferably, allow_url_fopen enabled in php.ini. We've written a custom cURL workaround that works with Twitter, Google and Facebook strategies, but it's proprietary.\nFor extended cURL support we rely on an Opauth fork\n\nFAQ\nWhat does this module include?\nIt includes:\n\nthe Opauth core (see below);\nOpauthAuthenticator: intended to be comparable with MemberAuthenticator;\nOpauthLoginForm: which offers different ways you can authenticate;\nOpauthRegisterForm: which, if configured, provides an intermediate step so incomplete OpauthIdentity-authenticating members can fill in extra information as required;\nOpauthController: which acts as a negotiator for the communication that strategies undertake;\nOpauthIdentity: which acts as a service-agnostic interface with which to save Oauth identities in to the Member object. These are associated with a Member upon successful login so that the auth provider's UID and signed response act as a key.\n\nNB: Opauth's maintainers recommend you include strategies as required, rather than bundling them together.\nWhere can I get strategies?\nYou can find a list under the \"Available Strategies\" heading on the Opauth homepage\nPackagist provides a list of strategies you can use to install via Composer.\nWhere should I put strategies?\nUse composer to require your strategies\nHow do I map the API responses to a Member?\nYou define the OpauthIdentity member_mapper block in your _config.yml. Simply provide a hash map of member fields to dot notated paths of the Opauth response array for simple fields, or if you need to perform some parsing to retrieve the value you want, an array of class name and function, like ['OpauthResponseHelper', 'get_first_name']. It takes the auth response array as an argument. See the example config YAML below for more details.\nHow do I configure the module and its strategies?\nAll Opauth-specific configuration variables can be put under opauth_settings and are passed directly to Opauth.\nYou can put these settings in your _config.yml file. Additionally, as your strategy API details will likely change per domain and thus per environment, you are able to update these using the Config API. Please see the Opauth config documentation. Here's some examples to help you:\n_config.yml example:\n---\nName: silverstripe-opauth\nAfter: 'framework\/*','cms\/*'\n---\n# see the Opauth docs for the config settings - https:\/\/github.com\/opauth\/opauth\/wiki\/Opauth-configuration#configuration-array\nOpauthAuthenticator:\n  opauth_settings:\n    #Register your strategies here\n    #Including any extra config\n    Strategy:\n      Facebook:\n        app_id: ''\n        app_secret: ''\n        scope: email\n      Google:\n        client_id: ''\n        client_secret: ''\n      Twitter:\n        key: ''\n        secret: ''\n    security_salt: 'correct horse battery staple'\n    security_iteration: 500\n    security_timeout: '2 minutes'\n    callback_transport: 'session'\n#Configuration for the Identity-Member mapping\nOpauthIdentity:\n  member_mapper:\n    Facebook:\n      FirstName: 'info.first_name'\n      Surname: 'info.last_name'\n      Locale: 'raw.locale'\n      Email: 'info.email'\n    Twitter:\n      FirstName: ['OpauthResponseHelper', 'get_first_name']\n      Surname: ['OpauthResponseHelper', 'get_last_name']\n      Locale: ['OpauthResponseHelper', 'get_twitter_locale']\n    Google:\n      FirstName: 'info.first_name'\n      Surname: 'info.last_name'\n      Email: 'info.email'\n      Locale: ['OpauthResponseHelper', 'get_google_locale']\n_config.php example:\n\/\/Register and configure strategies\nConfig::inst()->update('OpauthAuthenticator', 'opauth_settings', array(\n  'Strategy' => array(\n    'Facebook' => array(\n      'app_id' => '',\n      'app_secret' => '',\n      'scope' => 'email',\n    ),\n    'Google' => array(\n      'client_id' => '',\n      'client_secret' => ''\n    ),\n    'Twitter' => array(\n      'key' => '',\n      'secret' => ''\n    ),\n  ),\n));\n\n\/\/Identity to member mapping settings per strategy\nConfig::inst()->update('OpauthIdentity', 'member_mapper', array(\n  'Facebook' => array(\n    'FirstName' => 'info.first_name',\n    'Surname' => 'info.last_name',\n    'Locale' => 'raw.locale',\n    'Email' => 'info.email',\n  ),\n  'Twitter' => array(\n    'FirstName' => array('OpauthResponseHelper', 'get_first_name'),\n    'Surname' => array('OpauthResponseHelper', 'get_last_name'),\n    'Locale' => array('OpauthResponseHelper', 'get_twitter_locale'),\n  ),\n  'Google' => array(\n    'FirstName' => 'info.first_name',\n    'Surname' => 'info.last_name',\n    'Email' => 'info.email',\n    'Locale' => array('OpauthResponseHelper', 'get_google_locale'),\n  ),\n));\nNB: As you can see, sometimes the Strategy configuration settings may have inconsistent namings - we can't help with that, sorry!\nDocumentation\nPlease read the Opauth documentation and our own documentation\nRaising bugs and suggesting enhancements\nIf you find a bug or have a Really Good Idea\u2122, please raise an issue. Better still, if you can fix the bug, then feel free to send in a pull request with the remedial code that ideally respects the coding conventions used thus far.\nAttribution\n\nOpauth available under MIT licence by U-Zyn Chua (http:\/\/uzyn.com) Copyright \u00a9 2012-2013\n\n","99":"Slim Framework\n\n\n\n\nSlim is a PHP micro-framework that helps you quickly write simple yet powerful web applications and APIs.\nInstallation\nIt's recommended that you use Composer to install Slim.\n$ composer require slim\/slim \"^3.0\"\nThis will install Slim and all required dependencies. Slim requires PHP 5.5.0 or newer.\nUsage\nCreate an index.php file with the following contents:\n<?php\n\nrequire 'vendor\/autoload.php';\n\n$app = new Slim\\App();\n\n$app->get('\/hello\/{name}', function ($request, $response, $args) {\n    $response->write(\"Hello, \" . $args['name']);\n    return $response;\n});\n\n$app->run();\nYou may quickly test this using the built-in PHP server:\n$ php -S localhost:8000\nGoing to http:\/\/localhost:8000\/hello\/world will now display \"Hello, world\".\nFor more information on how to configure your web server, see the Documentation.\nTests\nTo execute the test suite, you'll need phpunit.\n$ phpunit\nContributing\nPlease see CONTRIBUTING for details.\nLearn More\nLearn more at these links:\n\nWebsite\nDocumentation\nSupport Forum\nTwitter\nResources\n\nSecurity\nIf you discover security related issues, please email security@slimframework.com instead of using the issue tracker.\nCredits\n\nJosh Lockhart\nAndrew Smith\nRob Allen\nGabriel Manricks\nAll Contributors\n\nLicense\nThe Slim Framework is licensed under the MIT license. See License File for more information.\n"},"top_code":{"0":"\n\nPython\n99.5%\n","1":"\n\nPython\n72.3%\n","2":"\n\nPython\n89.2%\n","3":"\n\nPython\n98.6%\n","4":"\n\nJavaScript\n93.2%\n","5":"\n\nJavaScript\n100.0%\n","6":"\n\nPython\n76.0%\n","7":"\n\nJavaScript\n61.3%\n","8":"\n\nPython\n58.4%\n","9":"\n\nRuby\n100.0%\n","10":"\n\nRuby\n98.3%\n","11":"\n\nRuby\n100.0%\n","12":"\n\nPython\n97.1%\n","13":"\n\nPython\n100.0%\n","14":"\n\nRuby\n100.0%\n","15":"\n\nRuby\n54.6%\n","16":"\n\nRuby\n100.0%\n","17":"\n\nRuby\n94.3%\n","18":"\n\nRuby\n75.2%\n","19":"\n\nRuby\n100.0%\n","20":"\n\nRuby\n90.7%\n","21":"\n\nRuby\n92.9%\n","22":"\n\nRuby\n92.5%\n","23":"\n\nRuby\n41.8%\n","24":"\n\nRuby\n100.0%\n","25":"\n\nRuby\n100.0%\n","26":"\n\nRuby\n100.0%\n","27":"\n\nRuby\n76.9%\n","28":"\n\nPython\n100.0%\n","29":"\n\nRuby\n100.0%\n","30":"\n\nJavaScript\n94.9%\n","31":"\n\nPython\n99.8%\n","32":"\n\nPHP\n100.0%\n","33":"\n\nPHP\n95.7%\n","34":"\n\nPHP\n100.0%\n","35":"\n\nPHP\n100.0%\n","36":"\n\nPHP\n56.2%\n","37":"\n\nPHP\n100.0%\n","38":"\n\nPHP\n99.4%\n","39":"\n\nPHP\n100.0%\n","40":"\n\nPHP\n100.0%\n","41":"\n\nPHP\n100.0%\n","42":"\n\nPHP\n100.0%\n","43":"\n\nJavaScript\n62.7%\n","44":"\n\nJavaScript\n71.4%\n","45":"\n\nJavaScript\n100.0%\n","46":"\n\nJavaScript\n100.0%\n","47":"\n\nPython\n99.9%\n","48":"\n\nPython\n55.7%\n","49":"\n\nPython\n53.8%\n","50":"\n\nPython\n100.0%\n","51":"\n\nPython\n79.3%\n","52":"\n\nPython\n99.9%\n","53":"\n\nPython\n100.0%\n","54":"\n\nPython\n83.0%\n","55":"\n\nRuby\n100.0%\n","56":"\n\nPython\n69.0%\n","57":"\n\nPython\n97.8%\n","58":"\n\nPython\n97.5%\n","59":"\n\nPython\n73.8%\n","60":"\n\nJavaScript\n54.9%\n","61":"\n\nJavaScript\n100.0%\n","62":"\n\nJavaScript\n100.0%\n","63":"\n\nJavaScript\n91.5%\n","64":"\n\nJavaScript\n51.3%\n","65":"\n\nJavaScript\n94.1%\n","66":"\n\nHTML\n97.7%\n","67":"\n\nHTML\n59.3%\n","68":"\n\nHTML\n66.7%\n","69":"\n\nHTML\n56.6%\n","70":"\n\nHTML\n66.3%\n","71":"\n\nHTML\n65.8%\n","72":"\n\nHTML\n59.3%\n","73":"\n\nHTML\n52.8%\n","74":"\n\nHTML\n30.9%\n","75":"\n\nHTML\n56.0%\n","76":"\n\nHTML\n100.0%\n","77":"\n\nJavaScript\n60.7%\n","78":"\n\nJavaScript\n100.0%\n","79":"\n\nHTML\n58.3%\n","80":"\n\nHTML\n48.9%\n","81":"\n\nHTML\n56.4%\n","82":"\n\nHTML\n90.9%\n","83":"\n\nHTML\n98.6%\n","84":"\n\nHTML\n79.7%\n","85":"\n\nHTML\n62.0%\n","86":"\n\nHTML\n69.2%\n","87":"\n\nHTML\n68.0%\n","88":"\n\nJavaScript\n51.8%\n","89":"\n\nJavaScript\n100.0%\n","90":"\n\nJavaScript\n76.2%\n","91":"\n\nPHP\n38.3%\n","92":"\n\nPHP\n92.2%\n","93":"\n\nPHP\n100.0%\n","94":"\n\nPHP\n74.1%\n","95":"\n\nPHP\n99.7%\n","96":"\n\nPHP\n100.0%\n","97":"\n\nPHP\n100.0%\n","98":"\n\nPHP\n100.0%\n","99":"\n\nPHP\n99.9%\n"}}